{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI-Assisted Data Curation Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook demonstrates the AI-Assisted Data Curation Toolkit. It is capable of suggesting harmonizations from a source data model into a target data model using AI-backed approaches, but leaving the expert curator in complete control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Set available GPUs (skip this step if using CPUs) - must be done before importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath('./'))\n",
    "sys.path.append(CURRENT_DIR)\n",
    "\n",
    "from harmonization.interactive import (\n",
    "    get_interactive_table_for_suggestions,\n",
    "    get_nodes_and_properties_df,\n",
    ")\n",
    "from harmonization.simple_data_model import (\n",
    "    SimpleDataModel,\n",
    "    get_data_model_as_node_prop_type_descriptions,\n",
    ")\n",
    "from harmonization.harmonization_approaches.similarity_inmem import (\n",
    "    SimilaritySearchInMemoryVectorDb,\n",
    ")\n",
    "from harmonization.harmonization_approaches.rag import (\n",
    "    RetrievalAugmentedGeneration,\n",
    ")\n",
    "from harmonization.harmonization_approaches.embeddings import (\n",
    "    MedGemmaEmbeddings,\n",
    "    QwenEmbeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Use a Harmonization Approach to get Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Get Input Data\n",
    "\n",
    "- A `source data model` you want to harmonize from\n",
    "- A `target data model` you want to harmonize to\n",
    "\n",
    "For this initial example, you can just using hard-coded examples.\n",
    "\n",
    "- The `example_synthetic_source_model.json` is a synthetically generated model for example purposes\n",
    "- The `example_real_source_model.json` is a real original study before ingestion into the NHLBI BioData Catalyst ecosystem (e.g. not yet harmonized)\n",
    "- The `target data model` example is the **NHLBI BioData Catalyst Gen3 Data Dictionary v4.6.5** (latest version as of 21 AUG 2025)\n",
    "\n",
    "You can change this to supply your own source model, so long as the format follows the example. Similarly for target model. The source model will eventually come from a connection to a previously released AI-backed tool for Schema Generation, allowing this entire flow to start from arbitrary TSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = \"./examples/example_synthetic_source_model.json\"\n",
    "source_file = \"./examples/example_real_source_model.json\"\n",
    "\n",
    "target_file = \"./examples/example_target_model_BDC.json\"\n",
    "\n",
    "with open(source_file, \"r\") as f:\n",
    "    input_source_model = json.load(f)\n",
    "\n",
    "input_source_model = SimpleDataModel.get_from_unknown_json_format(\n",
    "    json.dumps(input_source_model)\n",
    ")\n",
    "\n",
    "with open(target_file, \"r\") as f:\n",
    "    input_target_model = json.load(f)\n",
    "\n",
    "input_target_model = SimpleDataModel.get_from_unknown_json_format(\n",
    "    json.dumps(input_target_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source Model\")\n",
    "input_source_model.get_property_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Model\")\n",
    "input_target_model.get_property_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Use Similarity Search Approach to get Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment desired embedding model\n",
    "# embedding_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/medgemma-4b-it\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/embeddinggemma-300m\")\n",
    "embedding_fn = QwenEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Similarity Search Approach\n",
    "# :62 b/c of limitation on chromadb collection names\n",
    "harmonization_approach = SimilaritySearchInMemoryVectorDb(\n",
    "    vectordb_persist_directory_name=f\"{os.path.basename(target_file)[:53]}-{embedding_fn.model.name_or_path.split('/')[-1][:5]}-0\",\n",
    "    input_target_model=input_target_model,\n",
    "    embedding_function=embedding_fn,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "max_suggestions_per_property = 10\n",
    "# set threshold low to just get top properties no matter what\n",
    "score_threshold = 0\n",
    "\n",
    "suggestions = harmonization_approach.get_harmonization_suggestions(\n",
    "    input_source_model=input_source_model,\n",
    "    input_target_model=input_target_model,\n",
    "    score_threshold=score_threshold,\n",
    "    k=max_suggestions_per_property,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Use RAG Approach to get Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment desired embedding model\n",
    "#embedding_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "#embedding_fn = MedGemmaEmbeddings(model_name=\"google/medgemma-4b-it\")\n",
    "#embedding_fn = MedGemmaEmbeddings(model_name=\"google/embeddinggemma-300m\")\n",
    "embedding_fn = QwenEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# RAG Approach\n",
    "\n",
    "llm_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# set how many suggestions from similarity search as the choices for llm, update llm_max_model_len accordingly.\n",
    "similarity_search_k_value = 30\n",
    "# set threshold low to just get top properties no matter what\n",
    "similarity_search_score_threshold = 0.1\n",
    "# if the prompt is too long, update max_model_len accordingly\n",
    "llm_max_model_len = 2048 #8192\n",
    "# the approach searches more candidates, but only output max number of suggestions\n",
    "max_suggestions_num = 5\n",
    "# the embedding occupys memory, lower llm's gpu_memory_utilization if llm runs into memory issue. the Default is 0.9.\n",
    "llm_gpu_memory_utilization = 0.7\n",
    "\n",
    "# :62 b/c of limitation on chromadb collection names\n",
    "harmonization_approach = RetrievalAugmentedGeneration(\n",
    "    vectordb_persist_directory_name=f\"{os.path.basename(target_file)[:53]}-{embedding_fn.model.name_or_path.split('/')[-1][:5]}-0\",\n",
    "    input_target_model=input_target_model,\n",
    "    embedding_function=embedding_fn,\n",
    "    embedding_batch_size=batch_size,\n",
    "    similarity_search_k_value=similarity_search_k_value,\n",
    "    similarity_search_score_threshold=similarity_search_score_threshold,\n",
    "    llm_gpu_memory_utilization=llm_gpu_memory_utilization,\n",
    "    max_suggestions_num=max_suggestions_num,\n",
    "    llm_max_model_len=llm_max_model_len,\n",
    "    llm_model_name=llm_model_name,\n",
    ")\n",
    "\n",
    "suggestions = harmonization_approach.get_harmonization_suggestions(\n",
    "    input_source_model=input_source_model,\n",
    "    input_target_model=input_target_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Visualize Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = suggestions.to_simlified_dataframe()\n",
    "table_df.sort_values(by=\"Similarity\", ascending=False, inplace=True)\n",
    "table_df.to_csv('examples/rag_suggestions-qwen_embedding.csv')\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Original Node.Property' and find the index of max similarity for each group\n",
    "idx = table_df.groupby(\"Original Node.Property\")[\"Similarity\"].idxmax()\n",
    "\n",
    "# Filter DataFrame using the indices found above\n",
    "filtered_df = table_df.loc[idx]\n",
    "filtered_df.drop(columns=[\"Original Description\", \"Target Description\"], inplace=True)\n",
    "filtered_df.sort_values(by=\"Similarity\", ascending=False, inplace=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Use Fine-Tuned LLM To Choose Best Match From Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Set Up For Inference\n",
    "\n",
    "I recommend restarting the kernel when you get to this part. I have not found a way to elegantly offload the embedding model used for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath('./'))\n",
    "sys.path.append(CURRENT_DIR)\n",
    "\n",
    "from harmonization.harmonization_approaches.inference import batch_inference\n",
    "from harmonization.harmonization_approaches.create_inference_prompts import prefiltering_to_inference_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Prepare inference prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FORMAT_FILE = \"examples/harmonization_prompt_1.txt\"\n",
    "\n",
    "table_df = pd.read_csv('examples/rag_suggestions-qwen_embedding.csv')\n",
    "\n",
    "test_df = prefiltering_to_inference_prompt(df=table_df, prompt_format_file_path=PROMPT_FORMAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.input_text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Check one more time that you're pointed at the right GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Loading the base model will take about 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\" \n",
    "LORA_MODEL_PATH = \"path_to_model_dir\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "llm = LLM(model=BASE_MODEL, enable_lora=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Property(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "class Node(BaseModel):\n",
    "    name: str\n",
    "    properties: Property\n",
    "\n",
    "class ResponseSchema(BaseModel):\n",
    "    nodes: Node\n",
    "\n",
    "\n",
    "ref_schema = ResponseSchema.model_json_schema()\n",
    "decoding_params = GuidedDecodingParams(json=ref_schema) \n",
    "\n",
    "# Set inference sampling parameters\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=8192,\n",
    "    guided_decoding=decoding_params,\n",
    ")\n",
    "\n",
    "results_df = batch_inference(test_df, batch_size=BATCH_SIZE, llm=llm, lora_model_path=LORA_MODEL_PATH, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Choose Index of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Source Node, Property, and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.input_text.iloc[index].split('\\n')[2].split('Here is the source model node and property: ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Target Node, Property, and Description Possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.input_text.iloc[index].split('\\n')[4].split('Here are the target model nodes and properties: ')[1].replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Generated Node, Property, and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(results_df.generated_output_text.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Create Interactive Table for Selecting Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = get_interactive_table_for_suggestions(\n",
    "    table_df,\n",
    "    column_for_filtering=1,\n",
    "    # additional config for the interactive table\n",
    "    maxBytes=\"2MB\",\n",
    "    pageLength=50,\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "> **Don't see the table or see an error above?** Try restarting the kernel, then try restarting jupyter lab (if that's what you're using). The installs for AnyWidgets might not be picked up yet.\n",
    "\n",
    "> **Dark Theme?** If you're using a dark theme, you might need to switch to light for the table to display properly. \n",
    "\n",
    "> **Using VS Code Jupyter Extension?** Links might not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "To use the selections above, record them below in `manual_selection_indexes` or ue multi-select in the above table and the below will automatically use those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this out manually as you go, or we'll use the table selections\n",
    "manual_selection_indexes = []  # [1, 8, 24, ...]\n",
    "\n",
    "selected_rows = manual_selection_indexes or table.selected_rows\n",
    "\n",
    "print(f\"Selected Suggestions: {selected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.iloc[selected_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.iloc[selected_rows].to_csv(\n",
    "    \"./selected_suggestions.tsv\",\n",
    "    index=False,\n",
    "    na_rep=\"N/A\",\n",
    "    sep=\"\\t\",\n",
    "    quotechar='\"',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arpa-h",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
