{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Training Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We'll be using a multiple negative loss function, so we need to adapt our training data to that format (anchor, positive, negative) pairs.\n",
    "\n",
    "This notebook contains the logic to do the training.\n",
    "\n",
    "**If you want to generate the training data in the right format, go to the \"Training Data\" notebook.**\n",
    "\n",
    "### Background: Existing Training Data File Format\n",
    "\n",
    "Each row in the existing JSONL training file includes a source model, with desire to harmonize to a target. We then have ground truth harmonization in `expected_mappings.tsv`.\n",
    "\n",
    "The JSONL file has 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns are effectively populated by content of files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`\n",
    "\n",
    "### How we converted existing training JSONL file to the format expected for embedding model training\n",
    "\n",
    "For each ground truth mapping in `harmonized_mapping`:\n",
    "\n",
    "- Extrapolate the \"negatives\" (wrong choices) by providing the same source variable but with every target variable _except_ the correct, \"positive\" one, in the harmonized mapping\n",
    "- Output a new CSV file with 3 columns: `anchor`, `positive`, `negative`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `anchor` == the source variable from the harmonized mapping\n",
    "- `positive` == the target variable from the harmonized mapping\n",
    "- `negative` == every other target variable except the right one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "> tl;dr the NOTE below: to simplify things to start, we are NOT going to provide explicit negatives and let the batch deal with them itself. The training dataset will still includet them for potential future use.\n",
    "\n",
    "> NOTE: negatives were originally expanded in a denormalized way so that a given achor/positive pair ended up with n number of test cases where n is the number of unique negatives. \n",
    ">\n",
    "> It's not clear if that will result in better outcomes than if we have a single test case for each anchor/positive pair with a list of negatives. Update: It seems like MultiNegativeLoss can't take an actual list. So it appears like they expect an expanded version.\n",
    ">\n",
    "> It's unclear to me how to support a list without\n",
    "> fully expanding duplicate anchor/positive rows with every possible negative. And doing\n",
    "> so I don't think really achieves what I want with the MultipleNegativesRankingLoss function.\n",
    "> It seemingly treats every other option as negative if just providing achor/positive,\n",
    "> so we can try that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Change the paths below to wherever you have the existing training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_csv_path = (\n",
    "    \"../datasets/embedding_training_data_v0.0.1/embedding_training.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Use Training Data CSV from above and create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def stream_split_csv(\n",
    "    input_path: str,\n",
    "    output_dir: str,\n",
    "    seed: int = 42,\n",
    "    fractions: Tuple[float, float, float] = (0.80, 0.10, 0.10),\n",
    "    header: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a gigantic CSV into train/eval/test without loading it into RAM.\n",
    "\n",
    "    The function reads *one line at a time*, assigns it to a split using a\n",
    "    deterministic random choice, and writes the line to the appropriate output\n",
    "    file.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to the huge CSV\n",
    "        output_dir: Folder where `train.csv`, `validation.csv` and\n",
    "            `test.csv` will be written.  Will be created if it does not exist.\n",
    "        seed: Seed for the random assignment - the split will be deterministic.\n",
    "        fractions: Desired (train, eval, test) fractions.\n",
    "            Must sum to 1.0.  Default is (0.80, 0.10, 0.10).\n",
    "        header: If the input file has a header row, it will be copied to\n",
    "            every output file.\n",
    "\n",
    "    Returns:\n",
    "        None that three CSV files are written into `output_dir`.\n",
    "    \"\"\"\n",
    "    # Check that the fractions sum to 1.0\n",
    "    if abs(sum(fractions) - 1.0) > 1e-6:\n",
    "        raise ValueError(\"fractions must sum to 1.0\")\n",
    "\n",
    "    # Compute the split thresholds\n",
    "    train_threshhold, validate_threshhold, _ = fractions\n",
    "    train_threshhold = fractions[0]\n",
    "    validate_threshhold = fractions[0] + fractions[1]\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    out_paths = {\n",
    "        \"train\": os.path.join(output_dir, \"train.csv\"),\n",
    "        \"eval\": os.path.join(output_dir, \"eval.csv\"),\n",
    "        \"test\": os.path.join(output_dir, \"test.csv\"),\n",
    "    }\n",
    "\n",
    "    # Open a CSV writer for each split\n",
    "    files = {}\n",
    "    writers = {}\n",
    "    for key, path in out_paths.items():\n",
    "        file = open(path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "        files[key] = file\n",
    "        writers[key] = csv.writer(file)\n",
    "\n",
    "    open_kwargs = {\"newline\": \"\", \"encoding\": \"utf-8\"}\n",
    "    reader = open(input_path, \"r\", **open_kwargs)\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Counters for a quick sanity check after processing\n",
    "    n_rows = n_train = n_val = n_test = 0\n",
    "\n",
    "    with reader as f_in:\n",
    "        reader_obj = csv.reader(f_in)\n",
    "\n",
    "        # If the file has a header, pull it out first\n",
    "        header_row = next(reader_obj) if header else None\n",
    "\n",
    "        # Write the header to all output files\n",
    "        if header_row:\n",
    "            for w in writers.values():\n",
    "                w.writerow(header_row)\n",
    "\n",
    "        # Process each line once\n",
    "        for row in reader_obj:\n",
    "            n_rows += 1\n",
    "            r = random.random()\n",
    "            if r < train_threshhold:\n",
    "                writers[\"train\"].writerow(row)\n",
    "                n_train += 1\n",
    "            elif r < validate_threshhold:\n",
    "                writers[\"eval\"].writerow(row)\n",
    "                n_val += 1\n",
    "            else:\n",
    "                writers[\"test\"].writerow(row)\n",
    "                n_test += 1\n",
    "\n",
    "    # Close all file handles\n",
    "    for file in files.values():\n",
    "        file.close()\n",
    "\n",
    "    # Report the final counts\n",
    "    print(f\"✓ Split complete – {n_rows:,} rows processed\")\n",
    "    print(f\"   Train: {n_train:,} rows\")\n",
    "    print(f\"   Val:   {n_val:,} rows\")\n",
    "    print(f\"   Test:  {n_test:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "> **WARNING**: Below can take a long time and it will _duplicate_ the large input into separated files. So you need about double the hard drive space of the input for this whole process to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This splitting is also randomizing the data a bit, putting things in training, eval, test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_split_csv(\n",
    "    input_path=training_data_csv_path,\n",
    "    output_dir=os.path.join(os.path.dirname(training_data_csv_path), \"splits\"),\n",
    "    seed=123,\n",
    "    fractions=(0.80, 0.10, 0.10),  # train / val / test\n",
    "    header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # change as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The following is influenced heavily by [Huggingface's blog article](https://huggingface.co/blog/train-sentence-transformers#trainer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "If using the pre-computed training data, the counts are:\n",
    "\n",
    "```\n",
    "✓ Split complete – 812,327 rows processed\n",
    "   Train: 649,352 rows\n",
    "   Val:   81,496 rows\n",
    "   Test:  81,479 rows\n",
    "```\n",
    "\n",
    "> NOTE: The .zip when expanded from above is ~80GB and you'll need more disk space to deal with the AI models, loading of the dataset, and training. Ensure you have enough headroom on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import (\n",
    "    MultipleNegativesRankingLoss,\n",
    "    CachedMultipleNegativesRankingLoss,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "EMBEDDING_MODEL = \"google/embeddinggemma-300m\"\n",
    "# EMBEDDING_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "# EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# 1. Load a model to finetune with 2. model card data\n",
    "model = SentenceTransformer(\n",
    "    EMBEDDING_MODEL,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"Base trained on appropriate relationships between similar biomedical variables\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 3. Load a dataset to finetune on\n",
    "path_to_csv_splits = (\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/splits\"\n",
    ")\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(path_to_csv_splits, \"train.csv\"),\n",
    "        \"eval\": os.path.join(path_to_csv_splits, \"eval.csv\"),\n",
    "        \"test\": os.path.join(path_to_csv_splits, \"test.csv\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Subset the full dataset as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(100_000))\n",
    "eval_dataset = dataset[\"eval\"].select(range(20_000))\n",
    "test_dataset = dataset[\"test\"].select(range(20_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any rows that contain None b/c our loss function expects full triplet of anchor, positive and negative examples.\n",
    "# These shouldn't exist, but in case they do...\n",
    "def has_no_nones(example):\n",
    "    return all(example[key] is not None for key in example)\n",
    "\n",
    "train_dataset = train_dataset.filter(has_no_nones)\n",
    "eval_dataset = eval_dataset.filter(has_no_nones)\n",
    "test_dataset = test_dataset.filter(has_no_nones)\n",
    "\n",
    "# For now, let's remove the negatives. It's unclear to me how to support a list without\n",
    "# fully expanding duplicate anchor/positive rows with every possible negative.\n",
    "# Note that the negatives column right now is a JSON-ified array string\n",
    "#\n",
    "# Even if I could figure out how to supply these,\n",
    "# I don't think it really achieves what I want with the MultipleNegativesRankingLoss function.\n",
    "# It seemingly treats every other option as negative if just providing achor/positive,\n",
    "# so let's try that.\n",
    "try:\n",
    "    train_dataset = train_dataset.remove_columns(\"negatives\")\n",
    "    eval_dataset = eval_dataset.remove_columns(\"negatives\")\n",
    "    test_dataset = test_dataset.remove_columns(\"negatives\")\n",
    "except ValueError:\n",
    "    # might already be removed\n",
    "    pass\n",
    "\n",
    "print(len(train_dataset), len(eval_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a loss function\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# TODO: was running into device issues on apple silicon when trying CachedMultipleNegativesRankingLoss\n",
    "#       to save on memory usage. Giving up for now.\n",
    "# # Pick a device you actually have\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     # Apple‑silicon\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "# print(f\"device: {device}\")\n",
    "# # Build / load your model\n",
    "# model.to(device)\n",
    "# loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size=64)\n",
    "\n",
    "# 5. (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"models/{EMBEDDING_MODEL.split('/')[-1]}-bio-mapping\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if GPU supports BF16\n",
    "    # MultipleNegativesRankingLoss benefits from no duplicates\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    # Used in W&B if `wandb` is installed\n",
    "    run_name=f\"{EMBEDDING_MODEL.split('/')[-1]}-bio-mapping\",\n",
    ")\n",
    "\n",
    "# 6. (Optional) Create an evaluator & evaluate the base model\n",
    "# dev_evaluator = TripletEvaluator(\n",
    "#     anchors=eval_dataset[\"anchor\"],\n",
    "#     positives=eval_dataset[\"positive\"],\n",
    "#     negatives=eval_dataset[\"negatives\"],\n",
    "#     name=\"bio-mapping-eval\",\n",
    "# )\n",
    "# dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    # evaluator=dev_evaluator,\n",
    "    # Disable model‑card callback (if the library supports it)\n",
    "    # callbacks=[],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# (Optional) Evaluate the trained model on the test set, after training completes\n",
    "# test_evaluator = TripletEvaluator(\n",
    "#     anchors=test_dataset[\"anchor\"],\n",
    "#     positives=test_dataset[\"positive\"],\n",
    "#     negatives=test_dataset[\"negatives\"],\n",
    "#     name=\"bio-mapping-eval\",\n",
    "# )\n",
    "# test_evaluator(model)\n",
    "\n",
    "# 8. Save the trained model\n",
    "model.save_pretrained(f\"models/{EMBEDDING_MODEL.split('/')[-1]}-bio-mapping/final\")\n",
    "\n",
    "# 9. (Optional) Push it to the Hugging Face Hub\n",
    "# model.push_to_hub(f\"{EMBEDDING_MODEL.split('/')[-1]}-bio-mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Test the Trained AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from disk\n",
    "embedding_model = SentenceTransformer(\n",
    "    f\"models/{EMBEDDING_MODEL.split('/')[-1]}-bio-mapping/final\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
