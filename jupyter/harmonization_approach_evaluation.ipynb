{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Harmonization Approach Using Abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install package manager and sync required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231e081",
   "metadata": {},
   "source": [
    "Set available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # change as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model: `*__ai_model_output.json`, with desire to harmonize to `harmonized_data_model.json`. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "JSONL file with a test per row.\n",
    "\n",
    "The JSONL file has 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `*__ai_model_ouput.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `input_target_model`\n",
    "- `harmonized_data_model.json` == `harmonized_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from harmonization.jsonl import (\n",
    "    split_harmonization_jsonl_by_input_target_model,\n",
    "    jsonl_to_csv,\n",
    ")\n",
    "from harmonization.harmonization_benchmark import get_metrics_for_approach\n",
    "from harmonization.harmonization_approaches.similarity_inmem import (\n",
    "    SimilaritySearchInMemoryVectorDb,\n",
    ")\n",
    "from harmonization.harmonization_approaches.embeddings import (\n",
    "    MedGemmaEmbeddings,\n",
    "    QwenEmbeddings\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "`output.jsonl` file contains 710 lines and `limited_output.jsonl` file contains 10 first lines from `output.jsonl` file.\n",
    "\n",
    "`limited_output.jsonl` might be useful for testing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_json_filepath = (\n",
    "#    \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2/output.jsonl\"\n",
    "#)\n",
    "\n",
    "output_json_filepath = (\n",
    "    \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2/limited_output.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jsonls_per_target_model_dir_path = (\n",
    "    \"../output/temp/harmonization/v0.0.2/per_target\"\n",
    ")\n",
    "split_harmonization_jsonl_by_input_target_model(\n",
    "    output_json_filepath, output_jsonls_per_target_model_dir_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = time.time()\n",
    "output_directory = \"./output/harmonization/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Choose sentence-transformers, Medgemma or Qwen embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c9b6b",
   "metadata": {},
   "source": [
    " Here are links to the model that might be used for embeddings:\n",
    "\n",
    " * sentence-transformers model (default model, 768-dimension): https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    " * Qwen model (1024-dimension): https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\n",
    " * MedGemma model (2560-dimension): https://huggingface.co/google/medgemma-4b-it\n",
    " * EmbeddingGemma model (768-dimension): https://huggingface.co/google/embeddinggemma-300m \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbbae0",
   "metadata": {},
   "source": [
    "> Please note: You might need to get access prior to using MedGemma or EmbeddingGemma models and you need use your HF_TOKEN inside this notebook to allow it to connect to the model. In case you want to use Medgemma or EmbeddingGemma models, please uncomment the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7151f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code if your model requires authorization via HuggingFace token\n",
    "#import os\n",
    "#from huggingface_hub import login\n",
    "#hf_token = None\n",
    "#with open(os.path.expanduser(\"~/.bashrc\"), \"r\") as f:\n",
    "#    for line in f:\n",
    "#        if line.startswith(\"export HF_TOKEN=\"):\n",
    "#            hf_token = line.strip().split(\"=\", 1)[1]\n",
    "#            break\n",
    "## Remove any quotes (if present)\n",
    "#if hf_token is not None:\n",
    "#    hf_token = hf_token.strip('\"').strip(\"'\")\n",
    "#login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Choose desired embedding by uncommenting a line, and configure batch size.\n",
    "\n",
    "> Tip: if you are using GPUs and getting Out of Memory error, try setting smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embedding_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embedding_fn = QwenEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "#embedding_fn = MedGemmaEmbeddings(model_name=\"google/medgemma-4b-it\")\n",
    "#embedding_fn = MedGemmaEmbeddings(model_name=\"google/embeddinggemma-300m\")\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Optional - test embeddings on small text inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"heart disease\"\n",
    "#embedded_text = embedding_fn.embed_query(text)\n",
    "#print(\"Embedded text:\", embedded_text)\n",
    "#print(\"Embedding dimension\": len(embedded_text))\n",
    "#del embedded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "> Warning: The next cells will take **a very long time** and a lot of CPU/GPU the first time you run it (took me 32 minutes on an M3 Mac), and just **a long time** (took me 20 minutes on an M3 Mac) on future runs. It's embedding every single target data model into a persistent vectorstore on disk (and loaded in mem) as it goes the first time. And then every run it's embedding all the test case `node.property` and doing similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove future warning from pandas\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "for file in os.listdir(output_jsonls_per_target_model_dir_path):\n",
    "    full_file_path = os.path.join(output_jsonls_per_target_model_dir_path, file)\n",
    "    print(f\"Opening {full_file_path}...\")\n",
    "    output_json_filepath = f\"{output_directory}/{folder_name}/{file}\"\n",
    "    os.makedirs(os.path.dirname(output_json_filepath), exist_ok=True)\n",
    "\n",
    "    # since these files are separated by target model already, just get the first row\n",
    "    input_target_model = None\n",
    "    with open(full_file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            row = json.loads(line)\n",
    "            input_target_model = json.loads(row[\"input_target_model\"])\n",
    "            break\n",
    "    print(\"Input target model received\")\n",
    "\n",
    "    # :62 b/c of limitation on chromadb collection names\n",
    "    harmonization_approach = SimilaritySearchInMemoryVectorDb(\n",
    "        vectordb_persist_directory_name=f\"{file[:62]}\",\n",
    "        input_target_model=input_target_model,\n",
    "        embedding_function=embedding_fn,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(\"Input target model added to vectorstore\")\n",
    "\n",
    "    output_filename = get_metrics_for_approach(\n",
    "        full_file_path,\n",
    "        harmonization_approach,\n",
    "        output_json_filepath,\n",
    "        metrics_column_name=\"custom_metrics\",\n",
    "    )\n",
    "    print(f\"Output metrics to {output_json_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af93694",
   "metadata": {},
   "source": [
    "Optional - empty GPU cache if GPU used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18666657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#del embedding_fn\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Example conversation to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_directory = \"./output/harmonization/\"\n",
    "# output_directory = os.path.join(\n",
    "#     output_directory, \"1755028259.3249412\"\n",
    "# )  # REPLACE with folder you want\n",
    "# for file in os.listdir(output_directory):\n",
    "#     full_file_path = os.path.abspath(os.path.join(output_directory, file))\n",
    "#     csv_path = full_file_path.replace(\".jsonl\", \".csv\")\n",
    "#     jsonl_to_csv(jsonl_path=full_file_path, csv_path=csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
