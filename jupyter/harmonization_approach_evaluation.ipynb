{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Harmonization Approach Using Abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install package manager and sync required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Tests are in a JSONL file with a source to target model per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from harmonization.jsonl import (\n",
    "    split_harmonization_jsonl_by_input_target_model,\n",
    "    jsonl_to_csv,\n",
    ")\n",
    "from harmonization.harmonization_benchmark import get_metrics_for_approach\n",
    "from harmonization.harmonization_approaches.similarity_inmem import (\n",
    "    SimilaritySearchInMemoryVectorDb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_filepath = (\n",
    "    # Synthetic Benchmark\n",
    "    # \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2/output.jsonl\"\n",
    "    # Real Benchmark\n",
    "    \"../datasets/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jsonls_per_target_model_dir_path = (\n",
    "    f\"../output/temp/{os.path.basename(benchmark_filepath)}/per_target\"\n",
    ")\n",
    "split_harmonization_jsonl_by_input_target_model(\n",
    "    benchmark_filepath, output_jsonls_per_target_model_dir_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "> Warning: The next cells may take **a very long time** (depending on the input dataset and how many target models exist) and it may take a lot of CPU/GPU the first time you run it (took me 32 minutes on an M3 Mac). Could take just **a long time** (took me 20 minutes on an M3 Mac) on future runs. It's embedding every single target data model into a persistent vectorstore on disk (and loaded in mem) as it goes the first time. And then every run it's embedding all the test case `node.property` and doing similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = time.time()\n",
    "output_directory = \"./output/harmonization/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(output_jsonls_per_target_model_dir_path):\n",
    "    benchmark_filepath = os.path.join(output_jsonls_per_target_model_dir_path, file)\n",
    "    print(f\"Opening {benchmark_filepath}...\")\n",
    "    output_filepath = f\"{output_directory}/{folder_name}/{file}\"\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "\n",
    "    # since these files are separated by target model already, just get the first row\n",
    "    input_target_model = None\n",
    "    with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                input_target_model = json.loads(row[\"input_target_model\"])\n",
    "            except Exception:\n",
    "                input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "            break\n",
    "\n",
    "    # :62 b/c of limitation on chromadb collection names\n",
    "    harmonization_approach = SimilaritySearchInMemoryVectorDb(\n",
    "        vectordb_persist_directory_name=f\"{file[:62]}\",\n",
    "        input_target_model=input_target_model\n",
    "    )\n",
    "\n",
    "    max_suggestions_per_property = 5\n",
    "    output_filename = get_metrics_for_approach(\n",
    "        benchmark_filepath,\n",
    "        harmonization_approach,\n",
    "        output_filepath,\n",
    "        k=max_suggestions_per_property,\n",
    "        metrics_column_name=\"custom_metrics\",\n",
    "        output_sssom_per_row=True,\n",
    "        output_tsvs_per_row=True,\n",
    "        output_expected_results_per_row=True,\n",
    "    )\n",
    "    print(f\"Output metrics to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Example conversation to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = output_filename.replace(\".jsonl\", \".csv\")\n",
    "jsonl_to_csv(jsonl_path=output_filename, csv_path=csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_metrics(jsonl_file):\n",
    "    \"\"\"\n",
    "    Visualizes metrics from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        jsonl_file (str): Path to the JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    row_numbers = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                if \"custom_metrics\" in json_data:\n",
    "                    data.append(json_data[\"custom_metrics\"])\n",
    "                    row_numbers.append(i + 1)  # Row numbers start at 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data found in the JSONL file.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # --- Overall Accuracy ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(row_numbers, df[\"recall\"])\n",
    "    mean_accuracy = df[\"recall\"].mean()\n",
    "    plt.axhline(\n",
    "        y=mean_accuracy, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_accuracy:.2f}\"\n",
    "    )\n",
    "    plt.xlabel(\"Row Number\")\n",
    "    plt.ylabel(\"Overall Accuracy (Recall)\")\n",
    "    plt.title(\"Overall Accuracy (Recall) by Row Number\")\n",
    "    plt.xticks(row_numbers, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Precision, Recall, F1-Score ---\n",
    "    metrics = [\"precision\", \"f1_score\"]\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(row_numbers, df[metric])\n",
    "        mean_metric = df[metric].mean()\n",
    "        plt.axhline(\n",
    "            y=mean_metric, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_metric:.2f}\"\n",
    "        )\n",
    "        plt.xlabel(\"Row Number\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f\"{metric.capitalize()} by Row Number\")\n",
    "        plt.xticks(row_numbers, rotation=90)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_metrics(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We expect High Recall, Low Precision above, because our approaches are providing *suggestions* for a final mapping and we're expecting an expert in the loop to select the right one. In other words, we expect a lot of false positives and that's okay.\n",
    "\n",
    "So we should only weigh heavily the \"recall\" / overall accuracy. Precision would be ideally high, but tuning for that might result in the right option not being presented at all, so we should not focus too heavily on that. Similarly, the F1 score provides a harmonic mean of precision and recall but we expect that to be low b/c precision is low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
