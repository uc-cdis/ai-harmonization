{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Harmonization Approach Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install package manager and sync required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from harmonization.jsonl import (\n",
    "    split_harmonization_jsonl_by_input_target_model,\n",
    "    jsonl_to_csv,\n",
    ")\n",
    "from harmonization.harmonization_benchmark import get_metrics_for_approach\n",
    "from harmonization.harmonization_approaches.similarity_inmem import (\n",
    "    SimilaritySearchInMemoryVectorDb,\n",
    ")\n",
    "from harmonization.harmonization_approaches.embeddings import (\n",
    "    MedGemmaEmbeddings,\n",
    "    QwenEmbeddings,\n",
    "    BGEEmbeddings,\n",
    ")\n",
    "from harmonization.simple_data_model import SimpleDataModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Set available GPUs (skip this step is using CPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # change as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Tests are in a JSONL file with a source to target model per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Below we are reading synthetic benchmark file (710 lines) or real benchmark file (3 lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_filepath = (\n",
    "    # Synthetic Benchmark\n",
    "    # \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2.jsonl\"\n",
    "    # Real Benchmark\n",
    "    \"../datasets/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jsonls_per_target_model_dir_path = (\n",
    "    f\"../output/temp/{os.path.basename(benchmark_filepath)}/per_target\"\n",
    ")\n",
    "split_harmonization_jsonl_by_input_target_model(\n",
    "    benchmark_filepath, output_jsonls_per_target_model_dir_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_object = datetime.datetime.fromtimestamp(time.time())\n",
    "folder_name = f\"{datetime_object.date()}_{os.path.splitext(os.path.basename(benchmark_filepath))[0]}\"\n",
    "\n",
    "output_directory = \"./output/harmonization/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Choose sentence-transformers, Medgemma or Qwen embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    " Here are links to the model that might be used for embeddings:\n",
    "\n",
    " * sentence-transformers model (default model, 768-dimension): https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    " * Qwen model (1024-dimension): https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\n",
    " * MedGemma model (2560-dimension): https://huggingface.co/google/medgemma-4b-it\n",
    " * EmbeddingGemma model (768-dimension): https://huggingface.co/google/embeddinggemma-300m \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "> Please note: You might need to get access prior to using MedGemma or EmbeddingGemma models and you need use your HF_TOKEN inside this notebook to allow it to connect to the model. In case you want to use Medgemma or EmbeddingGemma models, please uncomment the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code and ensure it works if your model requires authorization via HuggingFace token\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Choose desired embedding by uncommenting a line, and configure batch size.\n",
    "\n",
    "> Tip: if you are using GPUs and getting Out of Memory error, try setting smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# embedding_fn = QwenEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/medgemma-4b-it\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/embeddinggemma-300m\")\n",
    "embedding_fn = BGEEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Optional - test embeddings on small text inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"heart disease\"\n",
    "embedded_text = embedding_fn.embed_query(text)\n",
    "print(\"Embedded text:\", embedded_text)\n",
    "print(f\"Embedding dimension {len(embedded_text)}\")\n",
    "del embedded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> Warning: The next cells may take **a long time** and a lot of CPU/GPU when you run it. It's embedding every single target data model into a persistent vectorstore on disk (and loaded in mem) as it goes the first time. And then every run it's embedding all the test case `node.property` and doing similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove future warning from pandas\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "for file in os.listdir(output_jsonls_per_target_model_dir_path):\n",
    "    benchmark_filepath = os.path.join(output_jsonls_per_target_model_dir_path, file)\n",
    "    print(f\"Opening {benchmark_filepath}...\")\n",
    "    output_filepath = f\"{output_directory}/{folder_name}/{file}\"\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_filepath):\n",
    "        print(f\"File {output_filepath} already exists, SKIPPING...\")\n",
    "        continue\n",
    "\n",
    "    # since these files are separated by target model already, just get the first row\n",
    "    input_target_model = \"\"\n",
    "    with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                input_target_model = json.loads(row[\"input_target_model\"])\n",
    "            except Exception:\n",
    "                input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "            print(\"Input target model received\")\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        SimpleDataModel.get_from_unknown_json_format(json.dumps(input_target_model))\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "        print(\"Could not parse target model. SKIPPING...\")\n",
    "        continue\n",
    "\n",
    "    # :62 b/c of limitation on chromadb collection names\n",
    "    harmonization_approach = SimilaritySearchInMemoryVectorDb(\n",
    "        vectordb_persist_directory_name=f\"{file[:53]}-{embedding_fn.model.name_or_path.split(\"/\")[-1][:5]}-0\",\n",
    "        input_target_model=SimpleDataModel.get_from_unknown_json_format(\n",
    "            json.dumps(input_target_model)\n",
    "        ),\n",
    "        embedding_function=embedding_fn,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print(\"Input target model added to vectorstore\")\n",
    "\n",
    "    max_suggestions_per_property = 5\n",
    "    # max_suggestions_per_property = len(harmonization_approach.vectorstore.get()[\"ids\"])\n",
    "\n",
    "    # set threshold low to just get top properties no matter what\n",
    "    score_threshold = 0\n",
    "\n",
    "    output_filename = get_metrics_for_approach(\n",
    "        benchmark_filepath,\n",
    "        harmonization_approach,\n",
    "        output_filepath,\n",
    "        k=max_suggestions_per_property,\n",
    "        score_threshold=score_threshold,\n",
    "        metrics_column_name=\"custom_metrics\",\n",
    "        output_sssom_per_row=True,\n",
    "        output_tsvs_per_row=True,\n",
    "        output_expected_results_per_row=True,\n",
    "    )\n",
    "    print(f\"Output metrics to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Optional - empty GPU cache if GPU used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# del embedding_fn\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Get all the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filenames = []\n",
    "\n",
    "# adjust based on above output if necessary\n",
    "output_filepath = f\"{output_directory}/{folder_name}/\"\n",
    "\n",
    "for file in os.listdir(output_filepath):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        full_filepath = os.path.abspath(os.path.join(output_filepath, file))\n",
    "        output_filenames.append(full_filepath)\n",
    "\n",
    "print(f\"{len(output_filenames)} outputs found in {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Example conversation to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_filename in output_filenames:\n",
    "    csv_path = output_filename.replace(\".jsonl\", \".csv\")\n",
    "    jsonl_to_csv(jsonl_path=output_filename, csv_path=csv_path)\n",
    "    print(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Visualize Results Across All Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics_across_files(jsonl_files):\n",
    "    \"\"\"\n",
    "    Visualizes metrics across multiple JSONL files, with each file represented\n",
    "    on the x-axis and the bar height representing the mean of the metric for\n",
    "    that file. Adds a horizontal line for the overall mean.\n",
    "    Args:\n",
    "        jsonl_files (list): A list of paths to the JSONL files.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    filenames = []\n",
    "    for filename in jsonl_files:\n",
    "        shortened_filename = os.path.basename(filename)[:20]\n",
    "        data = []\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    json_data = json.loads(line)\n",
    "                    if \"custom_metrics\" in json_data:\n",
    "                        metrics = json_data[\"custom_metrics\"]\n",
    "                        metrics[\"filename\"] = shortened_filename\n",
    "                        data.append(metrics)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if not data:\n",
    "            print(f\"No data found in {filename}.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        all_data.append(df)\n",
    "        filenames.append(shortened_filename)\n",
    "\n",
    "    combined_df = pd.concat(all_data)\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "    metrics = [\"recall\", \"precision\", \"f1_score\"]\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        mean_values_per_file = [\n",
    "            combined_df[combined_df[\"filename\"] == filename][metric].mean()\n",
    "            for filename in filenames\n",
    "        ]\n",
    "\n",
    "        plt.bar(filenames, mean_values_per_file)\n",
    "\n",
    "        overall_mean = combined_df[metric].mean()\n",
    "\n",
    "        plt.axhline(\n",
    "            y=overall_mean,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Overall Mean: {overall_mean:.2f}\",\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\"Filename\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f\"Mean {metric.capitalize()} by Filename\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_metrics_across_files(output_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Visualize Results (More Details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust if you want to narrow down to specific file(s)\n",
    "output_filenames = output_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_custom_metrics(input_filenames):\n",
    "    \"\"\"\n",
    "    Opens JSONL files, extracts the 'custom_metrics' field from each line,\n",
    "    and writes the data to a Pandas DataFrame that is returned.  Includes\n",
    "    the filename as a column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "      input_filenames: A list of paths to the input JSONL files.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for filename in input_filenames:\n",
    "        with open(filename, \"r\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                try:\n",
    "                    json_data = json.loads(line)\n",
    "                    if \"custom_metrics\" in json_data:\n",
    "                        metrics = json_data[\"custom_metrics\"]\n",
    "                        metrics[\"shortened_filename\"] = os.path.basename(filename)[:20]\n",
    "                        metrics[\"row\"] = i + 1\n",
    "                        all_data.append(metrics)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "df = extract_custom_metrics(output_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Mappings Per Row\\n\")\n",
    "for i, row in enumerate(df.missing_mappings):\n",
    "    if not row:\n",
    "        continue\n",
    "\n",
    "    original_file = df[\"shortened_filename\"][i]\n",
    "    original_row_number = df[\"row\"][i]\n",
    "    print(f\"File {original_file}, Row #{original_row_number}\")\n",
    "    for item in sorted(row):\n",
    "        if len(item) > 80:\n",
    "            # print long items on their own line to ensure whole line is visible\n",
    "            print(item)\n",
    "        else:\n",
    "            parts = item.split(\" -> \")\n",
    "            if len(parts) == 2:\n",
    "                left, right = parts\n",
    "                print(f\"{left: <50} -> {right}\")\n",
    "            else:\n",
    "                print(item)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Visualize Row-Level Metrics for a Specific Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust as needed\n",
    "output_filename = output_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(jsonl_file):\n",
    "    \"\"\"\n",
    "    Visualizes metrics from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        jsonl_file (str): Path to the JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    row_numbers = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                if \"custom_metrics\" in json_data:\n",
    "                    data.append(json_data[\"custom_metrics\"])\n",
    "                    row_numbers.append(i + 1)  # Row numbers start at 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data found in the JSONL file.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # --- Overall Accuracy ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(row_numbers, df[\"recall\"])\n",
    "    mean_accuracy = df[\"recall\"].mean()\n",
    "    plt.axhline(\n",
    "        y=mean_accuracy, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_accuracy:.2f}\"\n",
    "    )\n",
    "    plt.xlabel(\"Row Number\")\n",
    "    plt.ylabel(\"Overall Accuracy (Recall)\")\n",
    "    plt.title(\"Overall Accuracy (Recall) by Row Number\")\n",
    "    plt.xticks(row_numbers, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Precision, Recall, F1-Score ---\n",
    "    metrics = [\"precision\", \"f1_score\"]\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(row_numbers, df[metric])\n",
    "        mean_metric = df[metric].mean()\n",
    "        plt.axhline(\n",
    "            y=mean_metric, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_metric:.2f}\"\n",
    "        )\n",
    "        plt.xlabel(\"Row Number\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f\"{metric.capitalize()} by Row Number\")\n",
    "        plt.xticks(row_numbers, rotation=90)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_metrics(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## A Note On Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "We expect High Recall, Low Precision above, because our approaches are providing *suggestions* for a final mapping and we're expecting an expert in the loop to select the right one. In other words, we expect a lot of false positives and that's okay.\n",
    "\n",
    "So we should only weigh heavily the \"recall\" / overall accuracy. Precision would be ideally high, but tuning for that might result in the right option not being presented at all, so we should not focus too heavily on that. Similarly, the F1 score provides a harmonic mean of precision and recall but we expect that to be low b/c precision is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
