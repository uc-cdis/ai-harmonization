{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Harmonization Approach Using Abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install package manager and sync required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from harmonization.jsonl import (\n",
    "    split_harmonization_jsonl_by_input_target_model,\n",
    "    jsonl_to_csv,\n",
    ")\n",
    "from harmonization.harmonization_benchmark import get_metrics_for_approach\n",
    "from harmonization.harmonization_approaches.similarity_inmem import (\n",
    "    SimilaritySearchInMemoryVectorDb,\n",
    ")\n",
    "from harmonization.harmonization_approaches.embeddings import (\n",
    "    MedGemmaEmbeddings,\n",
    "    QwenEmbeddings\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Set available GPUs (skip this step is using CPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # change as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Tests are in a JSONL file with a source to target model per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Below we are reading `output.jsonl` file that contains 710 lines or `limited_output.jsonl` file that contains 10 first lines from `output.jsonl` file.\n",
    "\n",
    "`limited_output.jsonl` might be useful for testing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_filepath = (\n",
    "    # Synthetic Benchmark\n",
    "    # \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2/output.jsonl\"\n",
    "    # Real Benchmark\n",
    "    \"../datasets/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jsonls_per_target_model_dir_path = (\n",
    "    f\"../output/temp/{os.path.basename(benchmark_filepath)}/per_target\"\n",
    ")\n",
    "split_harmonization_jsonl_by_input_target_model(\n",
    "    benchmark_filepath, output_jsonls_per_target_model_dir_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = time.time()\n",
    "output_directory = \"./output/harmonization/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Choose sentence-transformers, Medgemma or Qwen embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    " Here are links to the model that might be used for embeddings:\n",
    "\n",
    " * sentence-transformers model (default model, 768-dimension): https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    " * Qwen model (1024-dimension): https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\n",
    " * MedGemma model (2560-dimension): https://huggingface.co/google/medgemma-4b-it\n",
    " * EmbeddingGemma model (768-dimension): https://huggingface.co/google/embeddinggemma-300m \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "> Please note: You might need to get access prior to using MedGemma or EmbeddingGemma models and you need use your HF_TOKEN inside this notebook to allow it to connect to the model. In case you want to use Medgemma or EmbeddingGemma models, please uncomment the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code and ensure it works if your model requires authorization via HuggingFace token\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Choose desired embedding by uncommenting a line, and configure batch size.\n",
    "\n",
    "> Tip: if you are using GPUs and getting Out of Memory error, try setting smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embedding_fn = QwenEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/medgemma-4b-it\")\n",
    "# embedding_fn = MedGemmaEmbeddings(model_name=\"google/embeddinggemma-300m\")\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Optional - test embeddings on small text inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"heart disease\"\n",
    "#embedded_text = embedding_fn.embed_query(text)\n",
    "#print(\"Embedded text:\", embedded_text)\n",
    "#print(\"Embedding dimension\": len(embedded_text))\n",
    "#del embedded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> Warning: The next cells may take **a long time** and a lot of CPU/GPU when you run it. It's embedding every single target data model into a persistent vectorstore on disk (and loaded in mem) as it goes the first time. And then every run it's embedding all the test case `node.property` and doing similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove future warning from pandas\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "for file in os.listdir(output_jsonls_per_target_model_dir_path):\n",
    "    benchmark_filepath = os.path.join(output_jsonls_per_target_model_dir_path, file)\n",
    "    print(f\"Opening {benchmark_filepath}...\")\n",
    "    output_filepath = f\"{output_directory}/{folder_name}/{file}\"\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "\n",
    "    # since these files are separated by target model already, just get the first row\n",
    "    input_target_model = None\n",
    "    with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                input_target_model = json.loads(row[\"input_target_model\"])\n",
    "            except Exception:\n",
    "                input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "            break\n",
    "    print(\"Input target model received\")\n",
    "\n",
    "    # :62 b/c of limitation on chromadb collection names\n",
    "    harmonization_approach = SimilaritySearchInMemoryVectorDb(\n",
    "        vectordb_persist_directory_name=f\"{file[:53]}-{embedding_fn.model.name_or_path.split(\"/\")[-1][:5]}-0\",\n",
    "        input_target_model=input_target_model,\n",
    "        embedding_function=embedding_fn,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print(\"Input target model added to vectorstore\")\n",
    "\n",
    "    max_suggestions_per_property = 5\n",
    "    output_filename = get_metrics_for_approach(\n",
    "        benchmark_filepath,\n",
    "        harmonization_approach,\n",
    "        output_filepath,\n",
    "        k=max_suggestions_per_property,\n",
    "        metrics_column_name=\"custom_metrics\",\n",
    "        output_sssom_per_row=True,\n",
    "        output_tsvs_per_row=True,\n",
    "        output_expected_results_per_row=True,\n",
    "    )\n",
    "    print(f\"Output metrics to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Optional - empty GPU cache if GPU used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#del embedding_fn\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Example conversation to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = output_filename.replace(\".jsonl\", \".csv\")\n",
    "jsonl_to_csv(jsonl_path=output_filename, csv_path=csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_metrics(jsonl_file):\n",
    "    \"\"\"\n",
    "    Visualizes metrics from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        jsonl_file (str): Path to the JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    row_numbers = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                if \"custom_metrics\" in json_data:\n",
    "                    data.append(json_data[\"custom_metrics\"])\n",
    "                    row_numbers.append(i + 1)  # Row numbers start at 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data found in the JSONL file.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # --- Overall Accuracy ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(row_numbers, df[\"recall\"])\n",
    "    mean_accuracy = df[\"recall\"].mean()\n",
    "    plt.axhline(\n",
    "        y=mean_accuracy, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_accuracy:.2f}\"\n",
    "    )\n",
    "    plt.xlabel(\"Row Number\")\n",
    "    plt.ylabel(\"Overall Accuracy (Recall)\")\n",
    "    plt.title(\"Overall Accuracy (Recall) by Row Number\")\n",
    "    plt.xticks(row_numbers, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Precision, Recall, F1-Score ---\n",
    "    metrics = [\"precision\", \"f1_score\"]\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(row_numbers, df[metric])\n",
    "        mean_metric = df[metric].mean()\n",
    "        plt.axhline(\n",
    "            y=mean_metric, color=\"r\", linestyle=\"--\", label=f\"Mean: {mean_metric:.2f}\"\n",
    "        )\n",
    "        plt.xlabel(\"Row Number\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f\"{metric.capitalize()} by Row Number\")\n",
    "        plt.xticks(row_numbers, rotation=90)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_metrics(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We expect High Recall, Low Precision above, because our approaches are providing *suggestions* for a final mapping and we're expecting an expert in the loop to select the right one. In other words, we expect a lot of false positives and that's okay.\n",
    "\n",
    "So we should only weigh heavily the \"recall\" / overall accuracy. Precision would be ideally high, but tuning for that might result in the right option not being presented at all, so we should not focus too heavily on that. Similarly, the F1 score provides a harmonic mean of precision and recall but we expect that to be low b/c precision is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
