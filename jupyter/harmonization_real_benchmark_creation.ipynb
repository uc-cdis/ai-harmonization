{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create Harmonization Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Requires data from: https://uchicago.app.box.com/folder/341601784541 contents in a `real_benchmarks` folder. Put the \"combined_data_models\" into a separate folder `./real_benchmarks/_combo_models`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mutated real data dicts: https://uchicago.app.box.com/folder/320103463138?s=a58qiodbv0grnxq0h94ronz5qa5xlmpc\n",
    "# and place as input_dir\n",
    "input_combo_models_dir = os.path.abspath(\"../real_benchmarks/_combo_models\")\n",
    "input_source_models_dir = os.path.abspath(\"../real_benchmarks/source_models\")\n",
    "target_model_path = os.path.abspath(\"./examples/example_target_model_BDC.json\")\n",
    "output_dir = os.path.abspath(\n",
    "    \"../datasets/harmonization_benchmark_real_BIOLINCC_BDC_v0.0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually fill this out, maps from the source model to the corresponding combined model\n",
    "source_model_to_combo_model = {\n",
    "    f\"{input_source_models_dir}/source_model_phs003738.v1.p1.c1_BioLINCC-BL_ARIC_HMB-NPU-MDS.json\": f\"{input_combo_models_dir}/combined_data_model__phs003738.v1.p1.c1_BioLINCC-BL_ARIC_HMB-NPU-MDS.json\",\n",
    "    f\"{input_source_models_dir}/source_model__phs003948.v1.p1.c1_Individual_Study-PATH_HHT_DS-HHT-IRB-PUB-COL.json\": f\"{input_combo_models_dir}/combined_data_model__phs003948.v1.p1.c1_Individual_Study-PATH_HHT_DS-HHT-IRB-PUB-COL.json\",\n",
    "    f\"{input_source_models_dir}/source_model__phs004055.v1.p1.c1_BioLINCC-BL_CONCERT_HF_GRU.json\": f\"{input_combo_models_dir}/combined_data_model__phs004055.v1.p1.c1_BioLINCC-BL_CONCERT_HF_GRU.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_combo_model(source_model_to_combo_model, target_model_path, output_dir):\n",
    "    # right now this assumes a single target\n",
    "    with open(target_model_path) as input_file:\n",
    "        target_model = json.load(input_file)\n",
    "\n",
    "    for source_model_path, combo_model_path in source_model_to_combo_model.items():\n",
    "        with open(source_model_path) as input_file:\n",
    "            source_model = json.load(input_file)\n",
    "        with open(combo_model_path) as input_file:\n",
    "            combo_model = json.load(input_file)\n",
    "\n",
    "        phs_id = (\n",
    "            f\"phs\"\n",
    "            + os.path.basename(source_model_path).split(\"phs\")[1].split(\".json\")[0]\n",
    "        )\n",
    "        output_subdir = os.path.join(output_dir, phs_id)\n",
    "        os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "        # write the source model to the output directory\n",
    "        with open(os.path.join(output_subdir, \"source_model.json\"), \"w\") as output_file:\n",
    "            json.dump(source_model, output_file, indent=2)\n",
    "\n",
    "        # write the target model to the output directory\n",
    "        with open(os.path.join(output_subdir, \"target_model.json\"), \"w\") as output_file:\n",
    "            json.dump(target_model, output_file, indent=2)\n",
    "\n",
    "        # create a TSV file for the expected mappings\n",
    "        tsv_path = os.path.join(output_subdir, \"expected_mappings.tsv\")\n",
    "        with open(tsv_path, \"w\", newline=\"\") as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter=\"\\t\", quotechar='\"')\n",
    "            writer.writerow(\n",
    "                [\"source_node_prop_type_desc\", \"target_node_prop_type_desc\"]\n",
    "            )\n",
    "\n",
    "            for source_node in source_model[\"nodes\"]:\n",
    "                for source_prop in source_node[\"properties\"]:\n",
    "\n",
    "                    # handle malformated names in input\n",
    "                    source_node_name = source_node.get(\n",
    "                        \"name\", source_node.get(\"name:\", \"\")\n",
    "                    )\n",
    "                    source_prop_name = source_prop.get(\n",
    "                        \"name\", source_prop.get(\"name:\", \"\")\n",
    "                    )\n",
    "                    source_desc = f\"{source_node_name}.{source_prop_name} ({source_prop['type']}): {source_prop['description']}\"\n",
    "\n",
    "                    for combo_node in combo_model[\"nodes\"]:\n",
    "                        for combo_prop in combo_node[\"properties\"]:\n",
    "                            if (\n",
    "                                combo_prop.get(\"name\") == source_prop.get(\"name\")\n",
    "                                and combo_prop.get(\"description\")\n",
    "                                == source_prop.get(\"description\")\n",
    "                                and combo_prop.get(\"type\") == source_prop.get(\"type\")\n",
    "                                and \"gen3_name\" in combo_prop\n",
    "                                and \"gen3_description\" in combo_prop\n",
    "                                and \"gen3_type\" in combo_prop\n",
    "                            ):\n",
    "                                target_desc = f\"{combo_node['name']}.{combo_prop['gen3_name']} ({combo_prop['gen3_type']}): {combo_prop['gen3_description']}\"\n",
    "                                writer.writerow([source_desc, target_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_combo_model(\n",
    "    source_model_to_combo_model=source_model_to_combo_model,\n",
    "    target_model_path=target_model_path,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- output_dir\n",
    "    - source_target_folder_0\n",
    "        - `source_model.json`\n",
    "        - `expected_mappings.tsv`\n",
    "        - `target_model.json`\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The Target Data Model represents a target data model to harmonize to. The expected mappings are known links between the source and target node properties (e.g. columns in tables). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_from_structure(root_dir, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Iterates through subfolders under root_dir and writes a single JSONL file\n",
    "    with input_source_model, input_target_model, harmonized_mapping fields.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # find the first source_model.json file in this directory\n",
    "        print(f\"Current dir: {dirpath}\")\n",
    "        print(f\"Files in dir: {filenames}\")\n",
    "        source_model_files = glob.glob(os.path.join(dirpath, \"source_model*\"))\n",
    "        expected_mappings_path = os.path.join(dirpath, \"expected_mappings.tsv\")\n",
    "        target_model_files = glob.glob(os.path.join(dirpath, \"target_model*\"))\n",
    "        if (\n",
    "            source_model_files\n",
    "            and os.path.isfile(expected_mappings_path)\n",
    "            and target_model_files\n",
    "        ):\n",
    "            source_model_path = source_model_files[0]\n",
    "            target_model_path = target_model_files[0]\n",
    "            # Read files\n",
    "            with open(source_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_source_model = json.load(input_file)\n",
    "            with open(expected_mappings_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                harmonized_mapping = input_file.read()\n",
    "            with open(target_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_target_model = json.load(input_file)\n",
    "            record = {\n",
    "                \"input_source_model\": input_source_model,\n",
    "                \"input_target_model\": input_target_model,\n",
    "                \"harmonized_mapping\": harmonized_mapping,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    print(f\"Test count: {len(records)}\")\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for record in records:\n",
    "            output_file.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.path.join(output_dir, \"output.jsonl\")\n",
    "create_jsonl_from_structure(output_dir, output_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonization_data_jsonl_to_csv(jsonl_file, csv_file, input_headers=None):\n",
    "    \"\"\"\n",
    "    Converts a JSONL file to a CSV file.\n",
    "\n",
    "    Headers must include: `harmonized_mapping`\n",
    "\n",
    "    This denormalizes the harmonized mapping so each property mapped is its own row.\n",
    "    \"\"\"\n",
    "    input_headers = input_headers or [\n",
    "        \"input_source_model\",\n",
    "        \"input_target_model\",\n",
    "        \"harmonized_mapping\",\n",
    "    ]\n",
    "\n",
    "    if \"harmonized_mapping\" not in input_headers:\n",
    "        raise Exception(\"Headers must include: `harmonized_mapping`\")\n",
    "\n",
    "    input_headers.remove(\"harmonized_mapping\")\n",
    "    output_headers = copy.deepcopy(input_headers)\n",
    "    output_headers.extend(\n",
    "        [\n",
    "            \"source_node_prop_type_desc\",\n",
    "            \"target_node_prop_type_desc\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(jsonl_file, \"r\") as f_in, open(csv_file, \"w\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(output_headers)\n",
    "\n",
    "        for line in f_in:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    continue\n",
    "                for single_property_harmonized_mapping in data[\n",
    "                    \"harmonized_mapping\"\n",
    "                ].split(\"\\n\")[1:]:\n",
    "                    if not single_property_harmonized_mapping:\n",
    "                        continue\n",
    "                    source_node_prop_type_desc, target_node_prop_type_desc = (\n",
    "                        single_property_harmonized_mapping.split(\"\\t\")\n",
    "                    )\n",
    "                    row = []\n",
    "                    for header in input_headers:\n",
    "                        if header == \"harmonized_mapping\":\n",
    "                            continue\n",
    "                        row.append(data[header])\n",
    "                    row += [\n",
    "                        source_node_prop_type_desc,\n",
    "                        target_node_prop_type_desc,\n",
    "                    ]\n",
    "                    writer.writerow(row)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_data_jsonl_to_csv(\n",
    "    f\"{output_dir}/output.jsonl\",\n",
    "    f\"{output_dir}/output.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Validate Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "from ai_harmonization.simple_data_model import (\n",
    "    SimpleDataModel,\n",
    "    get_node_prop_type_desc_from_string,\n",
    ")\n",
    "\n",
    "benchmark_filepath = f\"{output_dir}/output.jsonl\"\n",
    "\n",
    "# since these files are separated by target model already, just get the first row\n",
    "input_target_model = \"\"\n",
    "with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        row = json.loads(line)\n",
    "        try:\n",
    "            input_target_model = json.loads(row[\"input_target_model\"])\n",
    "        except Exception:\n",
    "            input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "        try:\n",
    "            input_source_model = json.loads(row[\"input_source_model\"])\n",
    "        except Exception:\n",
    "            input_source_model = row[\"input_source_model\"]\n",
    "\n",
    "        target_model = SimpleDataModel.get_from_unknown_json_format(\n",
    "            json.dumps(input_target_model)\n",
    "        )\n",
    "        target_model_props_lookup = {}\n",
    "        for node in target_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                target_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        source_model = SimpleDataModel.from_simple_json(json.dumps(input_source_model))\n",
    "        source_model_props_lookup = {}\n",
    "        for node in source_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                source_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        print(\"Checking that all target props actually exist in target model...\")\n",
    "\n",
    "        harmonized_mapping = row[\"harmonized_mapping\"]\n",
    "\n",
    "        # Use io.StringIO to treat the string as a file-like object\n",
    "        tsv_file = io.StringIO(harmonized_mapping)\n",
    "        reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "\n",
    "        for node_prop_mapping in reader:\n",
    "            source_model_node_prop_type_desc, target_model_node_prop_type_desc = (\n",
    "                node_prop_mapping\n",
    "            )\n",
    "\n",
    "            source_node_name, source_prop_name, source_prop_type, source_prop_desc = (\n",
    "                get_node_prop_type_desc_from_string(source_model_node_prop_type_desc)\n",
    "            )\n",
    "            (\n",
    "                target_node_name,\n",
    "                target_prop_name,\n",
    "                target_prop_type,\n",
    "                target_prop_desc,\n",
    "            ) = get_node_prop_type_desc_from_string(target_model_node_prop_type_desc)\n",
    "\n",
    "            if (\n",
    "                f\"{target_node_name}.{target_prop_name}\".strip()\n",
    "                not in target_model_props_lookup\n",
    "            ):\n",
    "                print(\n",
    "                    f\"ERROR: {target_node_name}.{target_prop_name} is not in target_model\"\n",
    "                )\n",
    "                pass\n",
    "\n",
    "            if (\n",
    "                f\"{source_node_name}.{source_prop_name}\".strip()\n",
    "                not in source_model_props_lookup\n",
    "            ):\n",
    "                print(\n",
    "                    f\"ERROR: {source_node_name}.{source_prop_name} is not in source_model\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
