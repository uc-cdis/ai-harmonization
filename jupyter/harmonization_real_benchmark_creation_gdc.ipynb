{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create Harmonization Benchmark To GDC Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook utilizes data from the following paper:\n",
    "\n",
    "* Yurong Liu, Eduardo H. M. Pena, Aécio Santos, Eden Wu, and Juliana Freire. 2025. Magneto: Combining Small and Large Language Models for Schema Matching. Proc. VLDB Endow. 18, 8 (April 2025), 2681–2694. https://doi.org/10.14778/3742728.3742757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Set input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.abspath(\n",
    "    \"../datasets/harmonization_benchmark_real_GDC/inputs\"\n",
    ")\n",
    "\n",
    "output_dir = os.path.abspath(\n",
    "    \"../datasets/harmonization_benchmark_real_GDC/outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Get GDC Data Dictionary as a target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.gdc.cancer.gov/v0/submission/_dictionary/_all\"\n",
    "target_model_path = os.path.abspath(input_dir) + \"/target_model_GDC.json\"\n",
    "os.makedirs(os.path.dirname(target_model_path), exist_ok=True)\n",
    "!wget -q -O \"{target_model_path}\" \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Get 10 source CSVs from the paper as a source tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tables = [\n",
    "    (\"1MyQOryVm3S0iBz3-uqAC_bPqZjMtS6IA\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"1N3rbTHtnVDe19kMNei0opy_g-8Hr_Hl5\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1Ml-lY2LnAwpFpgHGeE7R2qqWRxBVLso9\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1Nac7mZR_reZPdK5zghI5Y3pEKq8VPTRQ\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1NIFT5dHcguZ1vzbQ_qz1tIhNDx-QENSe\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1MjNgXn-peUUaSadqIcWlqszECgxw7-ux\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"1ND-qu_62kGtz98O23AMFId4SHQX5GPzJ\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1NE13PtlXR6w2wRXyZrY6ar2lUXeLUw1-\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1MxEwZbz-31bQqM8ECIKnrClSQNTNwwpY\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1NgEsOT7jPdCll0Q3iQ_tuAMqe8L2XFBE\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_tables_path = os.path.abspath(input_dir) + \"/source_tables\"\n",
    "os.makedirs(source_tables_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_tables:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_tables_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Get 10 ground truth mappings CSVs from the paper as source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mappings = [\n",
    "    (\"1c64T1cq09T6WmOIIMGRO6yglIRBDDaYP\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"10pzRiZWuhE_jfNAm7D8XzKzM7ebJgbyj\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1vqL5HhFT6SxptQu7FyLidJnn2VKb4UMg\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1S0Fe2YlcqNhO1aFMwnePLjKVm1LPVDL8\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1Jy3FIE8jcrNiNlyXsoQIo86nfGVSeAsL\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1VS27jhKjNjxPnxn4SJt3OcvbMItYSfG2\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"107WFZ_-kCY-Yh9MGn1Fx1N93b23be27D\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1JY5fo4Tg3b_bgp-6JHPqweCiunjpWqPe\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1qZ_kOz9-iC8IzMSvdRHhZIc-mjrU-aSk\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1N8h2qwWBy8IO7QMx9ahkUE6vuhDdT6El\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_mappings_path = os.path.abspath(input_dir) + \"/source_mappings\"\n",
    "os.makedirs(source_mappings_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_mappings:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_mappings_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Format benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Some columns contain muliple numeric values seperated by semicolon (ex \"1;2;3\"), this function will extract numeric values from these strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_from_mixed(col_values):\n",
    "    flat = []\n",
    "    for val in col_values.dropna():\n",
    "        elements = str(val).split(\";\")\n",
    "        for x in elements:\n",
    "            try:\n",
    "                num = float(x)\n",
    "                flat.append(num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return pd.Series(flat, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This function creates JSONS from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_schema(df, bins=5, thresh_numeric=0.5, file_name=None):\n",
    "    schema = {\"type\": \"object\", \"properties\": []}\n",
    "    if file_name:\n",
    "        basename = os.path.splitext(file_name)[0]\n",
    "        schema[\"name\"] = basename\n",
    "    for col in df.columns:\n",
    "        col_info = {}\n",
    "        col_values = df[col]\n",
    "        numeric = pd.to_numeric(col_values, errors='coerce')\n",
    "        col_info['name'] = col\n",
    "        if numeric.notnull().sum() >= thresh_numeric * col_values.notnull().sum():\n",
    "            numeric_values = extract_numeric_from_mixed(col_values)\n",
    "            numeric_values = numeric_values.dropna().astype(float)\n",
    "            if len(numeric_values) > 1:\n",
    "                counts, bin_edges = np.histogram(numeric_values, bins=bins)\n",
    "                bin_mids = ((bin_edges[:-1] + bin_edges[1:]) / 2).tolist()\n",
    "            else:\n",
    "                bin_mids = numeric_values.tolist()\n",
    "                counts = [1] * len(numeric_values)\n",
    "            col_info['type'] = 'number'\n",
    "            col_info['histogram'] = {\n",
    "                \"bins\": bin_mids,\n",
    "                \"counts\": counts if isinstance(counts, list) else counts.tolist()\n",
    "            }\n",
    "        else:\n",
    "            value_list = []\n",
    "            for val in col_values.dropna():\n",
    "                if ';' in str(val):\n",
    "                    value_list.extend(str(val).split(';'))\n",
    "                elif '|' in str(val):\n",
    "                    value_list.extend(str(val).split('|'))\n",
    "                else:\n",
    "                    value_list.append(val)\n",
    "            counts = pd.Series(value_list).value_counts()\n",
    "            bins_ = counts.index.tolist()\n",
    "            values = counts.values.tolist()\n",
    "            col_info['type'] = 'string'\n",
    "            col_info['histogram'] = {\n",
    "                \"bins\": bins_,\n",
    "                \"counts\": values\n",
    "            }\n",
    "        schema[\"properties\"].append(col_info)\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Convert source CSVs to JSONs models and save source and target models in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"GDC\"\n",
    "\n",
    "for fname in os.listdir(source_tables_path):\n",
    "    source_name = os.path.splitext(fname)[0]\n",
    "    source_target_path = f\"{os.path.abspath(output_dir)}/{source_name}_{target_name}\"\n",
    "    os.makedirs(source_target_path, exist_ok=True)\n",
    "    if fname.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(source_tables_path, fname)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            schema = csv_to_schema(df, bins=5, file_name=fname)\n",
    "            json_path = os.path.join(source_target_path, f\"source_model.json\")\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(schema, f, indent=2)\n",
    "            print(f\"Source files processed and saved: {json_path}\")\n",
    "            target_path = shutil.copy(target_model_path, source_target_path + \"/target_model.json\")\n",
    "            print(f\"Target model copied to: {target_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Format mappings and save them in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously downloaded GDC dictionary\n",
    "with open(target_model_path, \"r\") as f:\n",
    "    gdc_dict = json.load(f)\n",
    "\n",
    "attribute_to_node = {}\n",
    "\n",
    "# Iterate all top-level keys (skip ones starting with \"_\", e.g. \"_definitions\")\n",
    "for node_name, node in gdc_dict.items():\n",
    "    if node_name.startswith(\"_\"):\n",
    "        continue              # these are not real data nodes\n",
    "    if \"properties\" in node:\n",
    "        for prop in node[\"properties\"].keys():\n",
    "            attribute_to_node[prop] = node_name\n",
    "\n",
    "# For debugging\n",
    "print(\"Sample mapping:\", dict(list(attribute_to_node.items())[:10]))\n",
    "\n",
    "for fname in os.listdir(source_mappings_path):\n",
    "    if fname.lower().endswith(\".csv\"):\n",
    "        source_name = os.path.splitext(fname)[0]\n",
    "        source_target_path = f\"{os.path.abspath(output_dir)}/{source_name}_{target_name}\"\n",
    "        os.makedirs(source_target_path, exist_ok=True)\n",
    "        df = pd.read_csv(os.path.join(source_mappings_path, fname))\n",
    "        src_col = [c for c in df.columns if \"original\" in c.lower()][0]\n",
    "        trg_col = [c for c in df.columns if \"gdc\" in c.lower()][0]\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            src = f\"{source_name}.{str(row[src_col]).strip()}\"\n",
    "            trg_var = str(row[trg_col]).strip()\n",
    "            node_prefix = attribute_to_node.get(trg_var, \"\")\n",
    "            trg = f\"{node_prefix}.{trg_var}\" if node_prefix else trg_var\n",
    "            rows.append({\"source_node_prop_type_desc\": src, \"target_node_prop_type_desc\": trg})\n",
    "        out_df = pd.DataFrame(rows)\n",
    "        out_tsv = os.path.join(source_target_path, f\"expected_mappings.tsv\")\n",
    "        out_df.to_csv(out_tsv, index=False, sep='\\t')\n",
    "        print(f\"Mappings processed and saved: {out_tsv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- output_dir\n",
    "    - source_target_folder_0\n",
    "        - `source_model.json`\n",
    "        - `expected_mappings.tsv`\n",
    "        - `target_model.json`\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File\n",
    "\n",
    "Code derived from harmonization_real_benchmark_creation.ipynb\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_from_structure(root_dir, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Iterates through subfolders under root_dir and writes a single JSONL file\n",
    "    with input_source_model, input_target_model, harmonized_mapping fields.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # find the first source_model.json file in this directory\n",
    "        print(f\"Current dir: {dirpath}\")\n",
    "        print(f\"Files in dir: {filenames}\")\n",
    "        source_model_files = glob.glob(os.path.join(dirpath, \"source_model*\"))\n",
    "        expected_mappings_path = os.path.join(dirpath, \"expected_mappings.tsv\")\n",
    "        target_model_files = glob.glob(os.path.join(dirpath, \"target_model*\"))\n",
    "        if (\n",
    "            source_model_files\n",
    "            and os.path.isfile(expected_mappings_path)\n",
    "            and target_model_files\n",
    "        ):\n",
    "            source_model_path = source_model_files[0]\n",
    "            target_model_path = target_model_files[0]\n",
    "            # Read files\n",
    "            with open(source_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_source_model = json.load(input_file)\n",
    "            with open(expected_mappings_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                harmonized_mapping = input_file.read()\n",
    "            with open(target_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_target_model = json.load(input_file)\n",
    "            record = {\n",
    "                \"input_source_model\": input_source_model,\n",
    "                \"input_target_model\": input_target_model,\n",
    "                \"harmonized_mapping\": harmonized_mapping,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    print(f\"Test count: {len(records)}\")\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for record in records:\n",
    "            output_file.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.path.join(output_dir, \"output.jsonl\")\n",
    "create_jsonl_from_structure(output_dir, output_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonization_data_jsonl_to_csv(jsonl_file, csv_file, input_headers=None):\n",
    "    \"\"\"\n",
    "    Converts a JSONL file to a CSV file.\n",
    "\n",
    "    Headers must include: `harmonized_mapping`\n",
    "\n",
    "    This denormalizes the harmonized mapping so each property mapped is its own row.\n",
    "    \"\"\"\n",
    "    input_headers = input_headers or [\n",
    "        \"input_source_model\",\n",
    "        \"input_target_model\",\n",
    "        \"harmonized_mapping\",\n",
    "    ]\n",
    "\n",
    "    if \"harmonized_mapping\" not in input_headers:\n",
    "        raise Exception(\"Headers must include: `harmonized_mapping`\")\n",
    "\n",
    "    input_headers.remove(\"harmonized_mapping\")\n",
    "    output_headers = copy.deepcopy(input_headers)\n",
    "    output_headers.extend(\n",
    "        [\n",
    "            \"source_node_prop_type_desc\",\n",
    "            \"target_node_prop_type_desc\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(jsonl_file, \"r\") as f_in, open(csv_file, \"w\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(output_headers)\n",
    "\n",
    "        for line in f_in:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    continue\n",
    "                for single_property_harmonized_mapping in data[\n",
    "                    \"harmonized_mapping\"\n",
    "                ].split(\"\\n\")[1:]:\n",
    "                    if not single_property_harmonized_mapping:\n",
    "                        continue\n",
    "                    source_node_prop_type_desc, target_node_prop_type_desc = (\n",
    "                        single_property_harmonized_mapping.split(\"\\t\")\n",
    "                    )\n",
    "                    row = []\n",
    "                    for header in input_headers:\n",
    "                        if header == \"harmonized_mapping\":\n",
    "                            continue\n",
    "                        row.append(data[header])\n",
    "                    row += [\n",
    "                        source_node_prop_type_desc,\n",
    "                        target_node_prop_type_desc,\n",
    "                    ]\n",
    "                    writer.writerow(row)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_data_jsonl_to_csv(\n",
    "    f\"{output_dir}/output.jsonl\",\n",
    "    f\"{output_dir}/output.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Validate Test File\n",
    "\n",
    "Code derived from harmonization_real_benchmark_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "from harmonization.simple_data_model import (\n",
    "    SimpleDataModel,\n",
    "    get_node_prop_type_desc_from_string,\n",
    ")\n",
    "\n",
    "benchmark_filepath = f\"{output_dir}/output.jsonl\"\n",
    "\n",
    "# since these files are separated by target model already, just get the first row\n",
    "input_target_model = \"\"\n",
    "with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        row = json.loads(line)\n",
    "        try:\n",
    "            input_target_model = json.loads(row[\"input_target_model\"])\n",
    "        except Exception:\n",
    "            input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "        try:\n",
    "            input_source_model = json.loads(row[\"input_source_model\"])\n",
    "        except Exception:\n",
    "            input_source_model = row[\"input_source_model\"]\n",
    "\n",
    "        target_model = SimpleDataModel.get_from_unknown_json_format(\n",
    "            json.dumps(input_target_model)\n",
    "        )\n",
    "        target_model_props_lookup = {}\n",
    "        for node in target_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                target_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        source_model = SimpleDataModel.from_simple_json(json.dumps(input_source_model))\n",
    "        source_model_props_lookup = {}\n",
    "        for node in source_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                source_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        print(\"Checking that all target props actually exist in target model...\")\n",
    "\n",
    "        harmonized_mapping = row[\"harmonized_mapping\"]\n",
    "\n",
    "        # Use io.StringIO to treat the string as a file-like object\n",
    "        tsv_file = io.StringIO(harmonized_mapping)\n",
    "        reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "\n",
    "        for node_prop_mapping in reader:\n",
    "            source_model_node_prop_type_desc, target_model_node_prop_type_desc = (\n",
    "                node_prop_mapping\n",
    "            )\n",
    "\n",
    "            source_node_name, source_prop_name, source_prop_type, source_prop_desc = (\n",
    "                get_node_prop_type_desc_from_string(source_model_node_prop_type_desc)\n",
    "            )\n",
    "            (\n",
    "                target_node_name,\n",
    "                target_prop_name,\n",
    "                target_prop_type,\n",
    "                target_prop_desc,\n",
    "            ) = get_node_prop_type_desc_from_string(target_model_node_prop_type_desc)\n",
    "\n",
    "            if (\n",
    "                f\"{target_node_name}.{target_prop_name}\".strip()\n",
    "                not in target_model_props_lookup\n",
    "            ):\n",
    "                print(\n",
    "                    f\"ERROR: {target_node_name}.{target_prop_name} is not in target_model\"\n",
    "                )\n",
    "                pass\n",
    "\n",
    "            if (\n",
    "                f\"{source_node_name}.{source_prop_name}\".strip()\n",
    "                not in source_model_props_lookup\n",
    "            ):\n",
    "                print(\n",
    "                    f\"ERROR: {source_node_name}.{source_prop_name} is not in source_model\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
