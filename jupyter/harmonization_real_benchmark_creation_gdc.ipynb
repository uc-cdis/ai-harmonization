{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create Harmonization Benchmark To GDC Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook utilizes data from the following paper:\n",
    "\n",
    "* Yurong Liu, Eduardo H. M. Pena, Aécio Santos, Eden Wu, and Juliana Freire. 2025. Magneto: Combining Small and Large Language Models for Schema Matching. Proc. VLDB Endow. 18, 8 (April 2025), 2681–2694. https://doi.org/10.14778/3742728.3742757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Set input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.abspath(\n",
    "    \"../datasets/harmonization_benchmark_real_GDC/inputs\"\n",
    ")\n",
    "\n",
    "output_dir = os.path.abspath(\n",
    "    \"../datasets/harmonization_benchmark_real_GDC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Get GDC Data Dictionary as a target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.gdc.cancer.gov/v0/submission/_dictionary/_all\"\n",
    "target_model_path = os.path.abspath(input_dir) + \"/target_model_GDC.json\"\n",
    "os.makedirs(os.path.dirname(target_model_path), exist_ok=True)\n",
    "!wget -q -O \"{target_model_path}\" \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Get 10 source CSVs from the paper as a source tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tables = [\n",
    "    (\"1MyQOryVm3S0iBz3-uqAC_bPqZjMtS6IA\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"1N3rbTHtnVDe19kMNei0opy_g-8Hr_Hl5\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1Ml-lY2LnAwpFpgHGeE7R2qqWRxBVLso9\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1Nac7mZR_reZPdK5zghI5Y3pEKq8VPTRQ\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1NIFT5dHcguZ1vzbQ_qz1tIhNDx-QENSe\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1MjNgXn-peUUaSadqIcWlqszECgxw7-ux\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"1ND-qu_62kGtz98O23AMFId4SHQX5GPzJ\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1NE13PtlXR6w2wRXyZrY6ar2lUXeLUw1-\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1MxEwZbz-31bQqM8ECIKnrClSQNTNwwpY\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1NgEsOT7jPdCll0Q3iQ_tuAMqe8L2XFBE\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_tables_path = os.path.abspath(input_dir) + \"/source_tables\"\n",
    "os.makedirs(source_tables_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_tables:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_tables_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Get 10 ground truth mappings CSVs from the paper as source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mappings = [\n",
    "    (\"1c64T1cq09T6WmOIIMGRO6yglIRBDDaYP\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"10pzRiZWuhE_jfNAm7D8XzKzM7ebJgbyj\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1vqL5HhFT6SxptQu7FyLidJnn2VKb4UMg\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1S0Fe2YlcqNhO1aFMwnePLjKVm1LPVDL8\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1Jy3FIE8jcrNiNlyXsoQIo86nfGVSeAsL\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1VS27jhKjNjxPnxn4SJt3OcvbMItYSfG2\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"107WFZ_-kCY-Yh9MGn1Fx1N93b23be27D\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1JY5fo4Tg3b_bgp-6JHPqweCiunjpWqPe\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1qZ_kOz9-iC8IzMSvdRHhZIc-mjrU-aSk\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1N8h2qwWBy8IO7QMx9ahkUE6vuhDdT6El\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_mappings_path = os.path.abspath(input_dir) + \"/source_mappings\"\n",
    "os.makedirs(source_mappings_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_mappings:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_mappings_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Format benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Some columns contain muliple numeric values seperated by semicolon (ex \"1;2;3\"), this function will extract numeric values from these strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_from_mixed(col_values):\n",
    "    flat = []\n",
    "    for val in col_values.dropna():\n",
    "        elements = str(val).split(\";\")\n",
    "        for x in elements:\n",
    "            try:\n",
    "                num = float(x)\n",
    "                flat.append(num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return pd.Series(flat, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This function creates JSONS from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_schema(df, bins=5, thresh_numeric=0.5, file_name=None):\n",
    "    schema = {\"type\": \"object\", \"properties\": []}\n",
    "    if file_name:\n",
    "        basename = os.path.splitext(file_name)[0]\n",
    "        schema[\"name\"] = basename\n",
    "    for col in df.columns:\n",
    "        col_info = {}\n",
    "        col_values = df[col]\n",
    "        numeric = pd.to_numeric(col_values, errors='coerce')\n",
    "        col_info['name'] = col\n",
    "        if numeric.notnull().sum() >= thresh_numeric * col_values.notnull().sum():\n",
    "            numeric_values = extract_numeric_from_mixed(col_values)\n",
    "            numeric_values = numeric_values.dropna().astype(float)\n",
    "            if len(numeric_values) > 1:\n",
    "                counts, bin_edges = np.histogram(numeric_values, bins=bins)\n",
    "                bin_mids = ((bin_edges[:-1] + bin_edges[1:]) / 2).tolist()\n",
    "            else:\n",
    "                bin_mids = numeric_values.tolist()\n",
    "                counts = [1] * len(numeric_values)\n",
    "            col_info['type'] = 'number'\n",
    "            col_info['histogram'] = {\n",
    "                \"bins\": bin_mids,\n",
    "                \"counts\": counts if isinstance(counts, list) else counts.tolist()\n",
    "            }\n",
    "        else:\n",
    "            value_list = []\n",
    "            for val in col_values.dropna():\n",
    "                if ';' in str(val):\n",
    "                    value_list.extend(str(val).split(';'))\n",
    "                elif '|' in str(val):\n",
    "                    value_list.extend(str(val).split('|'))\n",
    "                else:\n",
    "                    value_list.append(val)\n",
    "            counts = pd.Series(value_list).value_counts()\n",
    "            bins_ = counts.index.tolist()\n",
    "            values = counts.values.tolist()\n",
    "            col_info['type'] = 'string'\n",
    "            col_info['histogram'] = {\n",
    "                \"bins\": bins_,\n",
    "                \"counts\": values\n",
    "            }\n",
    "        schema[\"properties\"].append(col_info)\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Convert source CSVs to JSONs models and save source and target models in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"GDC\"\n",
    "\n",
    "for fname in os.listdir(source_tables_path):\n",
    "    source_name = os.path.splitext(fname)[0]\n",
    "    source_target_path = f\"{os.path.abspath(output_dir)}/{source_name}_{target_name}\"\n",
    "    os.makedirs(source_target_path, exist_ok=True)\n",
    "    if fname.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(source_tables_path, fname)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            schema = csv_to_schema(df, bins=5, file_name=fname)\n",
    "            json_path = os.path.join(source_target_path, f\"source_model.json\")\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(schema, f, indent=2)\n",
    "            print(f\"Source files processed and saved: {json_path}\")\n",
    "            target_path = shutil.copy(target_model_path, source_target_path + \"/target_model.json\")\n",
    "            print(f\"Target model copied to: {target_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Format mappings and save them in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load previously downloaded GDC dictionary\n",
    "with open(target_model_path, \"r\") as f:\n",
    "    gdc_dict = json.load(f)\n",
    "\n",
    "attribute_to_node = {}\n",
    "\n",
    "# Iterate all top-level keys (skip ones starting with \"_\", e.g. \"_definitions\")\n",
    "for node_name, node in gdc_dict.items():\n",
    "    if node_name.startswith(\"_\"):\n",
    "        continue              # these are not real data nodes\n",
    "    if \"properties\" in node:\n",
    "        for prop in node[\"properties\"].keys():\n",
    "            attribute_to_node[prop] = node_name\n",
    "\n",
    "# For debugging\n",
    "print(\"Sample mapping:\", dict(list(attribute_to_node.items())[:10]))\n",
    "\n",
    "for fname in os.listdir(source_mappings_path):\n",
    "    if fname.lower().endswith(\".csv\"):\n",
    "        source_name = os.path.splitext(fname)[0]\n",
    "        source_target_path = f\"{os.path.abspath(output_dir)}/{source_name}_{target_name}\"\n",
    "        os.makedirs(source_target_path, exist_ok=True)\n",
    "        df = pd.read_csv(os.path.join(source_mappings_path, fname))\n",
    "        src_col = [c for c in df.columns if \"original\" in c.lower()][0]\n",
    "        trg_col = [c for c in df.columns if \"gdc\" in c.lower()][0]\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            src = f\"{source_name}.{str(row[src_col]).strip()}\"\n",
    "            trg_var = str(row[trg_col]).strip()\n",
    "            node_prefix = attribute_to_node.get(trg_var, \"\")\n",
    "            trg = f\"{node_prefix}.{trg_var}\" if node_prefix else trg_var\n",
    "            rows.append({\"source_node_prop_type_desc\": src, \"target_node_prop_type_desc\": trg})\n",
    "        out_df = pd.DataFrame(rows)\n",
    "        out_csv = os.path.join(source_target_path, f\"expected_mappings.csv\")\n",
    "        out_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Mappings processed and saved: {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- output_dir\n",
    "    - source_target_folder_0\n",
    "        - `source_model.json`\n",
    "        - `expected_mappings.tsv`\n",
    "        - `target_model.json`\n",
    "    - ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
