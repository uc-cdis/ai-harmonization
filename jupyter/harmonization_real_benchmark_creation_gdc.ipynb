{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create Harmonization Benchmark To GDC Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook utilizes data from the following paper:\n",
    "\n",
    "* Yurong Liu, Eduardo H. M. Pena, Aécio Santos, Eden Wu, and Juliana Freire. 2025. Magneto: Combining Small and Large Language Models for Schema Matching. Proc. VLDB Endow. 18, 8 (April 2025), 2681–2694. https://doi.org/10.14778/3742728.3742757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import copy\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from harmonization.simple_data_model import (\n",
    "    SimpleDataModel,\n",
    "    get_node_property_as_string,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Set input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(\"../datasets/harmonization_benchmark_real_GDC_v0.0.1\")\n",
    "input_dir = os.path.abspath(root_dir) + \"/input_dir\"\n",
    "output_dir = os.path.abspath(root_dir) + \"/output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Get GDC Data Dictionary as a target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.gdc.cancer.gov/v0/submission/_dictionary/_all\"\n",
    "target_model_path = os.path.abspath(input_dir) + \"/target_model_GDC.json\"\n",
    "os.makedirs(os.path.dirname(target_model_path), exist_ok=True)\n",
    "!wget -q -O \"{target_model_path}\" \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Get 10 source CSVs from the paper as a source tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tables = [\n",
    "    (\"1MyQOryVm3S0iBz3-uqAC_bPqZjMtS6IA\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"1N3rbTHtnVDe19kMNei0opy_g-8Hr_Hl5\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1Ml-lY2LnAwpFpgHGeE7R2qqWRxBVLso9\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1Nac7mZR_reZPdK5zghI5Y3pEKq8VPTRQ\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1NIFT5dHcguZ1vzbQ_qz1tIhNDx-QENSe\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1MjNgXn-peUUaSadqIcWlqszECgxw7-ux\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"1ND-qu_62kGtz98O23AMFId4SHQX5GPzJ\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1NE13PtlXR6w2wRXyZrY6ar2lUXeLUw1-\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1MxEwZbz-31bQqM8ECIKnrClSQNTNwwpY\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1NgEsOT7jPdCll0Q3iQ_tuAMqe8L2XFBE\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_tables_path = os.path.abspath(input_dir) + \"/source_tables\"\n",
    "os.makedirs(source_tables_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_tables:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_tables_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Get 10 ground truth mappings CSVs from the paper as source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mappings = [\n",
    "    (\"1c64T1cq09T6WmOIIMGRO6yglIRBDDaYP\", \"Cao.csv\"), # pragma: allowlist secret\n",
    "    (\"10pzRiZWuhE_jfNAm7D8XzKzM7ebJgbyj\", \"Clark.csv\"), # pragma: allowlist secret\n",
    "    (\"1vqL5HhFT6SxptQu7FyLidJnn2VKb4UMg\", \"Dou.csv\"), # pragma: allowlist secret\n",
    "    (\"1S0Fe2YlcqNhO1aFMwnePLjKVm1LPVDL8\", \"Gilette.csv\"), # pragma: allowlist secret\n",
    "    (\"1Jy3FIE8jcrNiNlyXsoQIo86nfGVSeAsL\", \"Huang.csv\"), # pragma: allowlist secret\n",
    "    (\"1VS27jhKjNjxPnxn4SJt3OcvbMItYSfG2\", \"Krug.csv\"), # pragma: allowlist secret\n",
    "    (\"107WFZ_-kCY-Yh9MGn1Fx1N93b23be27D\", \"McDermott.csv\"), # pragma: allowlist secret\n",
    "    (\"1JY5fo4Tg3b_bgp-6JHPqweCiunjpWqPe\", \"Satpathy.csv\"), # pragma: allowlist secret\n",
    "    (\"1qZ_kOz9-iC8IzMSvdRHhZIc-mjrU-aSk\", \"Vasaikar.csv\"), # pragma: allowlist secret\n",
    "    (\"1N8h2qwWBy8IO7QMx9ahkUE6vuhDdT6El\", \"Wang.csv\") # pragma: allowlist secret\n",
    "]\n",
    "\n",
    "source_mappings_path = os.path.abspath(input_dir) + \"/source_mappings\"\n",
    "os.makedirs(source_mappings_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for id, name in source_mappings:\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={id}\"\n",
    "    !wget -q --no-check-certificate \"{url}\" -O \"{source_mappings_path}/{name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Format benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Some columns contain muliple numeric values seperated by semicolon (ex \"1;2;3\"), this function will extract numeric values from these strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_and_string_from_mixed(col_values, multi_value_delimiters=[\";\", \"|\"]):\n",
    "    \"\"\"\n",
    "    Splits cell values using provided delimiters and returns two lists: numerics and strings.\n",
    "\n",
    "    Args:\n",
    "        col_values (pd.Series): The column values to parse.\n",
    "        multi_value_delimiters (list): Delimiters for splitting each cell (e.g., [\";\", \"|\"]).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (numeric, string) where numeric contains both int and float objects,\n",
    "               and string contains non-numeric non-empty values.\n",
    "    \n",
    "    Example usage:\n",
    "        numeric, string = extract_numeric_and_string_from_mixed(df['column'], [\";\", \"|\"])\n",
    "    \"\"\"\n",
    "    pattern = \"|\".join([re.escape(sep) for sep in multi_value_delimiters])\n",
    "    numeric = []\n",
    "    string = []\n",
    "    for val in col_values.dropna():\n",
    "        elements = re.split(pattern, str(val))\n",
    "        for x in elements:\n",
    "            x = x.strip()\n",
    "            if x == \"\" or x.lower() == \"na\":\n",
    "                continue\n",
    "            try:\n",
    "                num = int(x)\n",
    "                numeric.append(num)\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    num = float(x)\n",
    "                    numeric.append(num)\n",
    "                except ValueError:\n",
    "                    string.append(x)\n",
    "    return numeric, string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This function creates JSONS from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_schema(csv_path, bins=5, multi_value_delimiters=[\";\", \"|\"], column_separator=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV or TSV file, analyzes each column for numeric and string values,\n",
    "    builds histograms for both, and outputs a schema with type and value counts.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV/TSV file.\n",
    "        bins (int): Number of bins for numeric histograms (used if more than 1 number in column).\n",
    "        multi_value_delimiters (list): Delimiters to split multi-valued cells (e.g., [\";\", \"|\"]).\n",
    "        column_separator (str, optional): Character to split columns; auto-detected by file extension if not provided.\n",
    "\n",
    "    Returns:\n",
    "        dict: A schema containing value histograms and type info for each column.\n",
    "\n",
    "    Example usage:\n",
    "        schema = csv_to_schema('data.csv', bins=5, multi_value_delimiters=[';', '|'])\n",
    "        schema = csv_to_schema('table.tsv', bins=10, multi_value_delimiters=[';', '|', '/'])\n",
    "        schema = csv_to_schema('custom.txt', column_separator=\"|\", multi_value_delimiters=[\";\"])\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(csv_path)\n",
    "    basename, ext = os.path.splitext(filename)\n",
    "    if not column_separator:\n",
    "        if ext.lower() == \".tsv\":\n",
    "            column_separator = \"\\t\"\n",
    "        else:\n",
    "            column_separator = \",\"\n",
    "\n",
    "    csv_schema = {\"type\": \"object\", \"name\": basename, \"properties\": []}\n",
    "    df = pd.read_csv(csv_path, sep=column_separator)\n",
    "    df.columns = [c.strip().replace('\\t', '') for c in df.columns]\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_info = {}\n",
    "        col_values = df[col]\n",
    "\n",
    "        numeric_raw, string_raw = extract_numeric_and_string_from_mixed(\n",
    "            col_values,\n",
    "            multi_value_delimiters=multi_value_delimiters\n",
    "        )\n",
    "        numeric_values = pd.Series(numeric_raw).dropna()\n",
    "        string_counts = pd.Series(string_raw).value_counts()\n",
    "        string_bins = string_counts.index.tolist()\n",
    "        string_values_counts = string_counts.values.tolist()\n",
    "\n",
    "\n",
    "        # Numeric histogram\n",
    "        if len(numeric_values) > 1:\n",
    "            numeric_counts, numeric_edges = np.histogram(numeric_values.astype(float), bins=bins)\n",
    "            # Create bin intervals as \"edge1-edge2\" strings\n",
    "            numeric_bins = [\n",
    "                f\"{numeric_edges[i]:.2f}-{numeric_edges[i+1]:.2f}\"\n",
    "                for i in range(len(numeric_edges) - 1)\n",
    "            ]\n",
    "            numeric_counts = [int(x) for x in numeric_counts]\n",
    "        elif len(numeric_values) == 1:\n",
    "            numeric_bins = [str(numeric_values.iloc[0])]\n",
    "            numeric_counts = [1]\n",
    "        else:\n",
    "            numeric_bins = []\n",
    "            numeric_counts = []\n",
    "\n",
    "        n_na = int(col_values.isnull().sum())\n",
    "\n",
    "        serializable_bins = numeric_bins + string_bins\n",
    "        serializable_counts = [int(x) if isinstance(x, (np.int64, np.int32, np.float64, np.float32)) else x for x in numeric_counts] + \\\n",
    "                             [int(x) if isinstance(x, (np.int64, np.int32, np.float64, np.float32)) else x for x in string_values_counts]\n",
    "        if n_na:\n",
    "            serializable_bins += [\"NA\"]\n",
    "            serializable_counts += [n_na]\n",
    "\n",
    "        # Construct schema\n",
    "        col_info['name'] = col\n",
    "        col_info['description'] = col\n",
    "        if not string_bins and numeric_bins:\n",
    "            col_info['type'] = \"number\"\n",
    "        elif not numeric_bins and string_bins:\n",
    "            col_info['type'] = \"string\"\n",
    "            # Add values that are not numbers to \"values\"\n",
    "            col_info[\"values\"] = string_bins\n",
    "        elif numeric_bins and string_bins:\n",
    "            col_info['type'] = 'mixed'\n",
    "            # Add values that are not numbers to \"values\"\n",
    "            col_info[\"values\"] = string_bins\n",
    "        else:\n",
    "            col_info['type'] = \"unknown\"\n",
    "        col_info['histogram'] = {\n",
    "            \"bins\": serializable_bins,\n",
    "            \"counts\": serializable_counts\n",
    "        }\n",
    "        csv_schema[\"properties\"].append(col_info)\n",
    "    schema = {\"nodes\": [csv_schema]}\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Convert source CSVs to JSONs models and save source and target models in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"GDC\"\n",
    "\n",
    "for fname in os.listdir(source_tables_path):\n",
    "    source_name = os.path.splitext(fname)[0]\n",
    "    source_target_path = f\"{os.path.abspath(output_dir)}/{source_name}_{target_name}\"\n",
    "    os.makedirs(source_target_path, exist_ok=True)\n",
    "    if fname.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(source_tables_path, fname)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            schema = csv_to_schema(csv_path, bins=5)\n",
    "            json_path = os.path.join(source_target_path, f\"source_model.json\")\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(schema, f, indent=2)\n",
    "            print(f\"Source files processed and saved: {json_path}\")\n",
    "            target_path = shutil.copy(target_model_path, source_target_path + \"/target_model.json\")\n",
    "            print(f\"Target model copied to: {target_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Format mappings and save them in {source}_{target} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prop_desc_lookup(data_model):\n",
    "    lookup = {}\n",
    "    for node in data_model.nodes:\n",
    "        for prop in node.properties:\n",
    "            desc = get_node_property_as_string(node, prop)\n",
    "            lookup[f\"{node.name}.{prop.name}\".strip()] = desc\n",
    "    return lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GDC target model\n",
    "with open(target_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    target_model_json = f.read()\n",
    "target_model = SimpleDataModel.get_from_unknown_json_format(target_model_json)\n",
    "target_prop_lookup = build_prop_desc_lookup(target_model)\n",
    "\n",
    "\n",
    "# For each mapping file, load corresponding source model and create descriptor mapping\n",
    "for fname in os.listdir(source_mappings_path):\n",
    "    if fname.lower().endswith(\".csv\"):\n",
    "        source_name = os.path.splitext(fname)[0]\n",
    "        source_target_path = os.path.join(os.path.abspath(output_dir), f\"{source_name}_{target_name}\")\n",
    "        os.makedirs(source_target_path, exist_ok=True)\n",
    "\n",
    "        # Try to find corresponding source model JSON\n",
    "        source_model_path = os.path.join(source_target_path, \"source_model.json\")\n",
    "        if not os.path.exists(source_model_path):\n",
    "            print(f\"WARNING: Missing source model for {source_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(source_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            source_model_json = f.read()\n",
    "        source_model = SimpleDataModel.from_simple_json(source_model_json)\n",
    "        source_prop_lookup = build_prop_desc_lookup(source_model)\n",
    "\n",
    "        # Read mappings CSV\n",
    "        df = pd.read_csv(os.path.join(source_mappings_path, fname))\n",
    "        src_col = [c for c in df.columns if \"original\" in c.lower()][0]\n",
    "        trg_col = [c for c in df.columns if \"gdc\" in c.lower()][0]\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            src_key = f\"{source_name}.{str(row[src_col]).strip()}\"\n",
    "            trg_key = str(row[trg_col]).strip()\n",
    "\n",
    "            src_desc = source_prop_lookup.get(src_key, src_key)\n",
    "            # For GDC target, find node.name from property\n",
    "            trg_node_name = None\n",
    "            for k in target_prop_lookup:\n",
    "                if k.endswith(f\".{trg_key}\"):\n",
    "                    trg_node_name = k.split(\".\")[0]\n",
    "                    break\n",
    "            if trg_node_name:\n",
    "                trg_desc = target_prop_lookup.get(f\"{trg_node_name}.{trg_key}\", f\"{trg_node_name}.{trg_key}\")\n",
    "            else:\n",
    "                trg_desc = trg_key\n",
    "\n",
    "            rows.append({\n",
    "                \"source_node_prop_type_desc\": src_desc,\n",
    "                \"target_node_prop_type_desc\": trg_desc\n",
    "            })\n",
    "        out_tsv = os.path.join(source_target_path, f\"expected_mappings.tsv\")\n",
    "        pd.DataFrame(rows).to_csv(out_tsv, sep='\\t', index=False)\n",
    "        print(f\"Mappings processed and saved: {out_tsv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- output_dir\n",
    "    - source_target_folder_0\n",
    "        - `source_model.json`\n",
    "        - `expected_mappings.tsv`\n",
    "        - `target_model.json`\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File\n",
    "\n",
    "Code derived from harmonization_real_benchmark_creation.ipynb\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `source_model.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `harmonized_mapping`\n",
    "- `target_model.json` == `input_target_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_from_structure(root_dir, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Iterates through subfolders under root_dir and writes a single JSONL file\n",
    "    with input_source_model, input_target_model, harmonized_mapping fields.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # find the first source_model.json file in this directory\n",
    "        print(f\"Current dir: {dirpath}\")\n",
    "        print(f\"Files in dir: {filenames}\")\n",
    "        source_model_files = glob.glob(os.path.join(dirpath, \"source_model*\"))\n",
    "        expected_mappings_path = os.path.join(dirpath, \"expected_mappings.tsv\")\n",
    "        target_model_files = glob.glob(os.path.join(dirpath, \"target_model*\"))\n",
    "        if (\n",
    "            source_model_files\n",
    "            and os.path.isfile(expected_mappings_path)\n",
    "            and target_model_files\n",
    "        ):\n",
    "            source_model_path = source_model_files[0]\n",
    "            target_model_path = target_model_files[0]\n",
    "            # Read files\n",
    "            with open(source_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_source_model = json.load(input_file)\n",
    "            with open(expected_mappings_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                harmonized_mapping = input_file.read()\n",
    "            with open(target_model_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "                input_target_model = json.load(input_file)\n",
    "            record = {\n",
    "                \"input_source_model\": input_source_model,\n",
    "                \"input_target_model\": input_target_model,\n",
    "                \"harmonized_mapping\": harmonized_mapping,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    print(f\"Test count: {len(records)}\")\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for record in records:\n",
    "            output_file.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.path.join(root_dir, \"harmonization_benchmark_real_GDC_v0.0.1.jsonl\")\n",
    "create_jsonl_from_structure(output_dir, output_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonization_data_jsonl_to_csv(jsonl_file, csv_file, input_headers=None):\n",
    "    \"\"\"\n",
    "    Converts a JSONL file to a CSV file.\n",
    "\n",
    "    Headers must include: `harmonized_mapping`\n",
    "\n",
    "    This denormalizes the harmonized mapping so each property mapped is its own row.\n",
    "    \"\"\"\n",
    "    input_headers = input_headers or [\n",
    "        \"input_source_model\",\n",
    "        \"input_target_model\",\n",
    "        \"harmonized_mapping\",\n",
    "    ]\n",
    "\n",
    "    if \"harmonized_mapping\" not in input_headers:\n",
    "        raise Exception(\"Headers must include: `harmonized_mapping`\")\n",
    "\n",
    "    input_headers.remove(\"harmonized_mapping\")\n",
    "    output_headers = copy.deepcopy(input_headers)\n",
    "    output_headers.extend(\n",
    "        [\n",
    "            \"source_node_prop_type_desc\",\n",
    "            \"target_node_prop_type_desc\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(jsonl_file, \"r\") as f_in, open(csv_file, \"w\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(output_headers)\n",
    "\n",
    "        for line in f_in:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    continue\n",
    "                for single_property_harmonized_mapping in data[\n",
    "                    \"harmonized_mapping\"\n",
    "                ].split(\"\\n\")[1:]:\n",
    "                    if not single_property_harmonized_mapping:\n",
    "                        continue\n",
    "                    source_node_prop_type_desc, target_node_prop_type_desc = (\n",
    "                        single_property_harmonized_mapping.split(\"\\t\")\n",
    "                    )\n",
    "                    row = []\n",
    "                    for header in input_headers:\n",
    "                        if header == \"harmonized_mapping\":\n",
    "                            continue\n",
    "                        row.append(data[header])\n",
    "                    row += [\n",
    "                        source_node_prop_type_desc,\n",
    "                        target_node_prop_type_desc,\n",
    "                    ]\n",
    "                    writer.writerow(row)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_data_jsonl_to_csv(\n",
    "    f\"{root_dir}/harmonization_benchmark_real_GDC_v0.0.1.jsonl\",\n",
    "    f\"{root_dir}/harmonization_benchmark_real_GDC_v0.0.1.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Validate Test File\n",
    "\n",
    "Code derived from harmonization_real_benchmark_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "from harmonization.simple_data_model import (\n",
    "    SimpleDataModel,\n",
    "    get_node_prop_type_desc_from_string,\n",
    ")\n",
    "\n",
    "benchmark_filepath = f\"{root_dir}/harmonization_benchmark_real_GDC_v0.0.1.jsonl\"\n",
    "\n",
    "# since these files are separated by target model already, just get the first row\n",
    "input_target_model = \"\"\n",
    "with open(benchmark_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        row = json.loads(line)\n",
    "        try:\n",
    "            input_target_model = json.loads(row[\"input_target_model\"])\n",
    "        except Exception:\n",
    "            input_target_model = row[\"input_target_model\"]\n",
    "\n",
    "        try:\n",
    "            input_source_model = json.loads(row[\"input_source_model\"])\n",
    "        except Exception:\n",
    "            input_source_model = row[\"input_source_model\"]\n",
    "\n",
    "        target_model = SimpleDataModel.get_from_unknown_json_format(\n",
    "            json.dumps(input_target_model)\n",
    "        )\n",
    "        target_model_props_lookup = {}\n",
    "        for node in target_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                target_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        source_model = SimpleDataModel.get_from_unknown_json_format(json.dumps(input_source_model))\n",
    "        source_model_props_lookup = {}\n",
    "        for node in source_model.nodes:\n",
    "            for node_property in node.properties:\n",
    "                source_model_props_lookup[\n",
    "                    f\"{node.name}.{node_property.name}\".strip()\n",
    "                ] = node_property\n",
    "\n",
    "        print(\"Checking that all target props actually exist in target model...\")\n",
    "\n",
    "        harmonized_mapping = row[\"harmonized_mapping\"]\n",
    "\n",
    "        # Use io.StringIO to treat the string as a file-like object\n",
    "        tsv_file = io.StringIO(harmonized_mapping)\n",
    "        reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "\n",
    "        for node_prop_mapping in reader:\n",
    "            source_model_node_prop_type_desc, target_model_node_prop_type_desc = (\n",
    "                node_prop_mapping\n",
    "            )\n",
    "            source_node_name, source_prop_name, source_prop_type, source_prop_desc = (\n",
    "                get_node_prop_type_desc_from_string(source_model_node_prop_type_desc)\n",
    "            )\n",
    "            target_node_name, target_prop_name, target_prop_type, target_prop_desc = (\n",
    "                get_node_prop_type_desc_from_string(target_model_node_prop_type_desc)\n",
    "            )            \n",
    "\n",
    "            # Skip rows with empty node or prop names\n",
    "            if not target_node_name or not target_prop_name:\n",
    "                print(f\"SKIP: Empty node/property in target: '{target_model_node_prop_type_desc}'\")\n",
    "                continue\n",
    "            if not source_node_name or not source_prop_name:\n",
    "                print(f\"SKIP: Empty node/property in source: '{source_model_node_prop_type_desc}'\")\n",
    "                continue            \n",
    "\n",
    "            target_key = f\"{target_node_name}.{target_prop_name}\".strip()\n",
    "            source_key = f\"{source_node_name}.{source_prop_name}\".strip()            \n",
    "\n",
    "            if target_key not in target_model_props_lookup:\n",
    "                print(\n",
    "                    f\"ERROR: {target_key} is not in target_model; string was '{target_model_node_prop_type_desc}'\"\n",
    "                )            \n",
    "\n",
    "            if source_key not in source_model_props_lookup:\n",
    "                print(\n",
    "                    f\"ERROR: {source_key} is not in source_model; string was '{source_model_node_prop_type_desc}'\"\n",
    "                )\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
