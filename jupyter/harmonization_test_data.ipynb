{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create Harmonization Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We will use the same source of data as we did for the 1st objective benchmark (27 Real Gen3 Data Dictionaries with mutations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are actively working on related *.py files and would like changes to reload automatically into this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from harmonization.utils import TEMP_DIR, get_data_model_as_node_prop_descriptions, get_gen3_json_schemas_and_templates\n",
    "\n",
    "GEN3_DOMAIN_MAPPING = {\n",
    "    \"kidsfirst\": \"https://data.kidsfirstdrc.org\",\n",
    "}\n",
    "GEN3_MANUAL_DD_PATH = {\n",
    "    \"microbiome\": \"microbiome_schema.json\",\n",
    "    \"hnc\": \"hnc_schema.json\",\n",
    "    \"genomel\": \"genomel_schema.json\",\n",
    "    \"rerf\": \"rerf_schema.json\",\n",
    "    \"pdp\": \"pdp_schema.json\",\n",
    "    \"ibdgc\": \"ibdgc_schema.json\",\n",
    "    \"mmrf\": \"mmrf_schema.json\",\n",
    "    \"toxdatacommons\": \"toxdatacommons_schema.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Manually retrieve some data dictionaries instead of relying on a deployed Gen3 instance with API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/mmrf_dictionary/0.0.5/schema.json\" -O mmrf_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/rerf_dictionary/0.4.1/schema.json\" -O rerf_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/pdp_dictionary/1.0.0/schema.json\" -O pdp_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/kf-dictionary/1.2.3/schema.json\" -O kidsfirst_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/genomel-dictionary/0.3.2/schema.json\" -O genomel_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/ibdgc-dictionary/1.6.10/schema.json\" -O ibdgc_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/microbiome_datadictionary/1.1.1/schema.json\" -O microbiome_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/acctdictionary/0.7.1/schema.json\" -O account_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/canine_dictionary/1.1.0/schema.json\" -O canine_schema.json\n",
    "!wget \"https://s3.amazonaws.com/dictionary-artifacts/hnc_dictionary/0.1.0/schema.json\" -O hnc_schema.json\n",
    "!wget \"https://toxdatacommons.com/api/v0/submission/_dictionary/_all\" -O toxdatacommons_schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mutated real data dicts: https://uchicago.app.box.com/folder/320103463138?s=a58qiodbv0grnxq0h94ronz5qa5xlmpc\n",
    "# and place as input_dir\n",
    "input_dir = \"../data/benchmark_SDCs_27_Gen3_DMs_plus_mutated\"\n",
    "output_dir = \"../datasets/harmonization_benchmark_SDCs_27_Gen3_DMs_mutated_v0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gen3 dictionary schema description\n",
    "def load_gen3_dd_schemas(domain):\n",
    "    schema_path = (\n",
    "        GEN3_MANUAL_DD_PATH.get(domain)\n",
    "        or f\"{TEMP_DIR}/{domain}/Unmodified/schema.json\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(schema_path):\n",
    "        real_domain = GEN3_DOMAIN_MAPPING.get(domain, domain)\n",
    "        get_gen3_json_schemas_and_templates(f\"https://{real_domain}\", TEMP_DIR)\n",
    "\n",
    "    with open(schema_path, \"r\") as file:\n",
    "        gen3_dd_schema = json.load(file)\n",
    "\n",
    "    return gen3_dd_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_mappings_for_directory(input_dir, output_dir):\n",
    "    # Mirror directory structure and process files\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        relative_path = os.path.relpath(root, input_dir)\n",
    "        mirrored_path = os.path.join(output_dir, relative_path)\n",
    "        os.makedirs(mirrored_path, exist_ok=True)\n",
    "\n",
    "        gen3_dd_schemas = {}\n",
    "\n",
    "        ai_model_node_props = {}\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"__var_map.json\"):\n",
    "                gen3_domain = file_name.split(\"__\")[0]\n",
    "\n",
    "                if gen3_domain not in gen3_dd_schemas:\n",
    "                    continue\n",
    "\n",
    "                # Transform var_map.json to expected_mappings.tsv\n",
    "                with open(os.path.join(root, file_name), \"r\") as var_map_file:\n",
    "                    var_map_data = json.load(var_map_file)\n",
    "\n",
    "                tsv_lines = [\"ai_model_node_prop_desc\\tharmonized_model_node_prop_desc\"]\n",
    "                for harmonized_key, value in var_map_data.items():\n",
    "                    # the varmap data comes in 3 forms: node, node.property, and node.foreign_key_node.foreign_key_property\n",
    "                    # we only want really care about determining mapping from node.property at this point, so ignore\n",
    "                    # the other two\n",
    "                    if harmonized_key.count(\".\") != 1:\n",
    "                        continue\n",
    "\n",
    "                    new_name = value.get(\"new_name\", \"\")\n",
    "                    new_description = value.get(\"new_description\", \"\").strip()\n",
    "                    full_node_prop = ai_model_node_props.get(new_description, \"\")\n",
    "\n",
    "                    if not full_node_prop:\n",
    "                        full_node_prop = ai_model_node_props.get(new_name, \"\")\n",
    "\n",
    "                    if not full_node_prop:\n",
    "                        logging.debug(\n",
    "                            f\"can't find: {new_name} by description: {new_description} in output. skipping...\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    original_description = f\"{harmonized_key}: \"\n",
    "\n",
    "                    # Extract descriptions from Gen3 dictionary domain\n",
    "                    domain_element, domain_property = harmonized_key.split(\".\")\n",
    "                    gen3_node = gen3_dd_schemas[gen3_domain].get(domain_element, {})\n",
    "\n",
    "                    if not gen3_node:\n",
    "                        logging.debug(f\"{domain_element} not in {gen3_domain}\")\n",
    "                        continue\n",
    "\n",
    "                    gen3_node_prop = gen3_node[\"properties\"].get(domain_property, {})\n",
    "\n",
    "                    if not gen3_node_prop:\n",
    "                        logging.warning(\n",
    "                            f\"{domain_property} not in {gen3_node} in {gen3_domain}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    gen3_description = (\n",
    "                        gen3_node_prop.get(\"description\", \"\")\n",
    "                        .replace(\"\\t\", \"    \")\n",
    "                        .replace(\"\\n\", \" \")\n",
    "                    )\n",
    "                    original_description += f\"{gen3_description}\"\n",
    "\n",
    "                    ai_model_line = f\"{full_node_prop}\\t{original_description}\"\n",
    "                    tsv_lines.append(ai_model_line)\n",
    "\n",
    "                # Save as expected_mappings.tsv\n",
    "                if tsv_lines:\n",
    "                    os.makedirs(mirrored_path, exist_ok=True)\n",
    "                    with open(\n",
    "                        os.path.join(mirrored_path, \"expected_mappings.tsv\"), \"w\"\n",
    "                    ) as tsv_file:\n",
    "                        tsv_file.write(\"\\n\".join(tsv_lines))\n",
    "                else:\n",
    "                    logging.warning(f\"No mappings for {mirrored_path}\")\n",
    "\n",
    "            # Process files, rename jsonschema to ai_model_output\n",
    "            elif \"__jsonschema_dd__\" in file_name:\n",
    "                if file_name.endswith(\"__jsonschema_dd.json\"):\n",
    "                    gen3_domain = file_name.split(\"__\")[0]\n",
    "                    if gen3_domain not in gen3_dd_schemas:\n",
    "                        try:\n",
    "                            gen3_dd_schemas[gen3_domain] = load_gen3_dd_schemas(\n",
    "                                gen3_domain\n",
    "                            )\n",
    "\n",
    "                        except Exception:\n",
    "                            logging.debug(\n",
    "                                f\"Could not get Gen3 DD for: {gen3_domain}. Skipping...\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                    output_harmonized_model_path = os.path.join(\n",
    "                        mirrored_path, \"harmonized_data_model.json\"\n",
    "                    )\n",
    "                    with open(output_harmonized_model_path, \"w\") as dd_file:\n",
    "                        dd_file.write(json.dumps(gen3_dd_schemas[gen3_domain]))\n",
    "\n",
    "                    # Rename JSON schema files\n",
    "                    new_name = file_name.replace(\"__jsonschema_dd__\", \"__\").replace(\n",
    "                        \"__jsonschema_dd.json\", \"__ai_model_output.json\"\n",
    "                    )\n",
    "                    shutil.copy(\n",
    "                        os.path.join(root, file_name),\n",
    "                        os.path.join(mirrored_path, new_name),\n",
    "                    )\n",
    "                    with open(os.path.join(root, file_name)) as ai_model_ouput_file:\n",
    "                        ai_model_ouput = json.load(ai_model_ouput_file)\n",
    "                        ai_model_node_props_raw = (\n",
    "                            get_data_model_as_node_prop_descriptions(ai_model_ouput)\n",
    "                        )\n",
    "                        for node_prop in ai_model_node_props_raw:\n",
    "                            if \":\" in node_prop:\n",
    "                                prop = node_prop.split(\":\")[0]\n",
    "                                desc = \":\".join(node_prop.split(\":\")[1:])\n",
    "                            else:\n",
    "                                prop = node_prop\n",
    "                                desc = \"\"\n",
    "\n",
    "                            # key on description b/c that's what's in the mapping we can rely on\n",
    "                            ai_model_node_props[desc.strip()] = node_prop\n",
    "\n",
    "    print(f\"Done. Output: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_mappings_from_root_directory(input_dir, output_dir):\n",
    "    # Mirror directory structure and process files\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for dir in dirs:\n",
    "            if \"mutated_\" not in dir:\n",
    "                continue \n",
    "\n",
    "            mirrored_path = os.path.join(output_dir, dir)\n",
    "            os.makedirs(mirrored_path, exist_ok=True)\n",
    "\n",
    "            full_path = os.path.join(root, dir)\n",
    "            print(f\"Handling dir: {full_path}\")\n",
    "            output_mappings_for_directory(\n",
    "                full_path, mirrored_path, # gen3_dd_schema\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mappings_from_root_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def remove_property_folders_without_mapping(root_folder):\n",
    "    \"\"\"\n",
    "    In each root/mutated_dd_*/original_gen3_dd_*/<arbitrary_folder>/,\n",
    "    deletes <arbitrary_folder> if expected_mappings.tsv is missing.\n",
    "    \"\"\"\n",
    "    for mutated_dd_dir in os.listdir(root_folder):\n",
    "        mutated_dd_path = os.path.join(root_folder, mutated_dd_dir)\n",
    "        if not os.path.isdir(mutated_dd_path):\n",
    "            continue\n",
    "        for original_dd_dir in os.listdir(mutated_dd_path):\n",
    "            original_dd_path = os.path.join(mutated_dd_path, original_dd_dir)\n",
    "            if not os.path.isdir(original_dd_path):\n",
    "                continue\n",
    "            for prop_folder in os.listdir(original_dd_path):\n",
    "                prop_folder_path = os.path.join(original_dd_path, prop_folder)\n",
    "                if not os.path.isdir(prop_folder_path):\n",
    "                    continue\n",
    "                mapping_path = os.path.join(prop_folder_path, \"expected_mappings.tsv\")\n",
    "                if not os.path.exists(mapping_path):\n",
    "                    print(\n",
    "                        f\"Removing folder (no expected_mappings.tsv): {prop_folder_path} \"\n",
    "                    )\n",
    "                    shutil.rmtree(prop_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_property_folders_without_mapping(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- root\n",
    "    - mutated_dd_0\n",
    "        - original_gen3_dd_0\n",
    "            - original_node.property\n",
    "                - `*__ai_model_ouput.json`\n",
    "                - `expected_mappings.tsv`\n",
    "                - `harmonized_data_model.json`\n",
    "        - original_gen3_dd_1\n",
    "        - ...\n",
    "    - mutated_dd_1\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The AI Model Output simulates output from our AI model, a source data model to start from. The Harmonized Data Model represents a target data model to harmonize to. The expected mappings are known links between the source and target node properties (e.g. columns in tables). They are known b/c for each of them, we used an LLM to \"mutate\" the name and description from the harmonized model in order to generate the AI Model Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File\n",
    "\n",
    "Each test should include a source model: `*__ai_model_output.json`, with desire to harmonize to `harmonized_data_model.json`. We expect harmonization `expected_mappings.tsv`.\n",
    "\n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `*__ai_model_ouput.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `input_target_model`\n",
    "- `harmonized_data_model.json` == `harmonized_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_from_structure(root_dir, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Iterates through subfolders under root_dir and writes a single JSONL file\n",
    "    with input_source_model, input_target_model, harmonized_mapping fields.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Find the first * __ai_model_ouput.json file in this directory\n",
    "        print(f\"Current dir: {dirpath}\")\n",
    "        print(f\"Files in dir: {filenames}\")\n",
    "\n",
    "        ai_model_files = glob.glob(os.path.join(dirpath, \"*ai_model_output*\"))\n",
    "        expected_mappings_path = os.path.join(dirpath, \"expected_mappings.tsv\")\n",
    "        target_harmoized_model_path = os.path.join(dirpath, \"harmonized_data_model.json\")\n",
    "\n",
    "        if (\n",
    "            ai_model_files\n",
    "            and os.path.isfile(expected_mappings_path)\n",
    "            and os.path.isfile(target_harmoized_model_path)\n",
    "        ):\n",
    "            ai_model_path = ai_model_files[0]\n",
    "            # Read files\n",
    "            with open(ai_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                input_source_model = f.read()\n",
    "            with open(expected_mappings_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                harmonized_mapping = f.read()\n",
    "            with open(target_harmoized_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                input_target_model = f.read()\n",
    "            # Append JSON record\n",
    "            record = {\n",
    "                \"input_source_model\": input_source_model,\n",
    "                \"input_target_model\": input_target_model,\n",
    "                \"harmonized_mapping\": harmonized_mapping,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    # Write to JSONL file\n",
    "    print(f\"Test count: {len(records)}\")\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in records:\n",
    "            fout.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = os.path.join(output_dir, \"output.jsonl\")\n",
    "create_jsonl_from_structure(output_dir, output_json_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Test ability to get original files from JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_harmonization_benchmark_jsonl_row(row_dict, output_dir, row_index):\n",
    "    \"\"\"\n",
    "    Extracts the desired content from the row,\n",
    "    creates a per-row output subdirectory,\n",
    "    and writes each file into that subdirectory.\n",
    "    Returns a dict of {filename: content} for that row.\n",
    "    \"\"\"\n",
    "    # Make a subdirectory for this row\n",
    "    row_folder = os.path.join(output_dir, f\"row_{row_index}\")\n",
    "    os.makedirs(row_folder, exist_ok=True)\n",
    "\n",
    "    files = {\n",
    "        \"restored__ai_model_output.json\": row_dict[\"input_source_model\"],\n",
    "        \"expected_mappings.tsv\": row_dict[\"harmonized_mapping\"],\n",
    "        \"harmonized_data_model.json\": row_dict[\"input_target_model\"],\n",
    "    }\n",
    "    for filename, contents in files.items():\n",
    "        with open(os.path.join(row_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(contents)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_harmonization_benchmark_jsonl(jsonl_path, output_dir):\n",
    "    \"\"\"\n",
    "    Reads the JSONL file and calls the row handler for each row.\n",
    "    \"\"\"\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            row = json.loads(line)\n",
    "            _ = get_files_from_harmonization_benchmark_jsonl_row(row, output_dir, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_harmonization_benchmark_jsonl(\n",
    "    output_json_filepath, \"../output/temp/harmonization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
