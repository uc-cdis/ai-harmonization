{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Get Harmonization Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Goal:** Get mapping information from mutated synthetic data models to source models to serve as synthetic mapping/harmonization training data for an AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will use the same source of data as we did for v3 (the latest) of the synthetic data from objective 1. Using only ONE mutated version from here: https://uchicago.app.box.com/folder/318234520321?s=j0oq2pfhe59p3caqq3ss32kxqwciwbsd\n",
    "\n",
    "Utilize only one of the above sub-folders, for example, we will extract and use only one: `mutated_sdc_v3_nmax4_nmin2_pmax75_pmin25_limit20_dmax1000_20250423.zip` and ignore others. \n",
    "\n",
    "Ensure the above is extracted into a `../data/Mutated_SDCs_v3_20250423/mutated_sdc_v3_nmax4_nmin2_pmax75_pmin25_limit20_dmax1000_20250423` folder.\n",
    "\n",
    "Also we need the required the synthetic data dictionary information: https://uchicago.app.box.com/folder/318236025225\n",
    "\n",
    "Extract the above in a `../data/SDMs_nodes6-15_props50-100_20250423` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "from harmonization.utils import (\n",
    "    TEMP_DIR,\n",
    "    get_gen3_json_schemas_and_templates,\n",
    ")\n",
    "from harmonization.simple_data_model import (\n",
    "    get_data_model_as_node_prop_type_descriptions,\n",
    ")\n",
    "\n",
    "\n",
    "SDMS_FOLDER_PATH  = (\n",
    "    \"../data/SDMs_nodes6-15_props50-100_20250423/\"\n",
    ")\n",
    "\n",
    "GEN3_MANUAL_DD_PATH = {\n",
    "    \"aids.diseasedatahub.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/aids.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"bihstaging.data-commons.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/bihstaging.data-commons.org__jsonschema_dd_modified.json\",\n",
    "    \"caninedc.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/caninedc.org__jsonschema_dd_modified.json\",\n",
    "    \"chicagoland.pandemicresponsecommons.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/chicagoland.pandemicresponsecommons.org__jsonschema_dd_modified.json\",\n",
    "    \"chordshealth.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/chordshealth.org__jsonschema_dd_modified.json\",\n",
    "    \"data.bloodpac.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/data.bloodpac.org__jsonschema_dd_modified.json\",\n",
    "    \"data.midrc.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/data.midrc.org__jsonschema_dd_modified.json\",\n",
    "    \"diseasedatahub.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"flu.diseasedatahub.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/flu.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"gen3.biodatacatalyst.nhlbi.nih.gov\": f\"{SDMS_FOLDER_PATH}/input_schemas/gen3.biodatacatalyst.nhlbi.nih.gov__jsonschema_dd_modified.json\",\n",
    "    \"gen3.datacommons.io\": f\"{SDMS_FOLDER_PATH}/input_schemas/gen3.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"genomel\": f\"{SDMS_FOLDER_PATH}/input_schemas/genomel__jsonschema_dd_modified.json\",\n",
    "    \"healdata.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/healdata.org__jsonschema_dd_modified.json\",\n",
    "    \"hnc\": f\"{SDMS_FOLDER_PATH}/input_schemas/hnc__jsonschema_dd_modified.json\",\n",
    "    \"ibdgc\": f\"{SDMS_FOLDER_PATH}/input_schemas/ibdgc__jsonschema_dd_modified.json\",\n",
    "    \"icgc.bionimbus.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/icgc.bionimbus.org__jsonschema_dd_modified.json\",\n",
    "    \"jcoin.datacommons.io\": f\"{SDMS_FOLDER_PATH}/input_schemas/jcoin.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"kidsfirst\": f\"{SDMS_FOLDER_PATH}/input_schemas/kidsfirst__jsonschema_dd_modified.json\",\n",
    "    \"microbiome\": f\"{SDMS_FOLDER_PATH}/input_schemas/microbiome__jsonschema_dd_modified.json\",\n",
    "    \"mmrf\": f\"{SDMS_FOLDER_PATH}/input_schemas/mmrf__jsonschema_dd_modified.json\",\n",
    "    \"nci-crdc.datacommons.io\": f\"{SDMS_FOLDER_PATH}/input_schemas/nci-crdc.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"pdp\": f\"{SDMS_FOLDER_PATH}/input_schemas/pdp__jsonschema_dd_modified.json\",\n",
    "    \"portal.occ-data.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/portal.occ-data.org__jsonschema_dd_modified.json\",\n",
    "    \"rerf\": f\"{SDMS_FOLDER_PATH}/input_schemas/rerf__jsonschema_dd_modified.json\",\n",
    "    \"tb.diseasedatahub.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/tb.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"toxdatacommons\": f\"{SDMS_FOLDER_PATH}/input_schemas/toxdatacommons__jsonschema_dd_modified.json\",\n",
    "    \"vpodc.data-commons.org\": f\"{SDMS_FOLDER_PATH}/input_schemas/vpodc.data-commons.org__jsonschema_dd_modified.json\",\n",
    "}\n",
    "\n",
    "SYNONYMOUS_NODES = {\n",
    "    \"study\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"dataset\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"clinical_trial\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"collection\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"research\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"subject\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"patient\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"case\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"participant\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"biospecimen\": [\"biospecimen\", \"specimen\"],\n",
    "    \"specimen\": [\"biospecimen\", \"specimen\"],\n",
    "    \"project\": [\"project\"],  # this forces a search over other DDs later\n",
    "    \"program\": [\"program\"],  # this forces a search over other DDs later\n",
    "}\n",
    "SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD = {\n",
    "    # will be populated programatically\n",
    "}\n",
    "gen3_dd_schemas = {\n",
    "    # will be populated programatically\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mutated real data dicts: https://uchicago.app.box.com/folder/318234520321?s=j0oq2pfhe59p3caqq3ss32kxqwciwbsd\n",
    "# and place as input_dir\n",
    "input_dir = \"../data/Mutated_SDCs_v3_20250423\"\n",
    "\n",
    "sdm_stats_filepath = os.path.join(SDMS_FOLDER_PATH, \"_SDM_stats.json\")\n",
    "\n",
    "# delete this whole output directory and rerun to recreate\n",
    "output_dir = \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping from nodes that have snyns to a list of dicts with that particular node name\n",
    "for node, _ in SYNONYMOUS_NODES.items():\n",
    "    for schema_name, schema_path in GEN3_MANUAL_DD_PATH.items():\n",
    "        with open(schema_path, \"r\") as file:\n",
    "            gen3_dd_schema = json.load(file)\n",
    "            gen3_dd_schemas[schema_name] = gen3_dd_schema\n",
    "            if node in gen3_dd_schema:\n",
    "                SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD.setdefault(node, set()).add(\n",
    "                    schema_name\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Convert original data model schema format to more efficient dictionary-based format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm_input_schemas_dir = os.path.join(SDMS_FOLDER_PATH, \"/input_schemas/\")\n",
    "for root, dirs, files in os.walk(sdm_input_schemas_dir):\n",
    "    for file in files:\n",
    "        if \"_modified.json\" in file:\n",
    "            continue\n",
    "\n",
    "        output_filename = os.path.join(root, file.replace(\".json\", \"_modified.json\"))\n",
    "        if os.path.exists(output_filename):\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(root, file)) as input_file:\n",
    "            original_data = json.load(input_file)\n",
    "            final_output_structure = {\n",
    "                # \"{{node_name}}\": {\n",
    "                # \"properties\": {\n",
    "                # \"{{property_name}}\": {}\n",
    "                # }\n",
    "                # }\n",
    "            }\n",
    "            for node in original_data[\"nodes\"]:\n",
    "                new_node = copy.deepcopy(node)\n",
    "\n",
    "                # is currently a list, we want a dict\n",
    "                del new_node[\"properties\"]\n",
    "                new_node[\"properties\"] = {}\n",
    "\n",
    "                for node_property in node[\"properties\"]:\n",
    "                    new_node[\"properties\"][node_property[\"name\"]] = node_property\n",
    "\n",
    "                final_output_structure[node[\"name\"]] = new_node\n",
    "\n",
    "            with open(output_filename, \"w\") as output_file:\n",
    "                output_file.write(json.dumps(final_output_structure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_prop_desc_from_real_data_model(\n",
    "    gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "):\n",
    "    description = get_desc_from_real_data_model(\n",
    "        gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "    )\n",
    "    if description is None:\n",
    "        logging.warning(\n",
    "            f\"could not get desc for property {property_name} in node {node_name} from {gen3_domain}\"\n",
    "        )\n",
    "\n",
    "    return f\"{node_name}.{property_name}: {description}\"\n",
    "\n",
    "\n",
    "def get_desc_from_real_data_model(\n",
    "    gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "):\n",
    "    # Extract descriptions from Gen3 dictionary domain\n",
    "    gen3_node = gen3_dd_schemas[gen3_domain].get(node_name, {})\n",
    "\n",
    "    gen3_node_prop = gen3_node.get(\"properties\", {}).get(property_name, {})\n",
    "\n",
    "    if not gen3_node_prop:\n",
    "        # logging.warning(\n",
    "        #     f\"Property {property_name} not in node {node_name} in {gen3_domain}\"\n",
    "        # )\n",
    "        return None\n",
    "\n",
    "    gen3_description = (\n",
    "        gen3_node_prop.get(\"description\", \"\").replace(\"\\t\", \"    \").replace(\"\\n\", \" \")\n",
    "    )\n",
    "\n",
    "    return gen3_description\n",
    "\n",
    "\n",
    "def get_synth_model_to_original(sdm_stats_filepath):\n",
    "    synth_model_to_original = {}\n",
    "\n",
    "    with open(sdm_stats_filepath) as file:\n",
    "        sdm_stats = json.load(file)\n",
    "\n",
    "    for model_num, model_info in sdm_stats[\"models\"].items():\n",
    "        # we will populate this information per synth model\n",
    "        synth_model_to_original_info = {\n",
    "            \"node_to_original_info\": None,\n",
    "            \"synth_model_node_to_properties\": None,\n",
    "            \"synth_model_prop_to_original_models\": None,\n",
    "        }\n",
    "\n",
    "        # todo: helper func?\n",
    "        node_to_original_info = {}\n",
    "        for node_name, node_info in model_info[\"nodes\"].items():\n",
    "            real_data_model = node_info[\"dm_name\"]\n",
    "\n",
    "            node_to_original_info[node_name] = node_info\n",
    "\n",
    "        synth_model_to_original_info[\"node_to_original_info\"] = node_to_original_info\n",
    "\n",
    "        # now we need to get the actual nodes and props used in this synth model\n",
    "        # need to read in synthmodel.json\n",
    "        synth_model_path = os.path.join(\n",
    "            os.path.dirname(sdm_stats_filepath), f\"SDM_{model_num}.json\"\n",
    "        )\n",
    "        synth_model_node_to_properties = {}\n",
    "        with open(synth_model_path) as synth_model_file:\n",
    "            synth_model = json.load(synth_model_file)\n",
    "            for node_info in synth_model[\"nodes\"]:\n",
    "                for node_property in node_info[\"properties\"]:\n",
    "                    synth_model_node_to_properties.setdefault(\n",
    "                        node_info[\"name\"], []\n",
    "                    ).append(node_property)\n",
    "\n",
    "        synth_model_to_original_info[\"synth_model_node_to_properties\"] = (\n",
    "            synth_model_node_to_properties\n",
    "        )\n",
    "\n",
    "        # next step\n",
    "\n",
    "        synth_model_prop_to_original_models = {\n",
    "            # \"node.property\": [{\n",
    "            #                       \"gen3_domain\": gen3_domain\n",
    "            #                       \"original_node\": gen3_node\n",
    "            #                       \"original_property\": gen3_node_prop\n",
    "            #                   }, ...]\n",
    "        }\n",
    "        for node_name, node_info in synth_model_to_original_info[\n",
    "            \"synth_model_node_to_properties\"\n",
    "        ].items():\n",
    "            for node_property_info in node_info:\n",
    "                property_name = node_property_info[\"name\"]\n",
    "                original_sources_already_seen_for_prop = set()\n",
    "\n",
    "                # find which original dd and node property came from\n",
    "\n",
    "                # node is not synonymous so we just check the original\n",
    "                if node_name not in SYNONYMOUS_NODES:\n",
    "                    if (\n",
    "                        node_name\n",
    "                        in synth_model_to_original_info[\"node_to_original_info\"]\n",
    "                    ):\n",
    "                        original_model = synth_model_to_original_info[\n",
    "                            \"node_to_original_info\"\n",
    "                        ][node_name][\"dm_name\"]\n",
    "\n",
    "                        synth_node_prop_desc = node_property_info[\"description\"]\n",
    "\n",
    "                        gen3_node = gen3_dd_schemas[original_model].get(node_name, {})\n",
    "                        gen3_node_prop = gen3_node[\"properties\"].get(property_name, {})\n",
    "\n",
    "                        if gen3_node_prop:\n",
    "                            original_info = {\n",
    "                                \"gen3_domain\": gen3_domain,\n",
    "                                \"original_node\": gen3_node,\n",
    "                                \"original_property\": gen3_node_prop,\n",
    "                            }\n",
    "                            synth_model_prop_to_original_models.setdefault(\n",
    "                                f\"{node_name}.{property_name}\", []\n",
    "                            ).append(original_info)\n",
    "                    else:\n",
    "                        # node not found in specified original model and\n",
    "                        # not a synonmous node.\n",
    "                        # This means we need to check parent_nodes information to determine where this node came from\n",
    "                        # (due to missing information from the synthetic data generation SDM stats creation)\n",
    "                        for node, node_info in model_info[\"nodes\"].items():\n",
    "                            if node_name in node_info.get(\"parent_nodes\", {}):\n",
    "                                gen3_domain = node_info[\"dm_name\"]\n",
    "                                gen3_node = gen3_dd_schemas[gen3_domain].get(\n",
    "                                    node_name, {}\n",
    "                                )\n",
    "                                gen3_node_prop = gen3_node[\"properties\"].get(\n",
    "                                    node_name, {}\n",
    "                                )\n",
    "\n",
    "                                if gen3_node_prop:\n",
    "                                    original_info = {\n",
    "                                        \"gen3_domain\": gen3_domain,\n",
    "                                        \"original_node\": gen3_node,\n",
    "                                        \"original_property\": gen3_node_prop,\n",
    "                                    }\n",
    "                                    synth_model_prop_to_original_models.setdefault(\n",
    "                                        f\"{node_name}.{property_name}\", []\n",
    "                                    ).append(original_info)\n",
    "\n",
    "                    if original_model not in original_sources_already_seen_for_prop:\n",
    "                        original_info = {\n",
    "                            \"gen3_domain\": original_model,\n",
    "                            \"original_node\": gen3_node,\n",
    "                            \"original_property\": gen3_node_prop,\n",
    "                        }\n",
    "                        original_sources_already_seen_for_prop.add(original_model)\n",
    "                        synth_model_prop_to_original_models.setdefault(\n",
    "                            f\"{node_name}.{property_name}\", []\n",
    "                        ).append(original_info)\n",
    "                elif node_name in SYNONYMOUS_NODES:\n",
    "                    alternate_names = SYNONYMOUS_NODES.get(node_name, [])\n",
    "\n",
    "                    for alternate_node_name in alternate_names:\n",
    "                        for gen3_domain in SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD.get(\n",
    "                            alternate_node_name, []\n",
    "                        ):\n",
    "                            gen3_node = gen3_dd_schemas[gen3_domain].get(\n",
    "                                alternate_node_name, {}\n",
    "                            )\n",
    "                            new_property_name = property_name.replace(\n",
    "                                node_name, alternate_node_name\n",
    "                            )\n",
    "\n",
    "                            gen3_node_prop = gen3_node[\"properties\"].get(\n",
    "                                new_property_name, {}\n",
    "                            )\n",
    "\n",
    "                            if gen3_node_prop:\n",
    "                                original_info = {\n",
    "                                    \"gen3_domain\": gen3_domain,\n",
    "                                    \"original_node\": gen3_node,\n",
    "                                    \"original_property\": gen3_node_prop,\n",
    "                                }\n",
    "                                synth_model_prop_to_original_models.setdefault(\n",
    "                                    f\"{node_name}.{property_name}\", []\n",
    "                                ).append(original_info)\n",
    "                else:\n",
    "                    logging.warning(\n",
    "                        f\"could not find {node_name}.{property_name} in SDM_{model_num}.json\"\n",
    "                    )\n",
    "\n",
    "        synth_model_to_original_info[\"synth_model_prop_to_original_models\"] = (\n",
    "            synth_model_prop_to_original_models\n",
    "        )\n",
    "\n",
    "        synth_model_to_original[model_num] = synth_model_to_original_info\n",
    "    return synth_model_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes time, run this once then rely on persisted results (i.e. comment this out after running once)\n",
    "synth_model_to_original = get_synth_model_to_original(sdm_stats_filepath)\n",
    "print(f\"Got {len(synth_model_to_original.keys())} synthetic models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Process all SDCs into Training Data using the Pre-Computed Info Above\n",
    "\n",
    "This takes a _long_ time. If you leave the default settings, it'll skip recomputing folders that already exist in the output (i.e. you can stop and restart it many times and it'll basically continue where it left off).\n",
    "\n",
    "You can adjust the number of workers `num_workers` below and you should set that as high as possible.\n",
    "\n",
    "We're processing 10k synthetic data models, and for each synthetic data model we can have upwards of a dozen SDCs. And each SDC can create mappings for up to 27 of the original data models (e.g. if project contains properties that would map to any model, we generate expected mappings to that model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_synth_data_info_to_contribution_directory(\n",
    "    directory_path, synth_model_to_original, force_recreation=False\n",
    "):\n",
    "    if not synth_model_to_original:\n",
    "        logging.warning(\n",
    "            f\"Trying to persist synth_model_to_original but nothing was provided\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    model_path = os.path.dirname(directory_path)\n",
    "    mutation_path = os.path.dirname(model_path)\n",
    "    # logging.debug(f\"model_path: {model_path}\")\n",
    "    # logging.debug(f\"mutation_path: {mutation_path}\")\n",
    "\n",
    "    synth_model_to_original_filepath = os.path.join(\n",
    "        model_path, \"synth_model_to_original.json\"\n",
    "    )\n",
    "\n",
    "    if force_recreation and os.path.exists(synth_model_to_original_filepath):\n",
    "        os.remove(synth_model_to_original_filepath)\n",
    "\n",
    "    if not os.path.exists(synth_model_to_original_filepath):\n",
    "        with open(\n",
    "            os.path.join(model_path, \"synth_model_to_original.json\"), \"w\"\n",
    "        ) as output:\n",
    "            for entry in os.scandir(directory_path):\n",
    "                if os.path.basename(entry).endswith(\"__var_map.json\"):\n",
    "                    synth_model_name = entry.path.split(\"__\")[0]\n",
    "                    synth_model_num = synth_model_name.split(\"_\")[-1]\n",
    "\n",
    "                    specific_synth_model_to_original = synth_model_to_original[\n",
    "                        synth_model_num\n",
    "                    ]\n",
    "                    output.write(json.dumps(specific_synth_model_to_original))\n",
    "\n",
    "\n",
    "def process_synth_data_contribution_directory(\n",
    "    directory_path, output_dir, gen3_dd_schemas, force_recreation=False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each original model, generate a folder with files related\n",
    "    Format of output:\n",
    "    \"\"\"\n",
    "    # each of these SDCs will generate multiple test cases b/c properties from\n",
    "\n",
    "    model_path = os.path.dirname(directory_path)\n",
    "    mutation_path = os.path.dirname(model_path)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(mutation_path, exist_ok=True)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    synth_model_to_original_filepath = os.path.join(\n",
    "        model_path, \"synth_model_to_original.json\"\n",
    "    )\n",
    "    specific_synth_model_to_original = {}\n",
    "    with open(synth_model_to_original_filepath) as input_file:\n",
    "        try:\n",
    "            specific_synth_model_to_original = json.load(input_file)\n",
    "        except Exception as exc:\n",
    "            logging.error(\n",
    "                f\"Could not get specific_synth_model_to_original from: {input_file}. Skipping...\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "    output_dir_updated = os.path.join(output_dir, os.path.basename(mutation_path))\n",
    "    output_dir_updated = os.path.join(output_dir_updated, os.path.basename(model_path))\n",
    "    output_dir_updated = os.path.join(\n",
    "        output_dir_updated, os.path.basename(directory_path)\n",
    "    )\n",
    "\n",
    "    if force_recreation and os.path.exists(output_dir_updated):\n",
    "        os.remove(output_dir_updated)\n",
    "\n",
    "    if not os.path.exists(output_dir_updated):\n",
    "        output_mappings_for_directory(\n",
    "            gen3_dd_schemas=gen3_dd_schemas,\n",
    "            input_dir=directory_path,\n",
    "            output_dir=output_dir_updated,\n",
    "            specific_synth_model_to_original=specific_synth_model_to_original,\n",
    "        )\n",
    "    else:\n",
    "        # print(f\"skipping {output_dir_updated}, already exists\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def output_mappings_for_directory(\n",
    "    gen3_dd_schemas, input_dir, output_dir, specific_synth_model_to_original\n",
    "):\n",
    "    # Mirror directory structure and process files\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        relative_path = os.path.relpath(root, input_dir)\n",
    "        mirrored_path = os.path.join(output_dir, relative_path)\n",
    "        os.makedirs(mirrored_path, exist_ok=True)\n",
    "\n",
    "        ai_model_node_props = {}\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"__var_map.json\"):\n",
    "                synth_model_name = file_name.split(\"__\")[0]\n",
    "                synth_model_num = synth_model_name.split(\"_\")[-1]\n",
    "\n",
    "                synth_model_prop_to_original_models = specific_synth_model_to_original[\n",
    "                    \"synth_model_prop_to_original_models\"\n",
    "                ]\n",
    "\n",
    "                # Transform var_map.json to expected_mappings.tsv\n",
    "                with open(os.path.join(root, file_name), \"r\") as var_map_file:\n",
    "                    var_map_data = json.load(var_map_file)\n",
    "\n",
    "                # print(f\"var_map_data: {var_map_data}\")\n",
    "\n",
    "                # since we can have nodes from multiple different original models\n",
    "                tsv_lines_per_original_model = {}\n",
    "                tsv_lines_header = (\n",
    "                    \"ai_model_node_prop_desc\\tharmonized_model_node_prop_desc\\n\"\n",
    "                )\n",
    "                original_node_to_harmonized_node = {}\n",
    "                for original_key, value in var_map_data.items():\n",
    "                    # the varmap data comes in 3 forms: node, node.property, and node.\"foreign_key_node.foreign_key_property\"\n",
    "                    # we only want really care about determining mapping from node.property at this point, so ignore\n",
    "                    # the other two\n",
    "                    if \".\" not in original_key:\n",
    "                        original_node_to_harmonized_node[original_key] = value.get(\n",
    "                            \"new_name\", \"\"\n",
    "                        )\n",
    "\n",
    "                for original_key, value in var_map_data.items():\n",
    "                    if \".\" not in original_key:\n",
    "                        continue\n",
    "\n",
    "                    original_node = original_key.split(\".\")[0]\n",
    "                    harmonized_node = original_node_to_harmonized_node[original_node]\n",
    "                    original_property = \".\".join(original_key.split(\".\")[-2:])\n",
    "\n",
    "                    new_name = value.get(\"new_name\", \"\")\n",
    "                    new_description = value.get(\"new_description\", \"\").strip()\n",
    "\n",
    "                    ai_model_node_prop_desc = (\n",
    "                        f\"{harmonized_node}.{new_name}: {new_description}\"\n",
    "                    )\n",
    "                    # ai_model_node_prop_desc = specific_synth_model_to_original[\"synth_model_node_to_properties\"].get(\n",
    "                    #     new_description, \"\"\n",
    "                    # )\n",
    "\n",
    "                    # if not ai_model_node_prop_desc:\n",
    "                    #     ai_model_node_prop_desc = ai_model_node_props.get(new_name, \"\")\n",
    "\n",
    "                    # if not ai_model_node_prop_desc:\n",
    "                    #     logging.debug(\n",
    "                    #         f\"can't find: {new_name} by description: {new_description} in output. skipping...\"\n",
    "                    #     )\n",
    "                    #     continue\n",
    "\n",
    "                    # get gen3_domain (e.g. original model) for this particular node prop by\n",
    "                    # using previously constructured mapping information\n",
    "                    original_node_props = synth_model_prop_to_original_models.get(\n",
    "                        original_key, []\n",
    "                    )\n",
    "                    # if not original_node_props:\n",
    "                    #     logging.warning(\n",
    "                    #         f\"{original_key} not in synth_model_prop_to_original_models for {ai_model_node_prop_desc}\"\n",
    "                    #     )\n",
    "\n",
    "                    for original_node_prop in original_node_props:\n",
    "                        gen3_domain = original_node_prop[\"gen3_domain\"]\n",
    "                        original_node = original_node_prop[\"original_node\"][\"name\"]\n",
    "                        original_property = original_node_prop[\"original_property\"].get(\n",
    "                            \"name\", \"\"\n",
    "                        )\n",
    "\n",
    "                        # Extract descriptions from Gen3 dictionary domain\n",
    "                        gen3_description = (\n",
    "                            get_desc_from_real_data_model(\n",
    "                                gen3_dd_schemas,\n",
    "                                gen3_domain,\n",
    "                                original_node,\n",
    "                                original_property,\n",
    "                            )\n",
    "                            or \"\"\n",
    "                        )\n",
    "                        harmonized_model_node_prop_desc = (\n",
    "                            f\"{original_node}.{original_property}: {gen3_description}\"\n",
    "                        )\n",
    "\n",
    "                        ai_model_line = f\"{ai_model_node_prop_desc}\\t{harmonized_model_node_prop_desc}\"\n",
    "                        tsv_lines_per_original_model.setdefault(gen3_domain, []).append(\n",
    "                            ai_model_line\n",
    "                        )\n",
    "\n",
    "                # Save as expected_mappings.tsv\n",
    "                if tsv_lines_per_original_model:\n",
    "                    for gen3_domain, tsv_lines in tsv_lines_per_original_model.items():\n",
    "                        os.makedirs(\n",
    "                            os.path.join(mirrored_path, gen3_domain), exist_ok=True\n",
    "                        )\n",
    "                        with open(\n",
    "                            os.path.join(\n",
    "                                os.path.join(mirrored_path, gen3_domain),\n",
    "                                \"expected_mappings.tsv\",\n",
    "                            ),\n",
    "                            \"w\",\n",
    "                        ) as tsv_file:\n",
    "                            tsv_file.write(tsv_lines_header)\n",
    "                            tsv_file.write(\"\\n\".join(list(set(tsv_lines))))\n",
    "                else:\n",
    "                    logging.warning(f\"No mappings for {mirrored_path}\")\n",
    "\n",
    "            # Process files, rename jsonschema to ai_model_output\n",
    "            elif \"__jsonschema_dd__\" in file_name:\n",
    "                if file_name.endswith(\"__jsonschema_dd.json\"):\n",
    "                    gen3_domain = file_name.split(\"__\")[0]\n",
    "\n",
    "                    output_harmonized_model_path = os.path.join(\n",
    "                        mirrored_path, \"harmonized_data_model.json\"\n",
    "                    )\n",
    "                    with open(output_harmonized_model_path, \"w\") as dd_file:\n",
    "                        dd_file.write(json.dumps(gen3_dd_schemas[gen3_domain]))\n",
    "\n",
    "                    # Rename JSON schema files\n",
    "                    new_name = file_name.replace(\"__jsonschema_dd__\", \"__\").replace(\n",
    "                        \"__jsonschema_dd.json\", \"__ai_model_output.json\"\n",
    "                    )\n",
    "                    shutil.copy(\n",
    "                        os.path.join(root, file_name),\n",
    "                        os.path.join(mirrored_path, new_name),\n",
    "                    )\n",
    "                    with open(os.path.join(root, file_name)) as ai_model_ouput_file:\n",
    "                        ai_model_ouput = json.load(ai_model_ouput_file)\n",
    "                        ai_model_node_props_raw = (\n",
    "                            get_data_model_as_node_prop_type_descriptions(\n",
    "                                ai_model_ouput\n",
    "                            )\n",
    "                        )\n",
    "                        for node_prop in ai_model_node_props_raw:\n",
    "                            if \":\" in node_prop:\n",
    "                                prop = node_prop.split(\":\")[0]\n",
    "                                desc = \":\".join(node_prop.split(\":\")[1:])\n",
    "                            else:\n",
    "                                prop = node_prop\n",
    "                                desc = \"\"\n",
    "\n",
    "                            # key on description b/c that's what's in the mapping we can rely on\n",
    "                            ai_model_node_props[desc.strip()] = node_prop\n",
    "\n",
    "    # print(f\"Done. Output: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_training_data_for_directory(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    force_recreation_of_synth_data_info=False,\n",
    "    force_recreation_of_synth_data_contributions=False,\n",
    "    num_workers=8,\n",
    "):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for entry in os.scandir(input_dir):\n",
    "            if entry.is_dir() and os.path.basename(entry).startswith(\"mutated_\"):\n",
    "                logging.warning(f\"processing: {entry.path}\")\n",
    "                for data_model_dir in os.scandir(entry):\n",
    "                    if data_model_dir.is_dir() and os.path.basename(\n",
    "                        data_model_dir\n",
    "                    ).endswith(\"_tsvs\"):\n",
    "                        logging.debug(\n",
    "                            f\"processing data model directory: {data_model_dir.path}\"\n",
    "                        )\n",
    "                        for data_contribution_dir in os.scandir(data_model_dir):\n",
    "                            if data_contribution_dir.is_dir():\n",
    "                                logging.debug(\n",
    "                                    f\"queueing data contribution directory: {os.path.basename(data_contribution_dir.path)}\"\n",
    "                                )\n",
    "                                # submit each contribution dir to the pool\n",
    "                                futures.append(\n",
    "                                    executor.submit(\n",
    "                                        _process_single_data_contribution_dir,\n",
    "                                        data_contribution_dir.path,\n",
    "                                        output_dir,\n",
    "                                        synth_model_to_original,\n",
    "                                        force_recreation_of_synth_data_info,\n",
    "                                        force_recreation_of_synth_data_contributions,\n",
    "                                    )\n",
    "                                )\n",
    "        # wait for all to finish\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                f.result()\n",
    "            except Exception as exc:\n",
    "                logging.error(\n",
    "                    f\"[THREAD ERROR] Data contribution dir task failed: {exc}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def _process_single_data_contribution_dir(\n",
    "    data_contribution_path,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    force_recreation_of_synth_data_info,\n",
    "    force_recreation_of_synth_data_contributions,\n",
    "):\n",
    "    # This function is called in thread pool, for a single data_contribution directory\n",
    "    persist_synth_data_info_to_contribution_directory(\n",
    "        data_contribution_path,\n",
    "        synth_model_to_original,\n",
    "        force_recreation=force_recreation_of_synth_data_info,\n",
    "    )\n",
    "    process_synth_data_contribution_directory(\n",
    "        data_contribution_path,\n",
    "        output_dir,\n",
    "        gen3_dd_schemas,\n",
    "        force_recreation=force_recreation_of_synth_data_contributions,\n",
    "    )\n",
    "\n",
    "# IMPORTANT: Remove this condition if you want to continue generating from a previously failed or incomplete output\n",
    "if not os.path.exists(output_dir):\n",
    "    output_training_data_for_directory(input_dir, output_dir, synth_model_to_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "At this point, the output_dir contains a raw dump of potentially relevant training data organized into folders. We still need to do some cleanup and expanding of info to get a final, single JSONL file for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Use Folder-Based Data Generated from Above to Construct AI-ready Training Data in JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files_dir = (\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_folders_without_mapping_info(root_folder):\n",
    "    for mutated_dd_dir in os.listdir(root_folder):\n",
    "        mutated_dd_path = os.path.join(root_folder, mutated_dd_dir)\n",
    "        if not os.path.isdir(mutated_dd_path):\n",
    "            continue\n",
    "        for original_dd_dir in os.listdir(mutated_dd_path):\n",
    "            original_dd_path = os.path.join(mutated_dd_path, original_dd_dir)\n",
    "            if not os.path.isdir(original_dd_path):\n",
    "                continue\n",
    "            for prop_folder in os.listdir(original_dd_path):\n",
    "                prop_folder_path = os.path.join(original_dd_path, prop_folder)\n",
    "                if not os.path.isdir(prop_folder_path):\n",
    "                    continue\n",
    "                for gen3_dd_dir in os.listdir(prop_folder_path):\n",
    "                    gen3_dd_path = os.path.join(prop_folder_path, gen3_dd_dir)\n",
    "                    if not os.path.isdir(gen3_dd_path):\n",
    "                        continue\n",
    "                    mapping_path = os.path.join(gen3_dd_path, \"expected_mappings.tsv\")\n",
    "                    if not os.path.exists(mapping_path):\n",
    "                        print(\n",
    "                            f\"Removing folder (no expected_mappings.tsv): {gen3_dd_path}\"\n",
    "                        )\n",
    "                        shutil.rmtree(prop_folder_path)\n",
    "                        continue\n",
    "                    with open(mapping_path) as mapping_file:\n",
    "                        if len(mapping_file.readlines()) <= 1:\n",
    "                            print(\n",
    "                                f\"Removing folder (no content in expected_mappings.tsv): {gen3_dd_path}\"\n",
    "                            )\n",
    "                            shutil.rmtree(prop_folder_path)\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this once - it will take a while when you run it.\n",
    "# It's syncronously checking validity of everything generated above and removing anything non-conformant\n",
    "remove_folders_without_mapping_info(training_files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\n",
    "  - mutated_synthetic_data_with_specific_parameters\n",
    "    - SDM_synthetic_data_model_0\n",
    "      - synthetic_data_contribution_0\n",
    "        - original_real_source_data_model_0\n",
    "          - expected_mappings.tsv\n",
    "        - original_real_source_data_model_1\n",
    "          - expected_mappings.tsv\n",
    "        - ...\n",
    "      - ...\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "`expected_mappings.tsv` has the following columns: \n",
    "\n",
    "- `ai_model_node_prop_desc`\n",
    "    - The AI Model generated node property (e.g. the property we want to map/harmonize to a target model)\n",
    "- `harmonized_model_node_prop_desc`\n",
    "    - The source of truth target model node property (e.g. the property from the target model we know the above should map to)\n",
    "\n",
    "and the target model itself is identified by the `original_real_source_data_model` folder that the expected mappings are in.\n",
    "\n",
    "#### How do we know the proper mapping?\n",
    "\n",
    "We took mutated synthetic data from our first algorithm and traced backwards.\n",
    "\n",
    "This is how we got the mutated data in the first place:\n",
    "\n",
    "27 Real Gen3 Data Models -> 10k Synthetic Data Models -> Synthetic Data Contributions -> Mutated Synthetic Data Contributions\n",
    "\n",
    "We were able to trace from the mutations, back to the synthetic model, and from the synthetic model we are able to determine\n",
    "the set of possible mappings for that particular property back to the real data models. \n",
    "\n",
    "> Key point: A single mutated property could potentially map back to `n` original real data models, so we collect every option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File Incrementally\n",
    "\n",
    "Each test should include a source model, with desire to harmonize to a target model. We expect known mapping / harmonization in `expected_mappings.tsv`, which gets dumped as a string into the `harmonized_mapping` column.\n",
    "\n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default to getting training for all available target models\n",
    "target_models_to_get_training_data_for = list(GEN3_MANUAL_DD_PATH.keys())\n",
    "# Note: if you want to override to specific targets, the options below should be a subset of the names from GEN3_MANUAL_DD_PATH\n",
    "# ex: target_models_to_get_training_data_for = [\"gen3.biodatacatalyst.nhlbi.nih.gov\", \"data.midrc.org\"]\n",
    "# target_models_to_get_training_data_for = [\n",
    "#     \"gen3.biodatacatalyst.nhlbi.nih.gov\",\n",
    "#     # \"data.midrc.org\",\n",
    "# ]\n",
    "\n",
    "# Specify nodes to skip from ALL target models, e.g. no mapping data will be provided from these nodes in the final training data.\n",
    "# This is intended to allow removing excessive training data for \"standard\" and required Gen3-specific nodes\n",
    "# that exist in every model (e.g. make this more applicable beyond Gen3 models, hopefully a better generalized set of training,\n",
    "# less percentage of training for what is \"standard\" nodes)\n",
    "global_target_model_nodes_to_skip = {\"program\", \"project\"}\n",
    "\n",
    "# specify property endings to skip from ALL target models, e.g. no mapping data will be providedr if the property .endswith \n",
    "# something in this list.\n",
    "# The default is to skip simple ID properties (since we're more interested in training on properties with more complex data)\n",
    "property_endings_to_skip = [\".id\"]\n",
    "\n",
    "target_model_nodes_to_skip = {\n",
    "    # GEN3_MANUAL_DD_PATH key : set of nodes to REMOVE from the training PER specific model\n",
    "    # ex: \"gen3.biodatacatalyst.nhlbi.nih.gov\": {\"foobar\", \"fizzbuzz\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "\n",
    "def _process_single_sdm_dir(\n",
    "    entry,\n",
    "    synth_model_to_original,\n",
    "    output_filepath,\n",
    "    target_models_to_get_training_data_for,\n",
    "    global_target_model_nodes_to_skip,\n",
    "    target_model_nodes_to_skip,\n",
    "    property_endings_to_skip,\n",
    "):\n",
    "    for synthetic_data_contribution_dir in os.scandir(entry.path):\n",
    "        if synthetic_data_contribution_dir.is_dir():\n",
    "            # print(\n",
    "            #     f\"processing synthetic_data_contribution_dir: {synthetic_data_contribution_dir.path}\"\n",
    "            # )\n",
    "            sdm_records = []\n",
    "\n",
    "            for target_model_dir in os.scandir(synthetic_data_contribution_dir):\n",
    "                target_model = os.path.basename(target_model_dir)\n",
    "                if (\n",
    "                    target_model_dir.is_dir()\n",
    "                    and target_model in target_models_to_get_training_data_for\n",
    "                ):\n",
    "                    target_model_name = os.path.basename(target_model_dir.path)\n",
    "                    # print(f\"processing target model mappings: {target_model_name}\")\n",
    "\n",
    "                    with open(os.path.join(target_model_dir.path, \"expected_mappings.tsv\")) as expected_mappings_file:\n",
    "                        reader = csv.DictReader(\n",
    "                            expected_mappings_file,\n",
    "                            fieldnames=[\"ai_model_node_prop_desc\", \"harmonized_model_node_prop_desc\"],\n",
    "                            delimiter=\"\\t\",\n",
    "                        )\n",
    "                        rows = list(reader)\n",
    "\n",
    "                        # TODO should we filter out properties too, like ones that are just .id?\n",
    "\n",
    "                        filtered_rows = []\n",
    "                        for row in rows:\n",
    "                            if not row:\n",
    "                                continue\n",
    "\n",
    "                            if \"harmonized_model_node_prop_desc\" not in row:\n",
    "                                continue\n",
    "\n",
    "                            if not row[\"harmonized_model_node_prop_desc\"]:\n",
    "                                continue\n",
    "\n",
    "                            node_name = (\n",
    "                                row[\"harmonized_model_node_prop_desc\"]\n",
    "                                .split(\".\")[0]\n",
    "                                .strip()\n",
    "                            )\n",
    "                            source_property_name = (\n",
    "                                row[\"ai_model_node_prop_desc\"].split(\":\")[0].strip()\n",
    "                            )\n",
    "                            target_property_name = (\n",
    "                                row[\"harmonized_model_node_prop_desc\"]\n",
    "                                .split(\":\")[0]\n",
    "                                .strip()\n",
    "                            )\n",
    "\n",
    "                            found_ending_to_skip = False\n",
    "                            for property_ending in property_endings_to_skip:\n",
    "                                if target_property_name.endswith(\n",
    "                                    property_ending.strip()\n",
    "                                ) or source_property_name.endswith(\n",
    "                                    property_ending.strip()\n",
    "                                ):\n",
    "                                    found_ending_to_skip = True\n",
    "                                    break\n",
    "\n",
    "                            if found_ending_to_skip:\n",
    "                                continue\n",
    "\n",
    "                            if not source_property_name.split(\".\")[-1].strip():\n",
    "                                # print(f\"skipping row b/c source property appears to be empty\")\n",
    "                                continue\n",
    "\n",
    "                            if not target_property_name.split(\".\")[-1].strip():\n",
    "                                # print(f\"skipping row b/c target property appears to be empty\")\n",
    "                                continue\n",
    "\n",
    "                            if node_name in global_target_model_nodes_to_skip:\n",
    "                                # print(f\"skipping row b/c node_name {node_name} is in global_target_model_nodes_to_skip\")\n",
    "                                continue\n",
    "\n",
    "                            if node_name in target_model_nodes_to_skip.get(\n",
    "                                target_model_dir, {}\n",
    "                            ):\n",
    "                                continue\n",
    "\n",
    "                            filtered_rows.append(row)\n",
    "\n",
    "                        # create jsonl record\n",
    "                        sdm_name = os.path.basename(entry.path)\n",
    "\n",
    "                        if filtered_rows:\n",
    "                            header_line = \"ai_model_node_prop_desc\\tharmonized_model_node_prop_desc\"\n",
    "                            tsv_lines = []\n",
    "                            for row in filtered_rows:\n",
    "                                line = f\"{row['ai_model_node_prop_desc']}\\t{row['harmonized_model_node_prop_desc']}\"\n",
    "\n",
    "                                # unsure how this happens, but if we get headers again, skip them\n",
    "                                if line.strip() == header_line:\n",
    "                                    continue\n",
    "\n",
    "                                tsv_lines.append(line)\n",
    "\n",
    "                            harmonized_mapping = (\n",
    "                                header_line\n",
    "                                + \"\\n\"\n",
    "                                + \"\\n\".join(\n",
    "                                    [\n",
    "                                        line\n",
    "                                        for line in list(set(tsv_lines))\n",
    "                                        if \"harmonized_model_node_prop_desc\"\n",
    "                                        not in line\n",
    "                                    ]\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                            # unsure where this duplication of headers is happening, but fix it here\n",
    "                            harmonized_mapping.replace(\n",
    "                                \"{header_line}\\n{header_line}\\n\",\n",
    "                                \"{header_line}\\n\",\n",
    "                            )\n",
    "                            harmonized_mapping = harmonized_mapping.strip()\n",
    "\n",
    "                            if len(harmonized_mapping.split(\"\\n\")) <= 2:\n",
    "                                # print(\n",
    "                                #     f\"skipping record b/c harmonized_mapping is empty\"\n",
    "                                # )\n",
    "                                continue\n",
    "\n",
    "                            record = {\n",
    "                                \"input_source_model\": sdm_name,\n",
    "                                \"synthetic_data_contribution_dir\": synthetic_data_contribution_dir.path,\n",
    "                                \"input_target_model\": target_model_name,\n",
    "                                \"harmonized_mapping\": harmonized_mapping,\n",
    "                            }\n",
    "                            sdm_records.append(record)\n",
    "\n",
    "            # Write to JSONL file\n",
    "            # print(f\"Test count: {len(sdm_records)}\")\n",
    "            with open(output_filepath, \"w\", newline=\"\") as output_file:\n",
    "                for record in sdm_records:\n",
    "                    output_file.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "\n",
    "def merge_jsonl_files_into_one(output_dir, all_files):\n",
    "    with open(\n",
    "        os.path.join(output_dir, \"training_data.jsonl\"), \"w\", newline=\"\"\n",
    "    ) as outfile:\n",
    "        for filename in all_files:\n",
    "            with open(filename, \"r\") as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "    for file in all_files:\n",
    "        os.remove(file)\n",
    "\n",
    "\n",
    "def output_training_data_from_dataset_directory(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    target_models_to_get_training_data_for,\n",
    "    global_target_model_nodes_to_skip,\n",
    "    target_model_nodes_to_skip,\n",
    "    property_endings_to_skip,\n",
    "    num_workers=32,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    temp_files = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        print(f\"processing input_dir: {input_dir}\")\n",
    "        for entry in os.scandir(input_dir):\n",
    "            if entry.is_dir() and os.path.basename(entry).startswith(\"mutated_\"):\n",
    "                for entry in os.scandir(entry.path):\n",
    "                    if entry.is_dir() and os.path.basename(entry).endswith(\"_tsvs\"):\n",
    "                        # print(f\"processing SDM: {entry.path}\")\n",
    "                        output_filepath = os.path.join(output_dir, f\"temp_{os.path.basename(entry.path)}.jsonl\")\n",
    "                        temp_files.append(output_filepath)\n",
    "                        futures.append(\n",
    "                            executor.submit(\n",
    "                                _process_single_sdm_dir,\n",
    "                                entry,\n",
    "                                synth_model_to_original,\n",
    "                                output_filepath,\n",
    "                                target_models_to_get_training_data_for,\n",
    "                                global_target_model_nodes_to_skip,\n",
    "                                target_model_nodes_to_skip,\n",
    "                                property_endings_to_skip,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error in parallel worker: {exc}\")\n",
    "                raise\n",
    "\n",
    "    merge_jsonl_files_into_one(output_dir, temp_files)\n",
    "\n",
    "output_training_data_from_dataset_directory(\n",
    "    input_dir=training_files_dir,\n",
    "    output_dir=os.path.join(training_files_dir, \"_training_data\"),\n",
    "    synth_model_to_original=synth_model_to_original,\n",
    "    target_models_to_get_training_data_for=target_models_to_get_training_data_for,\n",
    "    global_target_model_nodes_to_skip=global_target_model_nodes_to_skip,\n",
    "    target_model_nodes_to_skip=target_model_nodes_to_skip,\n",
    "    property_endings_to_skip=property_endings_to_skip,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Covert from JSONL to TSV for easier viewing\n",
    "\n",
    "Since most editors have a hard time easily viewing and filtering on JSONL data, create a denormalized TSV with columns:\n",
    "\n",
    "```\n",
    "    \"input_source_model\",\n",
    "    \"input_target_model\",\n",
    "    \"ai_model_node_prop_desc\",\n",
    "    \"harmonized_model_node_prop_desc\",\n",
    "```\n",
    "\n",
    "The last two are extracted from the `harmonized_mapping` from the JSONL and a row is created *per property to property mapping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonization_training_data_jsonl_to_csv(jsonl_file, csv_file, input_headers=None):\n",
    "    \"\"\"\n",
    "    Converts a JSONL file to a CSV file.\n",
    "\n",
    "    Headers must include: `harmonized_mapping`\n",
    "\n",
    "    This denormalizes the harmonized mapping so each property mapped is its own row.\n",
    "    \"\"\"\n",
    "    input_headers = input_headers or [\n",
    "        \"input_source_model\",\n",
    "        \"input_target_model\",\n",
    "        \"harmonized_mapping\",\n",
    "    ]\n",
    "\n",
    "    if \"harmonized_mapping\" not in input_headers:\n",
    "        raise Exception(\"Headers must include: `harmonized_mapping`\")\n",
    "\n",
    "    input_headers.remove(\"harmonized_mapping\")\n",
    "    output_headers = copy.deepcopy(input_headers)\n",
    "    output_headers.extend([\n",
    "        \"ai_model_node_prop_desc\",\n",
    "        \"harmonized_model_node_prop_desc\",\n",
    "    ])\n",
    "\n",
    "    with open(jsonl_file, \"r\") as f_in, open(csv_file, \"w\", newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(output_headers)\n",
    "\n",
    "        for line in f_in:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                for single_property_harmonized_mapping in data[\n",
    "                    \"harmonized_mapping\"\n",
    "                ].split(\"\\n\")[1:]:\n",
    "                    ai_model_node_prop_desc, harmonized_model_node_prop_desc = single_property_harmonized_mapping.split(\"\\t\")\n",
    "                    row = []\n",
    "                    for header in input_headers:\n",
    "                        if header == \"harmonized_mapping\":\n",
    "                            continue\n",
    "                        row.append(data[header])\n",
    "                    row += [\n",
    "                        ai_model_node_prop_desc,\n",
    "                        harmonized_model_node_prop_desc,\n",
    "                    ]\n",
    "                    writer.writerow(row)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_training_data_jsonl_to_csv(\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/training_data.jsonl\",\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/training_data.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now you can open and view `../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/training_data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Final Conversion to Full Training Data\n",
    "\n",
    "The above gets you an intermediate .jsonl (and CSV) without the source or target models expanded to their full JSON data (for size/efficiency and debugging). In other words, we just have names for the target models and source models right now but we want the full JSON of the models in the final training data.\n",
    "\n",
    "Now we need to expand the source/target model names into their full descriptions. This is a bit more involved, but can be done with a few steps:\n",
    "1. **Load the JSONL file**: Read the JSONL file from above.\n",
    "2. **Expand Target Model Names**: Map model names to their full structures for target Gen3 data models\n",
    "3. **Expand Source Model Names**: Map model names to their full structures for synthetic data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_JSONL_FILE = \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/training_data.jsonl\"\n",
    "OUTPUT_JSONL_FILE = \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/final_training_data.jsonl\"\n",
    "\n",
    "# note: this only allows 1 source right now, so if you ran the above with multiple folders this may not work\n",
    "SOURCE_MODELS_ROOT_DIR = \"../data/Mutated_SDCs_v3_20250423/mutated_sdc_v3_nmax4_nmin2_pmax75_pmin25_limit20_dmax1000_20250423\"\n",
    "\n",
    "\n",
    "def expand_model_names(input_file, output_file):\n",
    "    with open(input_file, \"r\") as input_file, open(output_file, \"w\") as output_file:\n",
    "        for line in input_file:\n",
    "            data = json.loads(line)\n",
    "            data_to_write = {}\n",
    "\n",
    "            # Expand Target Model Names\n",
    "            target_model_name = data[\"input_target_model\"]\n",
    "\n",
    "            # Find the appropriate JSON file\n",
    "            target_model_path = GEN3_MANUAL_DD_PATH.get(target_model_name, \"\").replace(\"_modified\", \"\")\n",
    "\n",
    "            if not os.path.exists(target_model_path):\n",
    "                print(f\"No JSON file found for target model: {target_model_name}\")\n",
    "                continue\n",
    "\n",
    "            with open(target_model_path, \"r\") as f:\n",
    "                target_model_data = json.load(f)\n",
    "\n",
    "            # Replace name with full structure\n",
    "            data_to_write[\"input_target_model_name\"] = target_model_name\n",
    "            data_to_write[\"input_target_model\"] = target_model_data\n",
    "\n",
    "            # Expand Source Model Names\n",
    "            source_model_name = data[\"input_source_model\"]\n",
    "            original_synthetic_data_contribution_dir = os.path.basename(data[\"synthetic_data_contribution_dir\"])\n",
    "\n",
    "            sdm_file_dir = f\"{SOURCE_MODELS_ROOT_DIR}/{source_model_name}/{original_synthetic_data_contribution_dir}\"\n",
    "\n",
    "            sdm_data = None\n",
    "            for entry in os.scandir(sdm_file_dir):\n",
    "                if entry.is_file() and os.path.basename(entry).endswith(\"__jsonschema_dd.json\"):\n",
    "                    with open(entry, \"r\") as f:\n",
    "                        sdm_data = json.load(f)\n",
    "\n",
    "            if not sdm_data:\n",
    "                continue\n",
    "\n",
    "            # Replace name with full structure\n",
    "            data_to_write[\"input_source_model_name\"] = source_model_name\n",
    "            data_to_write[\"input_source_model\"] = sdm_data\n",
    "\n",
    "            data_to_write[\"harmonized_mapping\"] = data[\"harmonized_mapping\"]\n",
    "\n",
    "            output_file.write(json.dumps(data_to_write) + \"\\n\")\n",
    "\n",
    "expand_model_names(INTERMEDIATE_JSONL_FILE, OUTPUT_JSONL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We can also denormalize the output so there's a single source property to target property mapping per row with entire source and target models. Note that this file will be very large. Here's how you'd do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Now your `OUTPUT_JSONL_FILE` defined above has all the source and target models fully expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_training_data_jsonl_to_csv(\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/final_training_data.jsonl\",\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2/_training_data/final_training_data.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
