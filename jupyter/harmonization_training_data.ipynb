{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Get Harmonization Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will use the same source of data as we did for v3 (the latest) of the synthetic data from objective 1. Using only the mutated versions here: https://uchicago.app.box.com/folder/318234520321?s=j0oq2pfhe59p3caqq3ss32kxqwciwbsd\n",
    "\n",
    "You can limit the training data generation to utilize only one of the above sub-folders, for example, we will extract and use only one: `mutated_sdc_v3_nmax4_nmin2_pmax75_pmin25_limit20_dmax1000_20250423.zip` and ignore others. It's unclear how much data will be needed for training, so we could add more in the future.\n",
    "\n",
    "Ensure the above is extracted into a `../data/Mutated_SDCs_v3_20250423/mutated_sdc_v3_nmax4_nmin2_pmax75_pmin25_limit20_dmax1000_20250423` folder.\n",
    "\n",
    "Also we need the required the synthetic data dictionary information: https://uchicago.app.box.com/folder/318236025225\n",
    "\n",
    "Extract the above in a `../data/SDMs_nodes6-15_props50-100_20250423` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from harmonization.utils import (\n",
    "    TEMP_DIR,\n",
    "    get_data_model_as_node_prop_descriptions,\n",
    "    get_gen3_json_schemas_and_templates,\n",
    ")\n",
    "\n",
    "GEN3_MANUAL_DD_PATH = {\n",
    "    \"aids.diseasedatahub.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/aids.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"bihstaging.data-commons.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/bihstaging.data-commons.org__jsonschema_dd_modified.json\",\n",
    "    \"caninedc.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/caninedc.org__jsonschema_dd_modified.json\",\n",
    "    \"chicagoland.pandemicresponsecommons.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/chicagoland.pandemicresponsecommons.org__jsonschema_dd_modified.json\",\n",
    "    \"chordshealth.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/chordshealth.org__jsonschema_dd_modified.json\",\n",
    "    \"data.bloodpac.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/data.bloodpac.org__jsonschema_dd_modified.json\",\n",
    "    \"data.midrc.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/data.midrc.org__jsonschema_dd_modified.json\",\n",
    "    \"diseasedatahub.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"flu.diseasedatahub.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/flu.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"gen3.biodatacatalyst.nhlbi.nih.gov\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/gen3.biodatacatalyst.nhlbi.nih.gov__jsonschema_dd_modified.json\",\n",
    "    \"gen3.datacommons.io\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/gen3.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"genomel\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/genomel__jsonschema_dd_modified.json\",\n",
    "    \"healdata.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/healdata.org__jsonschema_dd_modified.json\",\n",
    "    \"hnc\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/hnc__jsonschema_dd_modified.json\",\n",
    "    \"ibdgc\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/ibdgc__jsonschema_dd_modified.json\",\n",
    "    \"icgc.bionimbus.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/icgc.bionimbus.org__jsonschema_dd_modified.json\",\n",
    "    \"jcoin.datacommons.io\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/jcoin.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"kidsfirst\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/kidsfirst__jsonschema_dd_modified.json\",\n",
    "    \"microbiome\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/microbiome__jsonschema_dd_modified.json\",\n",
    "    \"mmrf\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/mmrf__jsonschema_dd_modified.json\",\n",
    "    \"nci-crdc.datacommons.io\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/nci-crdc.datacommons.io__jsonschema_dd_modified.json\",\n",
    "    \"pdp\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/pdp__jsonschema_dd_modified.json\",\n",
    "    \"portal.occ-data.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/portal.occ-data.org__jsonschema_dd_modified.json\",\n",
    "    \"rerf\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/rerf__jsonschema_dd_modified.json\",\n",
    "    \"tb.diseasedatahub.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/tb.diseasedatahub.org__jsonschema_dd_modified.json\",\n",
    "    \"toxdatacommons\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/toxdatacommons__jsonschema_dd_modified.json\",\n",
    "    \"vpodc.data-commons.org\": \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/vpodc.data-commons.org__jsonschema_dd_modified.json\",\n",
    "}\n",
    "\n",
    "SYNONYMOUS_NODES = {\n",
    "    \"study\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"dataset\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"clinical_trial\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"collection\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"research\": [\"study\", \"dataset\", \"clinical_trial\", \"collection\", \"research\"],\n",
    "    \"subject\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"patient\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"case\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"participant\": [\"subject\", \"patient\", \"case\", \"participant\"],\n",
    "    \"biospecimen\": [\"biospecimen\", \"specimen\"],\n",
    "    \"specimen\": [\"biospecimen\", \"specimen\"],\n",
    "    \"project\": [\"project\"],  # this forces a search over other DDs later\n",
    "    \"program\": [\"program\"],  # this forces a search over other DDs later\n",
    "}\n",
    "SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD = {\n",
    "    # will be populated programatically\n",
    "}\n",
    "gen3_dd_schemas = {\n",
    "    # will be populated programatically\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping from nodes that have snyns to a list of dicts with that particular node name\n",
    "for node, _ in SYNONYMOUS_NODES.items():\n",
    "    for schema_name, schema_path in GEN3_MANUAL_DD_PATH.items():\n",
    "        with open(schema_path, \"r\") as file:\n",
    "            gen3_dd_schema = json.load(file)\n",
    "            gen3_dd_schemas[schema_name] = gen3_dd_schema\n",
    "            if node in gen3_dd_schema:\n",
    "                SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD.setdefault(node, set()).add(\n",
    "                    schema_name\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Convert original data model schema format to more efficient dictionary-based format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm_input_schemas_dir = \"../data/SDMs_nodes6-15_props50-100_20250423/input_schemas/\"\n",
    "for root, dirs, files in os.walk(sdm_input_schemas_dir):\n",
    "    for file in files:\n",
    "        if \"_modified.json\" in file:\n",
    "            continue\n",
    "\n",
    "        output_filename = os.path.join(root, file.replace(\".json\", \"_modified.json\"))\n",
    "        if os.path.exists(output_filename):\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(root, file)) as input_file:\n",
    "            original_data = json.load(input_file)\n",
    "            final_output_structure = {\n",
    "                # \"{{node_name}}\": {\n",
    "                # \"properties\": {\n",
    "                # \"{{property_name}}\": {}\n",
    "                # }\n",
    "                # }\n",
    "            }\n",
    "            for node in original_data[\"nodes\"]:\n",
    "                new_node = copy.deepcopy(node)\n",
    "\n",
    "                # is currently a list, we want a dict\n",
    "                del new_node[\"properties\"]\n",
    "                new_node[\"properties\"] = {}\n",
    "\n",
    "                for node_property in node[\"properties\"]:\n",
    "                    new_node[\"properties\"][node_property[\"name\"]] = node_property\n",
    "\n",
    "                final_output_structure[node[\"name\"]] = new_node\n",
    "\n",
    "            with open(output_filename, \"w\") as output_file:\n",
    "                output_file.write(json.dumps(final_output_structure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mutated real data dicts: https://uchicago.app.box.com/folder/318234520321?s=j0oq2pfhe59p3caqq3ss32kxqwciwbsd\n",
    "# and place as input_dir\n",
    "# input_dir = \"../data/Mutated_SDCs_v3_20250423\"\n",
    "input_dir = \"../data/Mutated_SDCs_v3_20250423\"\n",
    "\n",
    "# synth_dd_to_real_info_file = (\n",
    "#     \"../data/SDMs_nodes6-15_props50-100_20250423/_modified_SDM_stats.json\"\n",
    "# )\n",
    "sdm_stats_filepath = \"../data/SDMs_nodes6-15_props50-100_20250423/_SDM_stats.json\"\n",
    "output_dir = \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_prop_desc_from_real_data_model(\n",
    "    gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "):\n",
    "    description = get_desc_from_real_data_model(\n",
    "        gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "    )\n",
    "    if description is None:\n",
    "        logging.warning(\n",
    "            f\"could not get desc for property {property_name} in node {node_name} from {gen3_domain}\"\n",
    "        )\n",
    "\n",
    "    return f\"{node_name}.{property_name}: {description}\"\n",
    "\n",
    "\n",
    "def get_desc_from_real_data_model(\n",
    "    gen3_dd_schemas, gen3_domain, node_name, property_name\n",
    "):\n",
    "    # Extract descriptions from Gen3 dictionary domain\n",
    "    gen3_node = gen3_dd_schemas[gen3_domain].get(node_name, {})\n",
    "\n",
    "    gen3_node_prop = gen3_node.get(\"properties\", {}).get(property_name, {})\n",
    "\n",
    "    if not gen3_node_prop:\n",
    "        # logging.warning(\n",
    "        #     f\"Property {property_name} not in node {node_name} in {gen3_domain}\"\n",
    "        # )\n",
    "        return None\n",
    "\n",
    "    gen3_description = (\n",
    "        gen3_node_prop.get(\"description\", \"\").replace(\"\\t\", \"    \").replace(\"\\n\", \" \")\n",
    "    )\n",
    "\n",
    "    return gen3_description\n",
    "\n",
    "\n",
    "def get_synth_model_to_original(sdm_stats_filepath):\n",
    "    synth_model_to_original = {}\n",
    "\n",
    "    with open(sdm_stats_filepath) as file:\n",
    "        sdm_stats = json.load(file)\n",
    "\n",
    "    for model_num, model_info in sdm_stats[\"models\"].items():\n",
    "        # we will populate this information per synth model\n",
    "        synth_model_to_original_info = {\n",
    "            \"node_to_original_info\": None,\n",
    "            \"synth_model_node_to_properties\": None,\n",
    "            \"synth_model_prop_to_original_models\": None,\n",
    "        }\n",
    "\n",
    "        # todo: helper func?\n",
    "        node_to_original_info = {}\n",
    "        for node_name, node_info in model_info[\"nodes\"].items():\n",
    "            real_data_model = node_info[\"dm_name\"]\n",
    "\n",
    "            node_to_original_info[node_name] = node_info\n",
    "\n",
    "        synth_model_to_original_info[\"node_to_original_info\"] = node_to_original_info\n",
    "\n",
    "        # now we need to get the actual nodes and props used in this synth model\n",
    "        # need to read in synthmodel.json\n",
    "        synth_model_path = os.path.join(\n",
    "            os.path.dirname(sdm_stats_filepath), f\"SDM_{model_num}.json\"\n",
    "        )\n",
    "        synth_model_node_to_properties = {}\n",
    "        with open(synth_model_path) as synth_model_file:\n",
    "            synth_model = json.load(synth_model_file)\n",
    "            for node_info in synth_model[\"nodes\"]:\n",
    "                for node_property in node_info[\"properties\"]:\n",
    "                    synth_model_node_to_properties.setdefault(\n",
    "                        node_info[\"name\"], []\n",
    "                    ).append(node_property)\n",
    "\n",
    "        synth_model_to_original_info[\"synth_model_node_to_properties\"] = (\n",
    "            synth_model_node_to_properties\n",
    "        )\n",
    "\n",
    "        # next step\n",
    "\n",
    "        synth_model_prop_to_original_models = {\n",
    "            # \"node.property\": [{\n",
    "            #                       \"gen3_domain\": gen3_domain\n",
    "            #                       \"original_node\": gen3_node\n",
    "            #                       \"original_property\": gen3_node_prop\n",
    "            #                   }, ...]\n",
    "        }\n",
    "        for node_name, node_info in synth_model_to_original_info[\n",
    "            \"synth_model_node_to_properties\"\n",
    "        ].items():\n",
    "            for node_property_info in node_info:\n",
    "                property_name = node_property_info[\"name\"]\n",
    "                original_sources_already_seen_for_prop = set()\n",
    "\n",
    "                # find which original dd and node property came from\n",
    "\n",
    "                # node is not synonymous so we just check the original\n",
    "                if node_name not in SYNONYMOUS_NODES:\n",
    "                    if (\n",
    "                        node_name\n",
    "                        in synth_model_to_original_info[\"node_to_original_info\"]\n",
    "                    ):\n",
    "                        original_model = synth_model_to_original_info[\n",
    "                            \"node_to_original_info\"\n",
    "                        ][node_name][\"dm_name\"]\n",
    "\n",
    "                        synth_node_prop_desc = node_property_info[\"description\"]\n",
    "\n",
    "                        gen3_node = gen3_dd_schemas[original_model].get(node_name, {})\n",
    "                        gen3_node_prop = gen3_node[\"properties\"].get(property_name, {})\n",
    "\n",
    "                        if gen3_node_prop:\n",
    "                            original_info = {\n",
    "                                \"gen3_domain\": gen3_domain,\n",
    "                                \"original_node\": gen3_node,\n",
    "                                \"original_property\": gen3_node_prop,\n",
    "                            }\n",
    "                            synth_model_prop_to_original_models.setdefault(\n",
    "                                f\"{node_name}.{property_name}\", []\n",
    "                            ).append(original_info)\n",
    "                    else:\n",
    "                        # node not found in specified original model and\n",
    "                        # not a synonmous node.\n",
    "                        # This means we need to check parent_nodes information to determine where this node came from\n",
    "                        # (due to missing information from the synthetic data generation SDM stats creation)\n",
    "                        for node, node_info in model_info[\"nodes\"].items():\n",
    "                            if node_name in node_info.get(\"parent_nodes\", {}):\n",
    "                                gen3_domain = node_info[\"dm_name\"]\n",
    "                                gen3_node = gen3_dd_schemas[gen3_domain].get(\n",
    "                                    node_name, {}\n",
    "                                )\n",
    "                                gen3_node_prop = gen3_node[\"properties\"].get(\n",
    "                                    node_name, {}\n",
    "                                )\n",
    "\n",
    "                                if gen3_node_prop:\n",
    "                                    original_info = {\n",
    "                                        \"gen3_domain\": gen3_domain,\n",
    "                                        \"original_node\": gen3_node,\n",
    "                                        \"original_property\": gen3_node_prop,\n",
    "                                    }\n",
    "                                    synth_model_prop_to_original_models.setdefault(\n",
    "                                        f\"{node_name}.{property_name}\", []\n",
    "                                    ).append(original_info)\n",
    "\n",
    "                    if original_model not in original_sources_already_seen_for_prop:\n",
    "                        original_info = {\n",
    "                            \"gen3_domain\": original_model,\n",
    "                            \"original_node\": gen3_node,\n",
    "                            \"original_property\": gen3_node_prop,\n",
    "                        }\n",
    "                        original_sources_already_seen_for_prop.add(original_model)\n",
    "                        synth_model_prop_to_original_models.setdefault(\n",
    "                            f\"{node_name}.{property_name}\", []\n",
    "                        ).append(original_info)\n",
    "                elif node_name in SYNONYMOUS_NODES:\n",
    "                    alternate_names = SYNONYMOUS_NODES.get(node_name, [])\n",
    "\n",
    "                    for alternate_node_name in alternate_names:\n",
    "                        for gen3_domain in SYNONYMOUS_NODES_TO_GEN3_MANUAL_DD.get(\n",
    "                            alternate_node_name, []\n",
    "                        ):\n",
    "                            gen3_node = gen3_dd_schemas[gen3_domain].get(\n",
    "                                alternate_node_name, {}\n",
    "                            )\n",
    "                            new_property_name = property_name.replace(\n",
    "                                node_name, alternate_node_name\n",
    "                            )\n",
    "\n",
    "                            gen3_node_prop = gen3_node[\"properties\"].get(\n",
    "                                new_property_name, {}\n",
    "                            )\n",
    "\n",
    "                            if gen3_node_prop:\n",
    "                                original_info = {\n",
    "                                    \"gen3_domain\": gen3_domain,\n",
    "                                    \"original_node\": gen3_node,\n",
    "                                    \"original_property\": gen3_node_prop,\n",
    "                                }\n",
    "                                synth_model_prop_to_original_models.setdefault(\n",
    "                                    f\"{node_name}.{property_name}\", []\n",
    "                                ).append(original_info)\n",
    "                else:\n",
    "                    logging.warning(\n",
    "                        f\"could not find {node_name}.{property_name} in SDM_{model_num}.json\"\n",
    "                    )\n",
    "\n",
    "        synth_model_to_original_info[\"synth_model_prop_to_original_models\"] = (\n",
    "            synth_model_prop_to_original_models\n",
    "        )\n",
    "\n",
    "        synth_model_to_original[model_num] = synth_model_to_original_info\n",
    "    return synth_model_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_model_to_original = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes time, run this once then rely on persisted results (i.e. comment this out after running once)\n",
    "synth_model_to_original = get_synth_model_to_original(sdm_stats_filepath)\n",
    "print(len(synth_model_to_original.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_training_data_for_directory(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    force_recreation_of_synth_data_info=False,\n",
    "    force_recreation_of_synth_data_contributions=False,\n",
    "):\n",
    "    for entry in os.scandir(input_dir):\n",
    "        if entry.is_dir() and os.path.basename(entry).startswith(\"mutated_\"):\n",
    "            logging.warning(f\"processing: {entry.path}\")\n",
    "            for data_model_dir in os.scandir(entry):\n",
    "                if data_model_dir.is_dir() and os.path.basename(\n",
    "                    data_model_dir\n",
    "                ).endswith(\"_tsvs\"):\n",
    "                    logging.debug(\n",
    "                        f\"processing data model directory: {data_model_dir.path}\"\n",
    "                    )\n",
    "                    for data_contribution_dir in os.scandir(data_model_dir):\n",
    "                        if data_contribution_dir.is_dir():\n",
    "                            logging.debug(\n",
    "                                f\"processing data contribution directory: {os.path.basename(data_contribution_dir.path)}\"\n",
    "                            )\n",
    "                            persist_synth_data_info_to_contribution_directory(\n",
    "                                data_contribution_dir.path,\n",
    "                                synth_model_to_original,\n",
    "                                force_recreation=force_recreation_of_synth_data_info,\n",
    "                            )\n",
    "                            process_synth_data_contribution_directory(\n",
    "                                data_contribution_dir.path,\n",
    "                                output_dir,\n",
    "                                gen3_dd_schemas,\n",
    "                                force_recreation=force_recreation_of_synth_data_contributions,\n",
    "                            )\n",
    "\n",
    "\n",
    "def persist_synth_data_info_to_contribution_directory(\n",
    "    directory_path, synth_model_to_original, force_recreation=False\n",
    "):\n",
    "    if not synth_model_to_original:\n",
    "        logging.warning(\n",
    "            f\"Trying to persist synth_model_to_original but nothing was provided\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    model_path = os.path.dirname(directory_path)\n",
    "    mutation_path = os.path.dirname(model_path)\n",
    "    # logging.debug(f\"model_path: {model_path}\")\n",
    "    # logging.debug(f\"mutation_path: {mutation_path}\")\n",
    "\n",
    "    synth_model_to_original_filepath = os.path.join(\n",
    "        model_path, \"synth_model_to_original.json\"\n",
    "    )\n",
    "\n",
    "    if force_recreation and os.path.exists(synth_model_to_original_filepath):\n",
    "        os.remove(synth_model_to_original_filepath)\n",
    "\n",
    "    if not os.path.exists(synth_model_to_original_filepath):\n",
    "        with open(\n",
    "            os.path.join(model_path, \"synth_model_to_original.json\"), \"w\"\n",
    "        ) as output:\n",
    "            for entry in os.scandir(directory_path):\n",
    "                if os.path.basename(entry).endswith(\"__var_map.json\"):\n",
    "                    synth_model_name = entry.path.split(\"__\")[0]\n",
    "                    synth_model_num = synth_model_name.split(\"_\")[-1]\n",
    "\n",
    "                    specific_synth_model_to_original = synth_model_to_original[\n",
    "                        synth_model_num\n",
    "                    ]\n",
    "                    output.write(json.dumps(specific_synth_model_to_original))\n",
    "\n",
    "\n",
    "def process_synth_data_contribution_directory(\n",
    "    directory_path, output_dir, gen3_dd_schemas, force_recreation=False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each original model, generate a folder with files related\n",
    "    Format of output:\n",
    "    \"\"\"\n",
    "    # each of these SDCs will generate multiple test cases b/c properties from\n",
    "\n",
    "    model_path = os.path.dirname(directory_path)\n",
    "    mutation_path = os.path.dirname(model_path)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(mutation_path, exist_ok=True)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    synth_model_to_original_filepath = os.path.join(\n",
    "        model_path, \"synth_model_to_original.json\"\n",
    "    )\n",
    "    specific_synth_model_to_original = {}\n",
    "    with open(synth_model_to_original_filepath) as input_file:\n",
    "        try:\n",
    "            specific_synth_model_to_original = json.load(input_file)\n",
    "        except Exception as exc:\n",
    "            logging.error(\n",
    "                f\"Could not get specific_synth_model_to_original from: {input_file}. Skipping...\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "    output_dir_updated = os.path.join(output_dir, os.path.basename(mutation_path))\n",
    "    output_dir_updated = os.path.join(output_dir_updated, os.path.basename(model_path))\n",
    "    output_dir_updated = os.path.join(\n",
    "        output_dir_updated, os.path.basename(directory_path)\n",
    "    )\n",
    "\n",
    "    if force_recreation and os.path.exists(output_dir_updated):\n",
    "        os.remove(output_dir_updated)\n",
    "\n",
    "    if not os.path.exists(output_dir_updated):\n",
    "        output_mappings_for_directory(\n",
    "            gen3_dd_schemas=gen3_dd_schemas,\n",
    "            input_dir=directory_path,\n",
    "            output_dir=output_dir_updated,\n",
    "            specific_synth_model_to_original=specific_synth_model_to_original,\n",
    "        )\n",
    "    else:\n",
    "        # print(f\"skipping {output_dir_updated}, already exists\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def output_mappings_for_directory(\n",
    "    gen3_dd_schemas, input_dir, output_dir, specific_synth_model_to_original\n",
    "):\n",
    "    # Mirror directory structure and process files\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        relative_path = os.path.relpath(root, input_dir)\n",
    "        mirrored_path = os.path.join(output_dir, relative_path)\n",
    "        os.makedirs(mirrored_path, exist_ok=True)\n",
    "\n",
    "        ai_model_node_props = {}\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"__var_map.json\"):\n",
    "                synth_model_name = file_name.split(\"__\")[0]\n",
    "                synth_model_num = synth_model_name.split(\"_\")[-1]\n",
    "\n",
    "                synth_model_prop_to_original_models = specific_synth_model_to_original[\n",
    "                    \"synth_model_prop_to_original_models\"\n",
    "                ]\n",
    "\n",
    "                # Transform var_map.json to expected_mappings.tsv\n",
    "                with open(os.path.join(root, file_name), \"r\") as var_map_file:\n",
    "                    var_map_data = json.load(var_map_file)\n",
    "\n",
    "                # print(f\"var_map_data: {var_map_data}\")\n",
    "\n",
    "                # since we can have nodes from multiple different original models\n",
    "                tsv_lines_per_original_model = {}\n",
    "                tsv_lines_header = (\n",
    "                    \"ai_model_node_prop_desc\\tharmonized_model_node_prop_desc\\n\"\n",
    "                )\n",
    "                original_node_to_harmonized_node = {}\n",
    "                for original_key, value in var_map_data.items():\n",
    "                    # the varmap data comes in 3 forms: node, node.property, and node.\"foreign_key_node.foreign_key_property\"\n",
    "                    # we only want really care about determining mapping from node.property at this point, so ignore\n",
    "                    # the other two\n",
    "                    if \".\" not in original_key:\n",
    "                        original_node_to_harmonized_node[original_key] = value.get(\n",
    "                            \"new_name\", \"\"\n",
    "                        )\n",
    "\n",
    "                for original_key, value in var_map_data.items():\n",
    "                    if \".\" not in original_key:\n",
    "                        continue\n",
    "\n",
    "                    original_node = original_key.split(\".\")[0]\n",
    "                    harmonized_node = original_node_to_harmonized_node[original_node]\n",
    "                    original_property = \".\".join(original_key.split(\".\")[-2:])\n",
    "\n",
    "                    new_name = value.get(\"new_name\", \"\")\n",
    "                    new_description = value.get(\"new_description\", \"\").strip()\n",
    "\n",
    "                    ai_model_node_prop_desc = (\n",
    "                        f\"{harmonized_node}.{new_name}: {new_description}\"\n",
    "                    )\n",
    "                    # ai_model_node_prop_desc = specific_synth_model_to_original[\"synth_model_node_to_properties\"].get(\n",
    "                    #     new_description, \"\"\n",
    "                    # )\n",
    "\n",
    "                    # if not ai_model_node_prop_desc:\n",
    "                    #     ai_model_node_prop_desc = ai_model_node_props.get(new_name, \"\")\n",
    "\n",
    "                    # if not ai_model_node_prop_desc:\n",
    "                    #     logging.debug(\n",
    "                    #         f\"can't find: {new_name} by description: {new_description} in output. skipping...\"\n",
    "                    #     )\n",
    "                    #     continue\n",
    "\n",
    "                    # get gen3_domain (e.g. original model) for this particular node prop by\n",
    "                    # using previously constructured mapping information\n",
    "                    original_node_props = synth_model_prop_to_original_models.get(\n",
    "                        original_key, []\n",
    "                    )\n",
    "                    # if not original_node_props:\n",
    "                    #     logging.warning(\n",
    "                    #         f\"{original_key} not in synth_model_prop_to_original_models for {ai_model_node_prop_desc}\"\n",
    "                    #     )\n",
    "\n",
    "                    for original_node_prop in original_node_props:\n",
    "                        gen3_domain = original_node_prop[\"gen3_domain\"]\n",
    "                        original_node = original_node_prop[\"original_node\"][\"name\"]\n",
    "                        original_property = original_node_prop[\"original_property\"].get(\n",
    "                            \"name\", \"\"\n",
    "                        )\n",
    "\n",
    "                        # Extract descriptions from Gen3 dictionary domain\n",
    "                        gen3_description = (\n",
    "                            get_desc_from_real_data_model(\n",
    "                                gen3_dd_schemas,\n",
    "                                gen3_domain,\n",
    "                                original_node,\n",
    "                                original_property,\n",
    "                            )\n",
    "                            or \"\"\n",
    "                        )\n",
    "                        harmonized_model_node_prop_desc = (\n",
    "                            f\"{original_node}.{original_property}: {gen3_description}\"\n",
    "                        )\n",
    "\n",
    "                        ai_model_line = f\"{ai_model_node_prop_desc}\\t{harmonized_model_node_prop_desc}\"\n",
    "                        tsv_lines_per_original_model.setdefault(gen3_domain, []).append(\n",
    "                            ai_model_line\n",
    "                        )\n",
    "\n",
    "                # Save as expected_mappings.tsv\n",
    "                if tsv_lines_per_original_model:\n",
    "                    for gen3_domain, tsv_lines in tsv_lines_per_original_model.items():\n",
    "                        os.makedirs(\n",
    "                            os.path.join(mirrored_path, gen3_domain), exist_ok=True\n",
    "                        )\n",
    "                        with open(\n",
    "                            os.path.join(\n",
    "                                os.path.join(mirrored_path, gen3_domain),\n",
    "                                \"expected_mappings.tsv\",\n",
    "                            ),\n",
    "                            \"w\",\n",
    "                        ) as tsv_file:\n",
    "                            tsv_file.write(tsv_lines_header)\n",
    "                            tsv_file.write(\"\\n\".join(list(set(tsv_lines))))\n",
    "                else:\n",
    "                    logging.warning(f\"No mappings for {mirrored_path}\")\n",
    "\n",
    "            # Process files, rename jsonschema to ai_model_output\n",
    "            elif \"__jsonschema_dd__\" in file_name:\n",
    "                if file_name.endswith(\"__jsonschema_dd.json\"):\n",
    "                    gen3_domain = file_name.split(\"__\")[0]\n",
    "\n",
    "                    output_harmonized_model_path = os.path.join(\n",
    "                        mirrored_path, \"harmonized_data_model.json\"\n",
    "                    )\n",
    "                    with open(output_harmonized_model_path, \"w\") as dd_file:\n",
    "                        dd_file.write(json.dumps(gen3_dd_schemas[gen3_domain]))\n",
    "\n",
    "                    # Rename JSON schema files\n",
    "                    new_name = file_name.replace(\"__jsonschema_dd__\", \"__\").replace(\n",
    "                        \"__jsonschema_dd.json\", \"__ai_model_output.json\"\n",
    "                    )\n",
    "                    shutil.copy(\n",
    "                        os.path.join(root, file_name),\n",
    "                        os.path.join(mirrored_path, new_name),\n",
    "                    )\n",
    "                    with open(os.path.join(root, file_name)) as ai_model_ouput_file:\n",
    "                        ai_model_ouput = json.load(ai_model_ouput_file)\n",
    "                        ai_model_node_props_raw = (\n",
    "                            get_data_model_as_node_prop_descriptions(ai_model_ouput)\n",
    "                        )\n",
    "                        for node_prop in ai_model_node_props_raw:\n",
    "                            if \":\" in node_prop:\n",
    "                                prop = node_prop.split(\":\")[0]\n",
    "                                desc = \":\".join(node_prop.split(\":\")[1:])\n",
    "                            else:\n",
    "                                prop = node_prop\n",
    "                                desc = \"\"\n",
    "\n",
    "                            # key on description b/c that's what's in the mapping we can rely on\n",
    "                            ai_model_node_props[desc.strip()] = node_prop\n",
    "\n",
    "    # print(f\"Done. Output: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "\n",
    "# sequential approach, ignore for now. async approach is below\n",
    "# output_training_data_for_directory(input_dir, output_dir, synth_model_to_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Process all SDCs into Training Data using the Pre-Computed Info Above\n",
    "\n",
    "This takes a _long_ time. If you leave the default settings, it'll skip recomputing folders that already exist in the output (i.e. you can stop and restart it many times and it'll basically continue where it left off).\n",
    "\n",
    "You can adjust the number of workers `num_workers` below and you should set that as high as possible.\n",
    "\n",
    "We're processing 7 folders, each with 10k synthetic data models, and for each synthetic data model we can have upwards of a dozen SDCs. And each SDC can create mappings for up to 27 of the original data models (e.g. if project contains properties that would map to any model, we generate expected mappings to that model).\n",
    "\n",
    "We don't have the full count yet, but this could mean a maximum of ~20 million expected mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def output_training_data_for_directory(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    force_recreation_of_synth_data_info=False,\n",
    "    force_recreation_of_synth_data_contributions=False,\n",
    "    num_workers=8,\n",
    "):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for entry in os.scandir(input_dir):\n",
    "            if entry.is_dir() and os.path.basename(entry).startswith(\"mutated_\"):\n",
    "                logging.warning(f\"processing: {entry.path}\")\n",
    "                for data_model_dir in os.scandir(entry):\n",
    "                    if data_model_dir.is_dir() and os.path.basename(\n",
    "                        data_model_dir\n",
    "                    ).endswith(\"_tsvs\"):\n",
    "                        logging.debug(\n",
    "                            f\"processing data model directory: {data_model_dir.path}\"\n",
    "                        )\n",
    "                        for data_contribution_dir in os.scandir(data_model_dir):\n",
    "                            if data_contribution_dir.is_dir():\n",
    "                                logging.debug(\n",
    "                                    f\"queueing data contribution directory: {os.path.basename(data_contribution_dir.path)}\"\n",
    "                                )\n",
    "                                # submit each contribution dir to the pool\n",
    "                                futures.append(\n",
    "                                    executor.submit(\n",
    "                                        _process_single_data_contribution_dir,\n",
    "                                        data_contribution_dir.path,\n",
    "                                        output_dir,\n",
    "                                        synth_model_to_original,\n",
    "                                        force_recreation_of_synth_data_info,\n",
    "                                        force_recreation_of_synth_data_contributions,\n",
    "                                    )\n",
    "                                )\n",
    "        # wait for all to finish\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                f.result()\n",
    "            except Exception as exc:\n",
    "                logging.error(\n",
    "                    f\"[THREAD ERROR] Data contribution dir task failed: {exc}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def _process_single_data_contribution_dir(\n",
    "    data_contribution_path,\n",
    "    output_dir,\n",
    "    synth_model_to_original,\n",
    "    force_recreation_of_synth_data_info,\n",
    "    force_recreation_of_synth_data_contributions,\n",
    "):\n",
    "    # This function is called in thread pool, for a single data_contribution directory\n",
    "    persist_synth_data_info_to_contribution_directory(\n",
    "        data_contribution_path,\n",
    "        synth_model_to_original,\n",
    "        force_recreation=force_recreation_of_synth_data_info,\n",
    "    )\n",
    "    process_synth_data_contribution_directory(\n",
    "        data_contribution_path,\n",
    "        output_dir,\n",
    "        gen3_dd_schemas,\n",
    "        force_recreation=force_recreation_of_synth_data_contributions,\n",
    "    )\n",
    "\n",
    "\n",
    "output_training_data_for_directory(input_dir, output_dir, synth_model_to_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Use Data Generated from Above to Construct AI-ready Training Data in JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files_dir = (\n",
    "    \"../datasets/harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def remove_property_folders_without_mapping(root_folder):\n",
    "    \"\"\"\n",
    "    In each root/mutated_dd_*/original_gen3_dd_*/<arbitrary_folder>/,\n",
    "    deletes <arbitrary_folder> if expected_mappings.tsv is missing.\n",
    "    \"\"\"\n",
    "    for mutated_dd_dir in os.listdir(root_folder):\n",
    "        mutated_dd_path = os.path.join(root_folder, mutated_dd_dir)\n",
    "        if not os.path.isdir(mutated_dd_path):\n",
    "            continue\n",
    "        for original_dd_dir in os.listdir(mutated_dd_path):\n",
    "            original_dd_path = os.path.join(mutated_dd_path, original_dd_dir)\n",
    "            if not os.path.isdir(original_dd_path):\n",
    "                continue\n",
    "            for prop_folder in os.listdir(original_dd_path):\n",
    "                prop_folder_path = os.path.join(original_dd_path, prop_folder)\n",
    "                if not os.path.isdir(prop_folder_path):\n",
    "                    continue\n",
    "                for gen3_dd_dir in os.listdir(prop_folder_path):\n",
    "                    gen3_dd_path = os.path.join(prop_folder_path, gen3_dd_dir)\n",
    "                    if not os.path.isdir(gen3_dd_path):\n",
    "                        continue\n",
    "                    mapping_path = os.path.join(gen3_dd_path, \"expected_mappings.tsv\")\n",
    "                    if not os.path.exists(mapping_path):\n",
    "                        print(\n",
    "                            f\"Removing folder (no expected_mappings.tsv): {gen3_dd_path} \"\n",
    "                        )\n",
    "                        shutil.rmtree(prop_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_property_folders_without_mapping(training_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def _process_single_mutated_dir(\n",
    "#     entry,\n",
    "#     synth_model_to_original,\n",
    "#     output_dir,\n",
    "#     force_recreation_of_synth_data_info,\n",
    "#     force_recreation_of_synth_data_contributions,\n",
    "# ):\n",
    "#     for data_model_dir in os.scandir(entry.path):\n",
    "#         if data_model_dir.is_dir() and os.path.basename(data_model_dir).endswith(\n",
    "#             \"_tsvs\"\n",
    "#         ):\n",
    "#             logging.debug(f\"processing data model directory: {data_model_dir.path}\")\n",
    "#             for data_contribution_dir in os.scandir(data_model_dir):\n",
    "#                 if data_contribution_dir.is_dir():\n",
    "#                     logging.debug(\n",
    "#                         f\"processing data contribution directory: {os.path.basename(data_contribution_dir.path)}\"\n",
    "#                     )\n",
    "#                     persist_synth_data_info_to_contribution_directory(\n",
    "#                         data_contribution_dir.path,\n",
    "#                         synth_model_to_original,\n",
    "#                         force_recreation=force_recreation_of_synth_data_info,\n",
    "#                     )\n",
    "#                     process_synth_data_contribution_directory(\n",
    "#                         data_contribution_dir.path,\n",
    "#                         output_dir,\n",
    "#                         gen3_dd_schemas,\n",
    "#                         force_recreation=force_recreation_of_synth_data_contributions,\n",
    "#                     )\n",
    "\n",
    "\n",
    "# def output_training_data_for_directory(\n",
    "#     input_dir,\n",
    "#     output_dir,\n",
    "#     synth_model_to_original,\n",
    "#     force_recreation_of_synth_data_info=False,\n",
    "#     force_recreation_of_synth_data_contributions=False,\n",
    "#     num_workers=14,\n",
    "# ):\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         futures = []\n",
    "#         for entry in os.scandir(input_dir):\n",
    "#             if entry.is_dir() and os.path.basename(entry).startswith(\"mutated_\"):\n",
    "#                 logging.warning(f\"processing: {entry.path}\")\n",
    "#                 futures.append(\n",
    "#                     executor.submit(\n",
    "#                         _process_single_mutated_dir,\n",
    "#                         entry,\n",
    "#                         synth_model_to_original,\n",
    "#                         output_dir,\n",
    "#                         force_recreation_of_synth_data_info,\n",
    "#                         force_recreation_of_synth_data_contributions,\n",
    "#                     )\n",
    "#                 )\n",
    "#         # Optionally, wait for all to complete:\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             try:\n",
    "#                 future.result()\n",
    "#             except Exception as exc:\n",
    "#                 logging.error(f\"Error in parallel worker: {exc}\")\n",
    "\n",
    "# output_training_data_for_directory(input_dir, output_dir, synth_model_to_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_mappings_from_root_directory(gen3_dd_schemas, input_dir, output_dir):\n",
    "#     # Mirror directory structure and process files\n",
    "#     for root, dirs, files in os.walk(input_dir):\n",
    "#         for dir in dirs:\n",
    "#             if \"mutated_\" not in dir:\n",
    "#                 continue\n",
    "\n",
    "#             mirrored_path = os.path.join(output_dir, dir)\n",
    "#             os.makedirs(mirrored_path, exist_ok=True)\n",
    "\n",
    "#             full_path = os.path.join(root, dir)\n",
    "#             print(f\"Handling dir: {full_path}\")\n",
    "#             output_mappings_for_directory(\n",
    "#                 gen3_dd_schemas, full_path,\n",
    "#                 mirrored_path,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_mappings_from_root_directory(gen3_dd_schemas, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Format of output:\n",
    "\n",
    "- harmonization_training_Mutated_SDCs_v3_20250423_v0.0.2\n",
    "  - mutated_synthetic_data_with_specific_parameters\n",
    "    - SDM_synthetic_data_model_0\n",
    "      - synthetic_data_contribution_0\n",
    "        - original_real_source_data_model_0\n",
    "          - expected_mappings.tsv\n",
    "        - original_real_source_data_model_1\n",
    "          - expected_mappings.tsv\n",
    "        - ...\n",
    "      - ...\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "`expected_mappings.tsv` has the following columns: \n",
    "\n",
    "- `ai_model_node_prop_desc`\n",
    "    - The AI Model generated node property (e.g. the property we want to map/harmonize to a target model)\n",
    "- `harmonized_model_node_prop_desc`\n",
    "    - The source of truth target model node property (e.g. the property from the target model we know the above should map to)\n",
    "\n",
    "and the target model itself is identified by the `original_real_source_data_model` folder that the expected mappings are in.\n",
    "\n",
    "#### How do we know the proper mapping?\n",
    "\n",
    "We took mutated synthetic data from our first algorithm and traced backwards.\n",
    "\n",
    "This is how we got the mutated data in the first place:\n",
    "\n",
    "27 Real Gen3 Data Models -> 10k Synthetic Data Models -> Synthetic Data Contributions -> Mutated Synthetic Data Contributions\n",
    "\n",
    "We were able to trace from the mutations, back to the synthetic model, and from the synthetic model we are able to determine\n",
    "the set of possible mappings for that particular property back to the real data models. \n",
    "\n",
    "> Key point: A single mutated property could potentially map back to `n` original real data models, so we collect every option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Construct Single Benchmark Test File\n",
    "\n",
    "TBD\n",
    "<!-- \n",
    "Now let's create a JSONL file with a test per row.\n",
    "\n",
    "The JSONL file should have 3 columns: `input_source_model`, `input_target_model`, `harmonized_mapping`\n",
    "\n",
    "Those 3 columns should be populated by content of the files:\n",
    "\n",
    "- `*__ai_model_ouput.json` == `input_source_model`\n",
    "- `expected_mappings.tsv` == `input_target_model`\n",
    "- `harmonized_data_model.json` == `harmonized_mapping` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_jsonl_from_structure(root_dir, output_jsonl_path):\n",
    "#     \"\"\"\n",
    "#     Iterates through subfolders under root_dir and writes a single JSONL file\n",
    "#     with input_source_model, input_target_model, harmonized_mapping fields.\n",
    "#     \"\"\"\n",
    "#     records = []\n",
    "#     for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "#         # Find the first * __ai_model_ouput.json file in this directory\n",
    "#         print(f\"Current dir: {dirpath}\")\n",
    "#         print(f\"Files in dir: {filenames}\")\n",
    "\n",
    "#         ai_model_files = glob.glob(os.path.join(dirpath, \"*ai_model_output*\"))\n",
    "#         expected_mappings_path = os.path.join(dirpath, \"expected_mappings.tsv\")\n",
    "#         target_harmoized_model_path = os.path.join(dirpath, \"harmonized_data_model.json\")\n",
    "\n",
    "#         if (\n",
    "#             ai_model_files\n",
    "#             and os.path.isfile(expected_mappings_path)\n",
    "#             and os.path.isfile(target_harmoized_model_path)\n",
    "#         ):\n",
    "#             ai_model_path = ai_model_files[0]\n",
    "#             # Read files\n",
    "#             with open(ai_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 input_source_model = f.read()\n",
    "#             with open(expected_mappings_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 harmonized_mapping = f.read()\n",
    "#             with open(target_harmoized_model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 input_target_model = f.read()\n",
    "#             # Append JSON record\n",
    "#             record = {\n",
    "#                 \"input_source_model\": input_source_model,\n",
    "#                 \"input_target_model\": input_target_model,\n",
    "#                 \"harmonized_mapping\": harmonized_mapping,\n",
    "#             }\n",
    "#             records.append(record)\n",
    "\n",
    "#     # Write to JSONL file\n",
    "#     print(f\"Test count: {len(records)}\")\n",
    "#     with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "#         for record in records:\n",
    "#             fout.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_json_filepath = os.path.join(output_dir, \"output.jsonl\")\n",
    "# create_jsonl_from_structure(output_dir, output_json_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Test ability to get original files from JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_files_from_harmonization_benchmark_jsonl_row(row_dict, output_dir, row_index):\n",
    "#     \"\"\"\n",
    "#     Extracts the desired content from the row,\n",
    "#     creates a per-row output subdirectory,\n",
    "#     and writes each file into that subdirectory.\n",
    "#     Returns a dict of {filename: content} for that row.\n",
    "#     \"\"\"\n",
    "#     # Make a subdirectory for this row\n",
    "#     row_folder = os.path.join(output_dir, f\"row_{row_index}\")\n",
    "#     os.makedirs(row_folder, exist_ok=True)\n",
    "\n",
    "#     files = {\n",
    "#         \"restored__ai_model_output.json\": row_dict[\"input_source_model\"],\n",
    "#         \"expected_mappings.tsv\": row_dict[\"input_target_model\"],\n",
    "#         \"harmonized_data_model.json\": row_dict[\"harmonized_mapping\"],\n",
    "#     }\n",
    "#     for filename, contents in files.items():\n",
    "#         with open(os.path.join(row_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(contents)\n",
    "#     return files\n",
    "\n",
    "\n",
    "# def process_harmonization_benchmark_jsonl(jsonl_path, output_dir):\n",
    "#     \"\"\"\n",
    "#     Reads the JSONL file and calls the row handler for each row.\n",
    "#     \"\"\"\n",
    "#     with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for idx, line in enumerate(f):\n",
    "#             row = json.loads(line)\n",
    "#             _ = get_files_from_harmonization_benchmark_jsonl_row(row, output_dir, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_harmonization_benchmark_jsonl(\n",
    "#     output_json_filepath, \"../output/temp/harmonization_training\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
